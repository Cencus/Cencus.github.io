{"title":"spark-sql","uid":"c4e1f631332880dad580bd689797a6a3","slug":"spark-sql","date":"2022-04-16T12:49:55.000Z","updated":"2022-04-18T03:02:09.738Z","comments":true,"path":"api/articles/spark-sql.json","keywords":null,"cover":[],"content":"<h1 id=\"第1章-SparkSQL-概述\"><a href=\"#第1章-SparkSQL-概述\" class=\"headerlink\" title=\"第1章 SparkSQL 概述\"></a>第1章 SparkSQL 概述</h1><h2 id=\"1-3-SparkSQL-特点\"><a href=\"#1-3-SparkSQL-特点\" class=\"headerlink\" title=\"1.3 SparkSQL 特点\"></a>1.3 SparkSQL 特点</h2><h3 id=\"1-3-1-易整合\"><a href=\"#1-3-1-易整合\" class=\"headerlink\" title=\"1.3.1 易整合\"></a>1.3.1 易整合</h3><p>无缝的整合了 SQL 查询和 Spark 编程</p>\n<h3 id=\"1-3-2-统一的数据访问\"><a href=\"#1-3-2-统一的数据访问\" class=\"headerlink\" title=\"1.3.2 统一的数据访问\"></a>1.3.2 统一的数据访问</h3><p>使用相同的方式连接不同的数据源</p>\n<h3 id=\"1-3-3-兼容\"><a href=\"#1-3-3-兼容\" class=\"headerlink\" title=\"1.3.3 兼容\"></a>1.3.3 兼容</h3><p>Hive 在已有的仓库上直接运行 SQL 或者 HiveQL</p>\n<h3 id=\"1-3-4-标准数据连接\"><a href=\"#1-3-4-标准数据连接\" class=\"headerlink\" title=\"1.3.4 标准数据连接\"></a>1.3.4 标准数据连接</h3><p>通过 JDBC 或者 ODBC 来连接</p>\n<h2 id=\"1-4-DataFrame-是什么\"><a href=\"#1-4-DataFrame-是什么\" class=\"headerlink\" title=\"1.4 DataFrame 是什么\"></a>1.4 DataFrame 是什么</h2><p>在 Spark 中，DataFrame 是一种以 RDD 为基础的分布式数据集，类似于传统数据库中 的二维表格。DataFrame 与 RDD 的主要区别在于，前者带有 schema 元信息，即 DataFrame 所表示的二维表数据集的每一列都带有名称和类型。这使得 Spark SQL 得以洞察更多的结构 信息，从而对藏于 DataFrame 背后的数据源以及作用于 DataFrame 之上的变换进行了针对性 的优化，最终达到大幅提升运行时效率的目标。反观 RDD，由于无从得知所存数据元素的 具体内部结构，Spark Core 只能在 stage 层面进行简单、通用的流水线优化。 同时，与 Hive 类似，DataFrame 也支持嵌套数据类型（struct、array 和 map）。从 API  易用性的角度上看，DataFrame API 提供的是一套高层的关系操作，比函数式的 RDD API 要 更加友好，门槛更低。</p>\n<h2 id=\"1-5-DataSet-是什么\"><a href=\"#1-5-DataSet-是什么\" class=\"headerlink\" title=\"1.5 DataSet 是什么\"></a>1.5 DataSet 是什么</h2><p>DataSet 是分布式数据集合。DataSet 是 Spark 1.6 中添加的一个新抽象，是 DataFrame 的一个扩展。它提供了 RDD 的优势（强类型，使用强大的 lambda 函数的能力）以及 Spark  SQL 优化执行引擎的优点。DataSet 也可以使用功能性的转换（操作 map，flatMap，filter 等等）。 <br/> ➢ DataSet 是 DataFrame API 的一个扩展，是 SparkSQL 最新的数据抽象 <br/> ➢ 用户友好的 API 风格，既具有类型安全检查也具有 DataFrame 的查询优化特性； <br/> ➢ 用样例类来对 DataSet 中定义数据的结构信息，样例类中每个属性的名称直接映射到 DataSet 中的字段名称； <br/> ➢ DataSet 是强类型的。比如可以有 DataSet[Car]，DataSet[Person]。 <br/> ➢ DataFrame 是 DataSet 的特列，DataFrame&#x3D;DataSet[Row] ，所以可以通过 as 方法将 DataFrame 转换为 DataSet。Row 是一个类型，跟 Car、Person 这些的类型一样，所有的 表结构信息都用 Row 来表示。获取数据时需要指定顺序</p>\n<h1 id=\"第2章-SparkSQL-核心编程\"><a href=\"#第2章-SparkSQL-核心编程\" class=\"headerlink\" title=\"第2章 SparkSQL 核心编程\"></a>第2章 SparkSQL 核心编程</h1><h2 id=\"2-1-新的起点\"><a href=\"#2-1-新的起点\" class=\"headerlink\" title=\"2.1 新的起点\"></a>2.1 新的起点</h2><h2 id=\"2-2-DataFrame\"><a href=\"#2-2-DataFrame\" class=\"headerlink\" title=\"2.2 DataFrame\"></a>2.2 DataFrame</h2><h3 id=\"2-2-1-创建-DataFrame\"><a href=\"#2-2-1-创建-DataFrame\" class=\"headerlink\" title=\"2.2.1 创建 DataFrame\"></a>2.2.1 创建 DataFrame</h3><p>类似二维表格</p>\n<p>RDD只关心数据，而不关心数据所代表的含义，而DF既关心数据又关心数据所代表的含义</p>\n<p>在 Spark SQL 中 SparkSession 是创建 DataFrame 和执行 SQL 的入口，创建 DataFrame 有三种方式：</p>\n<ul>\n<li>通过 Spark 的数据源进行创建；</li>\n<li>从一个存在的 RDD 进行转换；</li>\n<li>还可以从 Hive  Table 进行查询返回。</li>\n</ul>\n<pre class=\"line-numbers language-none\"><code class=\"language-none\">scala&gt; val df &#x3D; spark.read.json(&quot;data&#x2F;user.json&quot;)\ndf: org.apache.spark.sql.DataFrame &#x3D; [age: bigint， username: string]<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span></span></code></pre>\n\n\n\n<p><span style=\"color:red\">注意：如果从内存中获取数据，spark 可以知道数据类型具体是什么。如果是数字，默认作 为 Int 处理；但是从文件中读取的数字，不能确定是什么类型，所以用 bigint 接收，可以和 Long 类型转换，但是和 Int 不能进行转换</span></p>\n<p>\\2) 从 RDD 进行转换</p>\n<p>\\3) 从 Hive Table 进行查询返回</p>\n<h3 id=\"2-2-2-SQL-语法\"><a href=\"#2-2-2-SQL-语法\" class=\"headerlink\" title=\"2.2.2 SQL 语法\"></a>2.2.2 SQL 语法</h3><pre class=\"line-numbers language-none\"><code class=\"language-none\">scala&gt; val df &#x3D; spark.read.json(&quot;data&#x2F;user.json&quot;)\nscala&gt; df.createOrReplaceTempView(&quot;people&quot;)<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span></span></code></pre>\n\n<pre class=\"line-numbers language-none\"><code class=\"language-none\">spark.sql(&quot;SELECT * FROM people&quot;).show<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span></span></code></pre>\n\n<p><span style=\"color:red\">注意：普通临时表是 Session 范围内的，如果想应用范围内有效，可以使用全局临时表。使 用全局临时表时需要全路径访问，如：<span style=\"color:blue\">global_temp</span>.people</span></p>\n<pre class=\"line-numbers language-none\"><code class=\"language-none\">scala&gt; df.createGlobalTempView(&quot;people&quot;)<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span></span></code></pre>\n\n<pre class=\"line-numbers language-none\"><code class=\"language-none\">scala&gt; spark.sql(&quot;SELECT * FROM global_temp.people&quot;).show() \n+---+--------+\n|age|username|\n+---+--------+\n| 20|zhangsan|\n| 30| lisi|\n| 40| wangwu|\n+---+--------+\n\nscala&gt; spark.newSession().sql(&quot;SELECT * FROM global_temp.people&quot;).show()\n+---+--------+\n|age|username|\n+---+--------+\n| 20|zhangsan|\n| 30| lisi|\n| 40| wangwu|\n+---+--------+<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n\n<h3 id=\"2-2-3-DSL-语法\"><a href=\"#2-2-3-DSL-语法\" class=\"headerlink\" title=\"2.2.3 DSL 语法\"></a>2.2.3 DSL 语法</h3><p>DataFrame 提供一个特定领域语言(domain-specific language, DSL)去管理结构化的数据。 可以在 Scala, Java, Python 和 R 中使用 DSL，使用 DSL 语法风格不必去创建临时视图了</p>\n<p>不把表当表，而当作数据源</p>\n<p>df.rdd</p>\n<p>df to rdd</p>\n<p>rdd.todf(列名1,列名2)</p>\n<h2 id=\"2-3-DataSet\"><a href=\"#2-3-DataSet\" class=\"headerlink\" title=\"2.3 DataSet\"></a>2.3 DataSet</h2><p>DataSet 是具有强类型的数据集合，需要提供对应的类型信息。</p>\n<p>dF是特俗的ds，又有数据又有结构又有类型，强类型，df的升级版</p>\n<h3 id=\"2-3-1-创建-DataSet\"><a href=\"#2-3-1-创建-DataSet\" class=\"headerlink\" title=\"2.3.1 创建 DataSet\"></a>2.3.1 创建 DataSet</h3><h3 id=\"2-3-2-RDD-转换为-DataSet\"><a href=\"#2-3-2-RDD-转换为-DataSet\" class=\"headerlink\" title=\"2.3.2 RDD 转换为 DataSet\"></a>2.3.2 RDD 转换为 DataSet</h3><h3 id=\"2-3-3-DataSet-转换为-RDD\"><a href=\"#2-3-3-DataSet-转换为-RDD\" class=\"headerlink\" title=\"2.3.3 DataSet 转换为 RDD\"></a>2.3.3 DataSet 转换为 RDD</h3><h2 id=\"2-4-DataFrame-和-DataSet-转换\"><a href=\"#2-4-DataFrame-和-DataSet-转换\" class=\"headerlink\" title=\"2.4 DataFrame 和 DataSet 转换\"></a>2.4 DataFrame 和 DataSet 转换</h2><h2 id=\"2-5-RDD、DataFrame、DataSet-三者的关系\"><a href=\"#2-5-RDD、DataFrame、DataSet-三者的关系\" class=\"headerlink\" title=\"2.5 RDD、DataFrame、DataSet 三者的关系\"></a>2.5 RDD、DataFrame、DataSet 三者的关系</h2><p> 在 SparkSQL 中 Spark 为我们提供了两个新的抽象，分别是 DataFrame 和 DataSet。他们 和 RDD 有什么区别呢？首先从版本的产生上来看： <br/> ➢ Spark1.0 &#x3D;&gt; RDD  <br/> ➢ Spark1.3 &#x3D;&gt; DataFrame <br/> ➢ Spark1.6 &#x3D;&gt; Dataset 如果同样的数据都给到这三个数据结构，他们分别计算之后，都会给出相同的结果。不 同是的他们的执行效率和执行方式。在后期的 Spark 版本中，DataSet 有可能会逐步取代 RDD 和 DataFrame 成为唯一的 API 接口。 </p>\n<h3 id=\"2-5-1-三者的共性\"><a href=\"#2-5-1-三者的共性\" class=\"headerlink\" title=\"2.5.1 三者的共性\"></a>2.5.1 三者的共性</h3><p> ➢ RDD、DataFrame、DataSet 全都是 spark 平台下的分布式弹性数据集，为处理超大型数 据提供便利; </p>\n<p> ➢ 三者都有惰性机制，在进行创建、转换，如 map 方法时，不会立即执行，只有在遇到 Action 如 foreach 时，三者才会开始遍历运算; </p>\n<p> ➢ 三者有许多共同的函数，如 filter，排序等; </p>\n<p> ➢ 在对 DataFrame 和 Dataset 进行操作许多操作都需要这个包:import spark.implicits._（在 创建好 SparkSession 对象后尽量直接导入） </p>\n<p> ➢ 三者都会根据 Spark 的内存情况自动缓存运算，这样即使数据量很大，也不用担心会 内存溢出 </p>\n<p> ➢ 三者都有 partition 的概念 </p>\n<p> ➢ DataFrame 和 DataSet 均可使用模式匹配获取各个字段的值和类型</p>\n<h3 id=\"2-5-2-三者的区别\"><a href=\"#2-5-2-三者的区别\" class=\"headerlink\" title=\"2.5.2 三者的区别\"></a>2.5.2 三者的区别</h3><ol>\n<li><p>RDD </p>\n<p> ➢ RDD 一般和 spark mllib 同时使用 <br/> ➢ RDD 不支持 sparksql 操作 2) DataFrame <br/> ➢ 与 RDD 和 Dataset 不同，DataFrame 每一行的类型固定为 Row，每一列的值没法直 接访问，只有通过解析才能获取各个字段的值 <br/> ➢ DataFrame 与 DataSet 一般不与 spark mllib 同时使用2.5.2 三者的区别 1) RDD <br/> ➢ RDD 一般和 spark mllib 同时使用 <br/> ➢ RDD 不支持 sparksql 操作 2) DataFrame <br/> ➢ 与 RDD 和 Dataset 不同，DataFrame 每一行的类型固定为 Row，每一列的值没法直 接访问，只有通过解析才能获取各个字段的值 <br/> ➢ DataFrame 与 DataSet 一般不与 spark mllib 同时使用</p>\n</li>\n</ol>\n<p><img src=\"https://gitee.com/cencus/blog-image/raw/master/blogImage/1650163447811.png\"></p>\n<h2 id=\"2-6-IDEA-开发-SparkSQL\"><a href=\"#2-6-IDEA-开发-SparkSQL\" class=\"headerlink\" title=\"2.6 IDEA 开发 SparkSQL\"></a>2.6 IDEA 开发 SparkSQL</h2><h2 id=\"2-7-用户自定义函数\"><a href=\"#2-7-用户自定义函数\" class=\"headerlink\" title=\"2.7 用户自定义函数\"></a>2.7 用户自定义函数</h2><h3 id=\"2-7-1-UDF\"><a href=\"#2-7-1-UDF\" class=\"headerlink\" title=\"2.7.1 UDF\"></a>2.7.1 UDF</h3><p>用户自定义函数</p>\n<p>比如说我们想要一个功能，对于一个有name的表，我们查出来name那一列后，在name前面加上一个前缀，这个时候我们很容易想到</p>\n<p>select “Name” + name as new_name from User;</p>\n<p>但是这样是不可以实现功能的</p>\n<p>这种时候就需要我们的自定义函数来做操作了</p>\n<pre class=\"line-numbers language-scala\" data-language=\"scala\"><code class=\"language-scala\"><span class=\"token comment\">//定义</span>\nspark<span class=\"token punctuation\">.</span>udf<span class=\"token punctuation\">.</span>register<span class=\"token punctuation\">(</span><span class=\"token string\">\"函数名\"</span><span class=\"token punctuation\">,</span> <span class=\"token punctuation\">(</span>参数列表<span class=\"token punctuation\">)</span> <span class=\"token keyword\">=></span> <span class=\"token punctuation\">&#123;</span>\n     操作\n<span class=\"token punctuation\">&#125;</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\">//示例</span>\nspark<span class=\"token punctuation\">.</span>udf<span class=\"token punctuation\">.</span>register<span class=\"token punctuation\">(</span><span class=\"token string\">\"perfixName\"</span><span class=\"token punctuation\">,</span> <span class=\"token punctuation\">(</span>name<span class=\"token operator\">:</span> <span class=\"token builtin\">String</span><span class=\"token punctuation\">)</span> <span class=\"token keyword\">=></span> <span class=\"token punctuation\">&#123;</span>\n\n​\t<span class=\"token string\">\"Name: \"</span> <span class=\"token operator\">+</span> name\n\n<span class=\"token punctuation\">&#125;</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\">//使用</span>\nspark<span class=\"token punctuation\">.</span>sql<span class=\"token punctuation\">(</span><span class=\"token string\">\"select age, perfixName(username) from user\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>show<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n\n\n\n<h3 id=\"2-7-2-UDAF\"><a href=\"#2-7-2-UDAF\" class=\"headerlink\" title=\"2.7.2 UDAF\"></a>2.7.2 UDAF</h3><p>聚合函数</p>\n<p>就是将很多数据算出一个结构，比如均值，方差等等</p>\n<p>计数也是最常用的聚合函数</p>\n<h4 id=\"1、弱类型\"><a href=\"#1、弱类型\" class=\"headerlink\" title=\"1、弱类型\"></a>1、弱类型</h4><pre class=\"line-numbers language-scala\" data-language=\"scala\"><code class=\"language-scala\"><span class=\"token comment\">//1. 准备</span>\n <span class=\"token comment\">//自定义聚合函数类，计算年龄的平均值</span>\n  <span class=\"token comment\">/*\n  1. extends UserDefinedAggregateFunction(不建议)\n  2. 重写方法\n   */</span>\n  <span class=\"token keyword\">class</span> MyAvgUDAF <span class=\"token keyword\">extends</span> UserDefinedAggregateFunction<span class=\"token punctuation\">&#123;</span>\n\n    <span class=\"token comment\">//输入的结构，这里只有年龄</span>\n    <span class=\"token keyword\">override</span> <span class=\"token keyword\">def</span> inputSchema<span class=\"token operator\">:</span> StructType <span class=\"token operator\">=</span> <span class=\"token punctuation\">&#123;</span>\n      StructType<span class=\"token punctuation\">(</span>\n        Array<span class=\"token punctuation\">(</span>\n          StructField<span class=\"token punctuation\">(</span><span class=\"token string\">\"age\"</span><span class=\"token punctuation\">,</span> LongType<span class=\"token punctuation\">)</span>\n        <span class=\"token punctuation\">)</span>\n      <span class=\"token punctuation\">)</span>\n    <span class=\"token punctuation\">&#125;</span>\n\n    <span class=\"token comment\">//buffer中的结构</span>\n    <span class=\"token keyword\">override</span> <span class=\"token keyword\">def</span> bufferSchema<span class=\"token operator\">:</span> StructType <span class=\"token operator\">=</span> <span class=\"token punctuation\">&#123;</span>\n      StructType<span class=\"token punctuation\">(</span>\n        Array<span class=\"token punctuation\">(</span>\n          StructField<span class=\"token punctuation\">(</span><span class=\"token string\">\"total\"</span><span class=\"token punctuation\">,</span> LongType<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n          StructField<span class=\"token punctuation\">(</span><span class=\"token string\">\"count\"</span><span class=\"token punctuation\">,</span> LongType<span class=\"token punctuation\">)</span>\n        <span class=\"token punctuation\">)</span>\n      <span class=\"token punctuation\">)</span>\n    <span class=\"token punctuation\">&#125;</span>\n\n    <span class=\"token comment\">//函数的计算结构的输出类型，就是输出的类型</span>\n    <span class=\"token keyword\">override</span> <span class=\"token keyword\">def</span> dataType<span class=\"token operator\">:</span> DataType <span class=\"token operator\">=</span> LongType\n\n    <span class=\"token comment\">//函数的稳定性</span>\n    <span class=\"token comment\">//单值函数就是稳定的</span>\n    <span class=\"token keyword\">override</span> <span class=\"token keyword\">def</span> deterministic<span class=\"token operator\">:</span> <span class=\"token builtin\">Boolean</span> <span class=\"token operator\">=</span> <span class=\"token boolean\">true</span>\n\n    <span class=\"token comment\">//缓冲区初始化</span>\n    <span class=\"token keyword\">override</span> <span class=\"token keyword\">def</span> initialize<span class=\"token punctuation\">(</span>buffer<span class=\"token operator\">:</span> MutableAggregationBuffer<span class=\"token punctuation\">)</span><span class=\"token operator\">:</span> <span class=\"token builtin\">Unit</span> <span class=\"token operator\">=</span> <span class=\"token punctuation\">&#123;</span>\n<span class=\"token comment\">//      buffer(0) = 0L</span>\n<span class=\"token comment\">//      buffer(1) = 0L</span>\n\n      <span class=\"token comment\">//更新buffer里面数据的值 buffer.update(position, initValue)</span>\n      buffer<span class=\"token punctuation\">.</span>update<span class=\"token punctuation\">(</span><span class=\"token number\">0</span><span class=\"token punctuation\">,</span> <span class=\"token number\">0L</span><span class=\"token punctuation\">)</span>\n      buffer<span class=\"token punctuation\">.</span>update<span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> <span class=\"token number\">0L</span><span class=\"token punctuation\">)</span>\n    <span class=\"token punctuation\">&#125;</span>\n\n    <span class=\"token comment\">//数据从input过来时，以何种方式更新缓冲区 缓冲区如何应对过来的数据</span>\n    <span class=\"token comment\">//getLong 所有的都是getanyType，只是泛型不一样，最好加类型，然后传入索引</span>\n    <span class=\"token comment\">//没有类型属性只靠索引的弊端已经显现</span>\n    <span class=\"token keyword\">override</span> <span class=\"token keyword\">def</span> update<span class=\"token punctuation\">(</span>buffer<span class=\"token operator\">:</span> MutableAggregationBuffer<span class=\"token punctuation\">,</span> input<span class=\"token operator\">:</span> Row<span class=\"token punctuation\">)</span><span class=\"token operator\">:</span> <span class=\"token builtin\">Unit</span> <span class=\"token operator\">=</span> <span class=\"token punctuation\">&#123;</span>\n      buffer<span class=\"token punctuation\">.</span>update<span class=\"token punctuation\">(</span><span class=\"token number\">0</span><span class=\"token punctuation\">,</span> buffer<span class=\"token punctuation\">.</span>getLong<span class=\"token punctuation\">(</span><span class=\"token number\">0</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">+</span> input<span class=\"token punctuation\">.</span>getLong<span class=\"token punctuation\">(</span><span class=\"token number\">0</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n      buffer<span class=\"token punctuation\">.</span>update<span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> buffer<span class=\"token punctuation\">.</span>getLong<span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">+</span> <span class=\"token number\">1</span><span class=\"token punctuation\">)</span>\n    <span class=\"token punctuation\">&#125;</span>\n\n    <span class=\"token comment\">//缓冲区合并 分布式计算 合并值 total+total count + count</span>\n    <span class=\"token comment\">//两两聚合是按照从前吃到后的顺序 比如 x y - - => x=x+y => x y - => ... => x - => ... => x，也就是不断向第一个聚合</span>\n    <span class=\"token keyword\">override</span> <span class=\"token keyword\">def</span> merge<span class=\"token punctuation\">(</span>buffer1<span class=\"token operator\">:</span> MutableAggregationBuffer<span class=\"token punctuation\">,</span> buffer2<span class=\"token operator\">:</span> Row<span class=\"token punctuation\">)</span><span class=\"token operator\">:</span> <span class=\"token builtin\">Unit</span> <span class=\"token operator\">=</span> <span class=\"token punctuation\">&#123;</span>\n      buffer1<span class=\"token punctuation\">.</span>update<span class=\"token punctuation\">(</span><span class=\"token number\">0</span><span class=\"token punctuation\">,</span> buffer1<span class=\"token punctuation\">.</span>getLong<span class=\"token punctuation\">(</span><span class=\"token number\">0</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">+</span> buffer2<span class=\"token punctuation\">.</span>getLong<span class=\"token punctuation\">(</span><span class=\"token number\">0</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n      buffer1<span class=\"token punctuation\">.</span>update<span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> buffer1<span class=\"token punctuation\">.</span>getLong<span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">+</span> buffer2<span class=\"token punctuation\">.</span>getLong<span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n    <span class=\"token punctuation\">&#125;</span>\n\n    <span class=\"token comment\">//计算平均值</span>\n    <span class=\"token keyword\">override</span> <span class=\"token keyword\">def</span> evaluate<span class=\"token punctuation\">(</span>buffer<span class=\"token operator\">:</span> Row<span class=\"token punctuation\">)</span><span class=\"token operator\">:</span> <span class=\"token builtin\">Any</span> <span class=\"token operator\">=</span> <span class=\"token punctuation\">&#123;</span>\n      buffer<span class=\"token punctuation\">.</span>getLong<span class=\"token punctuation\">(</span><span class=\"token number\">0</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">/</span> buffer<span class=\"token punctuation\">.</span>getLong<span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span>\n    <span class=\"token punctuation\">&#125;</span>\n  <span class=\"token punctuation\">&#125;</span>\n\n\n\n<span class=\"token comment\">//2. 使用</span>\nspark<span class=\"token punctuation\">.</span>udf<span class=\"token punctuation\">.</span>register<span class=\"token punctuation\">(</span><span class=\"token string\">\"ageAvg\"</span><span class=\"token punctuation\">,</span> <span class=\"token keyword\">new</span> MyAvgUDAF<span class=\"token punctuation\">)</span>\nspark<span class=\"token punctuation\">.</span>sql<span class=\"token punctuation\">(</span><span class=\"token string\">\"select ageAvg(age) from user\"</span><span class=\"token punctuation\">)</span><span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n\n\n\n<h4 id=\"2、强类型\"><a href=\"#2、强类型\" class=\"headerlink\" title=\"2、强类型\"></a>2、强类型</h4><pre class=\"line-numbers language-scala\" data-language=\"scala\"><code class=\"language-scala\"><span class=\"token comment\">//定义</span>\n<span class=\"token comment\">/*\n  1. 继承 org.apache.spark.sql.expressions.Aggregator，定义泛型\n    IN: 输入的数据类型 Long\n    BUF: 缓冲区的数据类型 Buff\n    OUT: 输出的数据类型 Long\n   */</span>\n  <span class=\"token comment\">// 模板类，默认是val</span>\n  <span class=\"token keyword\">case</span> <span class=\"token keyword\">class</span> Buff<span class=\"token punctuation\">(</span><span class=\"token keyword\">var</span> total<span class=\"token operator\">:</span><span class=\"token builtin\">Long</span><span class=\"token punctuation\">,</span> <span class=\"token keyword\">var</span> count<span class=\"token operator\">:</span><span class=\"token builtin\">Long</span><span class=\"token punctuation\">)</span>\n\n  <span class=\"token keyword\">class</span> MyAvgUDTF <span class=\"token keyword\">extends</span> Aggregator<span class=\"token punctuation\">[</span><span class=\"token builtin\">Long</span><span class=\"token punctuation\">,</span> Buff<span class=\"token punctuation\">,</span> <span class=\"token builtin\">Long</span><span class=\"token punctuation\">]</span> <span class=\"token punctuation\">&#123;</span>\n    <span class=\"token comment\">//zero 零值或者初始值</span>\n    <span class=\"token comment\">//缓冲区的初始化</span>\n    <span class=\"token keyword\">override</span> <span class=\"token keyword\">def</span> zero<span class=\"token operator\">:</span> Buff <span class=\"token operator\">=</span> <span class=\"token punctuation\">&#123;</span>\n      Buff<span class=\"token punctuation\">(</span><span class=\"token number\">0L</span><span class=\"token punctuation\">,</span> <span class=\"token number\">0L</span><span class=\"token punctuation\">)</span>\n    <span class=\"token punctuation\">&#125;</span>\n\n    <span class=\"token comment\">//根据输入的数据更新缓冲区的数据</span>\n    <span class=\"token keyword\">override</span> <span class=\"token keyword\">def</span> reduce<span class=\"token punctuation\">(</span>b<span class=\"token operator\">:</span> Buff<span class=\"token punctuation\">,</span> a<span class=\"token operator\">:</span> <span class=\"token builtin\">Long</span><span class=\"token punctuation\">)</span><span class=\"token operator\">:</span> Buff <span class=\"token operator\">=</span> <span class=\"token punctuation\">&#123;</span>\n      b<span class=\"token punctuation\">.</span>total <span class=\"token operator\">=</span> b<span class=\"token punctuation\">.</span>total <span class=\"token operator\">+</span> a\n      b<span class=\"token punctuation\">.</span>count <span class=\"token operator\">=</span> b<span class=\"token punctuation\">.</span> count <span class=\"token operator\">+</span> <span class=\"token number\">1</span>\n      b\n    <span class=\"token punctuation\">&#125;</span>\n\n    <span class=\"token comment\">//合并缓冲区</span>\n    <span class=\"token keyword\">override</span> <span class=\"token keyword\">def</span> merge<span class=\"token punctuation\">(</span>b1<span class=\"token operator\">:</span> Buff<span class=\"token punctuation\">,</span> b2<span class=\"token operator\">:</span> Buff<span class=\"token punctuation\">)</span><span class=\"token operator\">:</span> Buff <span class=\"token operator\">=</span> <span class=\"token punctuation\">&#123;</span>\n      b1<span class=\"token punctuation\">.</span>total <span class=\"token operator\">=</span> b1<span class=\"token punctuation\">.</span>total <span class=\"token operator\">+</span> b2<span class=\"token punctuation\">.</span>total\n      b1<span class=\"token punctuation\">.</span>count <span class=\"token operator\">=</span> b1<span class=\"token punctuation\">.</span>count <span class=\"token operator\">+</span> b2<span class=\"token punctuation\">.</span>count\n      b1\n    <span class=\"token punctuation\">&#125;</span>\n\n    <span class=\"token comment\">//计算结果</span>\n    <span class=\"token keyword\">override</span> <span class=\"token keyword\">def</span> finish<span class=\"token punctuation\">(</span>reduction<span class=\"token operator\">:</span> Buff<span class=\"token punctuation\">)</span><span class=\"token operator\">:</span> <span class=\"token builtin\">Long</span> <span class=\"token operator\">=</span> <span class=\"token punctuation\">&#123;</span>\n      reduction<span class=\"token punctuation\">.</span>total <span class=\"token operator\">/</span> reduction<span class=\"token punctuation\">.</span>count\n    <span class=\"token punctuation\">&#125;</span>\n\n    <span class=\"token comment\">//下面两个几乎不用改变</span>\n    <span class=\"token comment\">//分布式计算，设计序列化的问题 encoder是编码 缓冲区的编码操作</span>\n    <span class=\"token keyword\">override</span> <span class=\"token keyword\">def</span> bufferEncoder<span class=\"token operator\">:</span> Encoder<span class=\"token punctuation\">[</span>Buff<span class=\"token punctuation\">]</span> <span class=\"token operator\">=</span> Encoders<span class=\"token punctuation\">.</span>product\n\n    <span class=\"token comment\">//输出的编码操作</span>\n    <span class=\"token keyword\">override</span> <span class=\"token keyword\">def</span> outputEncoder<span class=\"token operator\">:</span> Encoder<span class=\"token punctuation\">[</span><span class=\"token builtin\">Long</span><span class=\"token punctuation\">]</span> <span class=\"token operator\">=</span> Encoders<span class=\"token punctuation\">.</span>scalaLong\n  <span class=\"token punctuation\">&#125;</span>\n\n<span class=\"token comment\">//使用</span>\n spark<span class=\"token punctuation\">.</span>udf<span class=\"token punctuation\">.</span>register<span class=\"token punctuation\">(</span><span class=\"token string\">\"ageAvg\"</span><span class=\"token punctuation\">,</span> functions<span class=\"token punctuation\">.</span>udaf<span class=\"token punctuation\">(</span><span class=\"token keyword\">new</span> MyAvgUDTF<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n spark<span class=\"token punctuation\">.</span>sql<span class=\"token punctuation\">(</span><span class=\"token string\">\"select ageAvg(age) from user\"</span><span class=\"token punctuation\">)</span><span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n\n<p>在早期版本实现强类型的使用</p>\n<pre class=\"line-numbers language-scala\" data-language=\"scala\"><code class=\"language-scala\"><span class=\"token comment\">//定义</span>\n<span class=\"token keyword\">case</span> <span class=\"token keyword\">class</span> User<span class=\"token punctuation\">(</span>username<span class=\"token operator\">:</span> <span class=\"token builtin\">String</span><span class=\"token punctuation\">,</span> age<span class=\"token operator\">:</span> <span class=\"token builtin\">Long</span><span class=\"token punctuation\">)</span>\n\n\n<span class=\"token keyword\">case</span> <span class=\"token keyword\">class</span> Buff<span class=\"token punctuation\">(</span><span class=\"token keyword\">var</span> total<span class=\"token operator\">:</span><span class=\"token builtin\">Long</span><span class=\"token punctuation\">,</span> <span class=\"token keyword\">var</span> count<span class=\"token operator\">:</span><span class=\"token builtin\">Long</span><span class=\"token punctuation\">)</span>\n\n\n  <span class=\"token keyword\">class</span> MyAvgUDTF <span class=\"token keyword\">extends</span> Aggregator<span class=\"token punctuation\">[</span>User<span class=\"token punctuation\">,</span> Buff<span class=\"token punctuation\">,</span> <span class=\"token builtin\">Long</span><span class=\"token punctuation\">]</span> <span class=\"token punctuation\">&#123;</span>\n    <span class=\"token comment\">//zero 零值或者初始值</span>\n    <span class=\"token comment\">//缓冲区的初始化</span>\n    <span class=\"token keyword\">override</span> <span class=\"token keyword\">def</span> zero<span class=\"token operator\">:</span> Buff <span class=\"token operator\">=</span> <span class=\"token punctuation\">&#123;</span>\n      Buff<span class=\"token punctuation\">(</span><span class=\"token number\">0L</span><span class=\"token punctuation\">,</span> <span class=\"token number\">0L</span><span class=\"token punctuation\">)</span>\n    <span class=\"token punctuation\">&#125;</span>\n\n    <span class=\"token comment\">//根据输入的数据更新缓冲区的数据</span>\n    <span class=\"token keyword\">override</span> <span class=\"token keyword\">def</span> reduce<span class=\"token punctuation\">(</span>b<span class=\"token operator\">:</span> Buff<span class=\"token punctuation\">,</span> a<span class=\"token operator\">:</span> User<span class=\"token punctuation\">)</span><span class=\"token operator\">:</span> Buff <span class=\"token operator\">=</span> <span class=\"token punctuation\">&#123;</span>\n      b<span class=\"token punctuation\">.</span>total <span class=\"token operator\">=</span> b<span class=\"token punctuation\">.</span>total <span class=\"token operator\">+</span> a<span class=\"token punctuation\">.</span>age\n      b<span class=\"token punctuation\">.</span>count <span class=\"token operator\">=</span> b<span class=\"token punctuation\">.</span> count <span class=\"token operator\">+</span> <span class=\"token number\">1</span>\n      b\n    <span class=\"token punctuation\">&#125;</span>\n\n    <span class=\"token comment\">//合并缓冲区</span>\n    <span class=\"token keyword\">override</span> <span class=\"token keyword\">def</span> merge<span class=\"token punctuation\">(</span>b1<span class=\"token operator\">:</span> Buff<span class=\"token punctuation\">,</span> b2<span class=\"token operator\">:</span> Buff<span class=\"token punctuation\">)</span><span class=\"token operator\">:</span> Buff <span class=\"token operator\">=</span> <span class=\"token punctuation\">&#123;</span>\n      b1<span class=\"token punctuation\">.</span>total <span class=\"token operator\">=</span> b1<span class=\"token punctuation\">.</span>total <span class=\"token operator\">+</span> b2<span class=\"token punctuation\">.</span>total\n      b1<span class=\"token punctuation\">.</span>count <span class=\"token operator\">=</span> b1<span class=\"token punctuation\">.</span>count <span class=\"token operator\">+</span> b2<span class=\"token punctuation\">.</span>count\n      b1\n    <span class=\"token punctuation\">&#125;</span>\n\n    <span class=\"token comment\">//计算结果</span>\n    <span class=\"token keyword\">override</span> <span class=\"token keyword\">def</span> finish<span class=\"token punctuation\">(</span>reduction<span class=\"token operator\">:</span> Buff<span class=\"token punctuation\">)</span><span class=\"token operator\">:</span> <span class=\"token builtin\">Long</span> <span class=\"token operator\">=</span> <span class=\"token punctuation\">&#123;</span>\n      reduction<span class=\"token punctuation\">.</span>total <span class=\"token operator\">/</span> reduction<span class=\"token punctuation\">.</span>count\n    <span class=\"token punctuation\">&#125;</span>\n\n    <span class=\"token comment\">//下面两个几乎不用改变</span>\n    <span class=\"token comment\">//分布式计算，设计序列化的问题 encoder是编码 缓冲区的编码操作</span>\n    <span class=\"token keyword\">override</span> <span class=\"token keyword\">def</span> bufferEncoder<span class=\"token operator\">:</span> Encoder<span class=\"token punctuation\">[</span>Buff<span class=\"token punctuation\">]</span> <span class=\"token operator\">=</span> Encoders<span class=\"token punctuation\">.</span>product\n\n    <span class=\"token comment\">//输出的编码操作</span>\n    <span class=\"token keyword\">override</span> <span class=\"token keyword\">def</span> outputEncoder<span class=\"token operator\">:</span> Encoder<span class=\"token punctuation\">[</span><span class=\"token builtin\">Long</span><span class=\"token punctuation\">]</span> <span class=\"token operator\">=</span> Encoders<span class=\"token punctuation\">.</span>scalaLong\n  <span class=\"token punctuation\">&#125;</span>\n\n<span class=\"token comment\">//使用</span>\n<span class=\"token keyword\">val</span> df <span class=\"token operator\">=</span> spark<span class=\"token punctuation\">.</span>read<span class=\"token punctuation\">.</span>json<span class=\"token punctuation\">(</span><span class=\"token string\">\"datas/user.json\"</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">val</span> ds<span class=\"token operator\">:</span> Dataset<span class=\"token punctuation\">[</span>User<span class=\"token punctuation\">]</span> <span class=\"token operator\">=</span> df<span class=\"token punctuation\">.</span>as<span class=\"token punctuation\">[</span>User<span class=\"token punctuation\">]</span>\n\n<span class=\"token comment\">//将udaf转换为查询的列对象</span>\n<span class=\"token keyword\">val</span> udafCol <span class=\"token operator\">=</span> <span class=\"token keyword\">new</span> MyAvgUDTF<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>toColumn\nds<span class=\"token punctuation\">.</span>select<span class=\"token punctuation\">(</span>udafCol<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>show<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n\n<p>什么是df，</p>\n<p>什么是ds，</p>\n<p>为什么是这么写的，</p>\n<p>什么是列对象</p>\n<p>首先看得到是一个对象</p>\n<h2 id=\"2-8-数据的加载和保存\"><a href=\"#2-8-数据的加载和保存\" class=\"headerlink\" title=\"2.8 数据的加载和保存\"></a>2.8 数据的加载和保存</h2><p>1、读取</p>\n<pre class=\"line-numbers language-none\"><code class=\"language-none\">&#x2F;&#x2F;读取\nspark.read.laod|json|csv\n\n\n&#x2F;&#x2F;保存\ndf.write.sava(&quot;output&quot;)\ndf.write.format(&quot;json&quot;).sava(&quot;output1&quot;)\ndf.write.sava(&quot;output&quot;)<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n\n\n\n<p>如果读取不同格式的数据，可以对不同的数据格式进行设定 </p>\n<p><code>scala&gt; spark.read.format(&quot;…&quot;)[.option(&quot;…&quot;)].load(&quot;…&quot;) </code></p>\n<p>➢ format(“…”)：指定加载的数据类型，包括”csv”、”jdbc”、”json”、”orc”、”parquet”和 “textFile”。 </p>\n<p>➢ load(“…”)：在”csv”、”jdbc”、”json”、”orc”、”parquet”和”textFile”格式下需要传入加载 数据的路径。 </p>\n<p>➢ option(“…”)：在”jdbc”格式下需要传入 JDBC 相应参数，url、user、password 和 dbtable 我们前面都是使用 read API 先把文件加载到 DataFrame 然后再查询，其实，我们也可以直 接在文件上进行查询:  <span style=\"color:red\">文件格式.`文件路径`</span></p>\n<pre>spark.sql(\"select * from json.`/opt/module/data/user.json\\`\").show</pre>\n\n\n\n<p>2、保存数据 df.write.save 是保存数据的通用方法</p>\n<p> scala&gt;df.write. </p>\n<blockquote><span class=\"custom-blockquote-svg\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"\" xmlns=\"http://www.w3.org/2000/svg\" data-reactroot=\"\">\n<path fill=\"\" d=\"M22 12C22 6.5 17.5 2 12 2C6.5 2 2 6.5 2 12C2 17.5 6.5 22 12 22C13.8 22 15.5 21.5 17 20.6L22 22L20.7 17C21.5 15.5 22 13.8 22 12Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\" undefined=\"1\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M17 8.5C15.23 8.97 14.07 10.84 14.01 13.27C14 13.33 14 13.4 14 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M9 8.5C7.23 8.97 6.07 10.84 6.01 13.27C6 13.33 6 13.4 6 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\"></path>\n</svg>\n</span><p>csv jdbc json orc parquet textFile… … </p></blockquote>\n<p>如果保存不同格式的数据，可以对不同的数据格式进行设定</p>\n<pre class=\"line-numbers language-none\"><code class=\"language-none\">scala&gt;df.write.format(&quot;…&quot;)[.option(&quot;…&quot;)].save(&quot;…&quot;) <span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span></span></code></pre>\n\n<p>➢ format(“…”)：指定保存的数据类型，包括”csv”、”jdbc”、”json”、”orc”、”parquet”和 “textFile”。 </p>\n<p>➢ save (“…”)：在”csv”、”orc”、”parquet”和”textFile”格式下需要传入保存数据的路径。 </p>\n<p>➢ option(“…”)：在”jdbc”格式下需要传入 JDBC 相应参数，url、user、password 和 dbtable 保存操作可以使用 SaveMode, 用来指明如何处理数据，使用 mode()方法来设置。 有一点很重要: <strong>这些 SaveMode 都是没有加锁的, 也不是原子操作</strong>。</p>\n<p>SaveMode 是一个枚举类，其中的常量包括：</p>\n<table>\n<thead>\n<tr>\n<th>Scala&#x2F;Java</th>\n<th>Any Language</th>\n<th>Meaning</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>SaveMode.ErrorIfExists(default)</td>\n<td>“error”(default)</td>\n<td>如果文件已经存在则抛出异常</td>\n</tr>\n<tr>\n<td>SaveMode.Append</td>\n<td>“append”</td>\n<td>如果文件已经存在则追加</td>\n</tr>\n<tr>\n<td>SaveMode.Overwrite</td>\n<td>“overwrite”</td>\n<td>如果文件已经存在则覆盖</td>\n</tr>\n<tr>\n<td>SaveMode.Ignore</td>\n<td>“ignore”</td>\n<td>如果文件已经存在则忽略</td>\n</tr>\n</tbody></table>\n<pre class=\"line-numbers language-none\"><code class=\"language-none\">df.write.mode(&quot;append&quot;).json(&quot;&#x2F;opt&#x2F;module&#x2F;data&#x2F;output&quot;)<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span></span></code></pre>\n\n<h3 id=\"2-8-2-Parquet\"><a href=\"#2-8-2-Parquet\" class=\"headerlink\" title=\"2.8.2 Parquet\"></a>2.8.2 Parquet</h3><p>Spark SQL 的默认数据源为 Parquet 格式。Parquet 是一种能够有效存储嵌套数据的列式 存储格式。 数据源为 Parquet 文件时，Spark SQL 可以方便的执行所有的操作，不需要使用 format。 修改配置项 spark.sql.sources.default，可修改默认数据源格式。</p>\n<p>1、加载数据 </p>\n<pre class=\"line-numbers language-none\"><code class=\"language-none\">scala&gt; val df &#x3D; spark.read.load(&quot;examples&#x2F;src&#x2F;main&#x2F;resources&#x2F;users.parquet&quot;)\nscala&gt; df.show <span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span></span></code></pre>\n\n<p>2、保存数据 </p>\n<pre class=\"line-numbers language-none\"><code class=\"language-none\">scala&gt; var df &#x3D; spark.read.json(&quot;&#x2F;opt&#x2F;module&#x2F;data&#x2F;input&#x2F;people.json&quot;) &#x2F;&#x2F;保存为 parquet 格式 scala&gt; df.write.mode(&quot;append&quot;).save(&quot;&#x2F;opt&#x2F;module&#x2F;data&#x2F;output&quot;)<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span></span></code></pre>\n\n<h3 id=\"2-8-3-JSON\"><a href=\"#2-8-3-JSON\" class=\"headerlink\" title=\"2.8.3 JSON\"></a>2.8.3 JSON</h3><p>Spark SQL 能够自动推测 JSON 数据集的结构，并将它加载为一个 Dataset[Row]. 可以 通过 SparkSession.read.json()去加载 JSON 文件。</p>\n<p><span style=\"color:red\">注意：Spark 读取的 JSON 文件不是传统的 JSON 文件，每一行都应该是一个 JSON 串</span></p>\n<p>格 式如下：</p>\n<blockquote><span class=\"custom-blockquote-svg\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"\" xmlns=\"http://www.w3.org/2000/svg\" data-reactroot=\"\">\n<path fill=\"\" d=\"M22 12C22 6.5 17.5 2 12 2C6.5 2 2 6.5 2 12C2 17.5 6.5 22 12 22C13.8 22 15.5 21.5 17 20.6L22 22L20.7 17C21.5 15.5 22 13.8 22 12Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\" undefined=\"1\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M17 8.5C15.23 8.97 14.07 10.84 14.01 13.27C14 13.33 14 13.4 14 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M9 8.5C7.23 8.97 6.07 10.84 6.01 13.27C6 13.33 6 13.4 6 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\"></path>\n</svg>\n</span><p>{“name”:”Michael”} </p>\n<p>{“name”:”Andy”， “age”:30}</p>\n<p>[{“name”:”Justin”， “age”:19},{“name”:”Justin”， “age”:19}]</p></blockquote>\n<p>1）导入隐式转换</p>\n<p><code>import spark.implicits._</code></p>\n<p>2）加载 JSON 文件 </p>\n<pre class=\"line-numbers language-none\"><code class=\"language-none\">val path &#x3D; &quot;&#x2F;opt&#x2F;module&#x2F;spark-local&#x2F;people.json&quot; \n\nval peopleDF &#x3D; spark.read.json(path) <span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span></span></code></pre>\n\n<p>3）创建临时表 <code>peopleDF.createOrReplaceTempView(&quot;people&quot;) </code></p>\n<p>4）数据查询 </p>\n<pre class=\"line-numbers language-none\"><code class=\"language-none\">val teenagerNamesDF &#x3D; spark.sql(&quot;SELECT name FROM people WHERE age BETWEEN 13  AND 19&quot;) \nteenagerNamesDF.show()\n\n+------+\n| name|\n+------+\n|Justin|\n+------+<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n\n<h3 id=\"2-8-4-CSV\"><a href=\"#2-8-4-CSV\" class=\"headerlink\" title=\"2.8.4 CSV\"></a>2.8.4 CSV</h3><p>Spark SQL 可以配置 CSV 文件的列表信息，读取 CSV 文件,CSV 文件的第一行设置为 数据列</p>\n<pre class=\"line-numbers language-none\"><code class=\"language-none\">spark.read\n\t.format(&quot;csv&quot;)\n\t.option(&quot;sep&quot;, &quot;;&quot;) &#x2F;&#x2F;分隔符，一般都是，\n\t.option(&quot;inferSchema&quot;,  &quot;true&quot;) &#x2F;&#x2F;不知道了\n\t.option(&quot;header&quot;, &quot;true&quot;) &#x2F;&#x2F;数据的第一行是列名\n\t.load(&quot;data&#x2F;user.csv&quot;)<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n\n<h3 id=\"2-8-5-MySQL\"><a href=\"#2-8-5-MySQL\" class=\"headerlink\" title=\"2.8.5 MySQL\"></a>2.8.5 MySQL</h3><p>Spark SQL 可以通过 JDBC 从关系型数据库中读取数据的方式创建 DataFrame，通过对 DataFrame 一系列的计算后，还可以将数据再写回关系型数据库中。如果使用 spark-shell 操 作，可在启动 shell 时指定相关的数据库驱动路径或者将相关的数据库驱动放到 spark 的类 路径下。 </p>\n<p>bin&#x2F;spark-shell  –jars mysql-connector-java-5.1.27-bin.jar </p>\n<p>在 Idea 中通过 JDBC 对 Mysql 进行操作 </p>\n<p>1）导入依赖</p>\n<pre class=\"line-numbers language-xml\" data-language=\"xml\"><code class=\"language-xml\"><span class=\"token tag\"><span class=\"token tag\"><span class=\"token punctuation\">&lt;</span>dependency</span><span class=\"token punctuation\">></span></span>\n <span class=\"token tag\"><span class=\"token tag\"><span class=\"token punctuation\">&lt;</span>groupId</span><span class=\"token punctuation\">></span></span>mysql<span class=\"token tag\"><span class=\"token tag\"><span class=\"token punctuation\">&lt;/</span>groupId</span><span class=\"token punctuation\">></span></span>\n <span class=\"token tag\"><span class=\"token tag\"><span class=\"token punctuation\">&lt;</span>artifactId</span><span class=\"token punctuation\">></span></span>mysql-connector-java<span class=\"token tag\"><span class=\"token tag\"><span class=\"token punctuation\">&lt;/</span>artifactId</span><span class=\"token punctuation\">></span></span>\n <span class=\"token tag\"><span class=\"token tag\"><span class=\"token punctuation\">&lt;</span>version</span><span class=\"token punctuation\">></span></span>5.1.27<span class=\"token tag\"><span class=\"token tag\"><span class=\"token punctuation\">&lt;/</span>version</span><span class=\"token punctuation\">></span></span>\n<span class=\"token tag\"><span class=\"token tag\"><span class=\"token punctuation\">&lt;/</span>dependency</span><span class=\"token punctuation\">></span></span><span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n\n<p>2）读取数据</p>\n<pre class=\"line-numbers language-none\"><code class=\"language-none\">val conf: SparkConf &#x3D; new SparkConf().setMaster(&quot;local[*]&quot;).setAppName(&quot;SparkSQL&quot;)\n&#x2F;&#x2F;创建 SparkSession 对象\nval spark: SparkSession &#x3D; SparkSession.builder().config(conf).getOrCreate()\nimport spark.implicits._\n\n\n&#x2F;&#x2F;方式 1：通用的 load 方法读取\nspark.read.format(&quot;jdbc&quot;)\n .option(&quot;url&quot;, &quot;jdbc:mysql:&#x2F;&#x2F;hadoop102:3306&#x2F;spark-sql&quot;)\n .option(&quot;driver&quot;, &quot;com.mysql.jdbc.Driver&quot;)\n .option(&quot;user&quot;, &quot;root&quot;)\n .option(&quot;password&quot;, &quot;123123&quot;)\n .option(&quot;dbtable&quot;, &quot;user&quot;)\n .load().show\n \n&#x2F;&#x2F;方式 2:通用的 load 方法读取 参数另一种形式\nspark.read.format(&quot;jdbc&quot;)\n .options(\n    Map(\n     &quot;url&quot;-&gt;&quot;jdbc:mysql:&#x2F;&#x2F;hadoop:3306&#x2F;spark-sql?user&#x3D;atguigu&amp;password&#x3D;111111&quot;,\n     &quot;dbtable&quot;-&gt;&quot;user&quot;,\n     &quot;driver&quot;-&gt;&quot;com.mysql.jdbc.Driver&quot;\n \t)\n )\n .load()\n .show()\n\n&#x2F;&#x2F;方式 3:使用 jdbc 方法读取\nval props: Properties &#x3D; new Properties()\nprops.setProperty(&quot;user&quot;, &quot;root&quot;)\nprops.setProperty(&quot;password&quot;, &quot;123123&quot;)\nval df: DataFrame &#x3D; spark.read.jdbc(&quot;jdbc:mysql:&#x2F;&#x2F;hadoop102:3306&#x2F;spark-sql&quot;, &quot;user&quot;, props)\ndf.show\n\n&#x2F;&#x2F;释放资源\nspark.stop()\n<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n\n<p>3）写入数据</p>\n<pre class=\"line-numbers language-none\"><code class=\"language-none\">case class User(name: String, age: Long)\n\nval conf: SparkConf &#x3D; new SparkConf().setMaster(&quot;local[*]&quot;).setAppName(&quot;SparkSQL&quot;)\n&#x2F;&#x2F;创建 SparkSession 对象\nval spark: SparkSession &#x3D; SparkSession.builder().config(conf).getOrCreate()\nimport spark.implicits._\n\n\nval rdd: RDD[User2] &#x3D; spark.sparkContext.makeRDD(List(User(&quot;lisi&quot;, 20), User(&quot;zs&quot;, 30)))\nval ds: Dataset[User2] &#x3D; rdd.toDS\n\n&#x2F;&#x2F;方式 1：通用的方式 format 指定写出类型\nds.write\n .format(&quot;jdbc&quot;)\n .option(&quot;url&quot;, &quot;jdbc:mysql:&#x2F;&#x2F;linux1:3306&#x2F;spark-sql&quot;)\n .option(&quot;user&quot;, &quot;root&quot;)\n .option(&quot;password&quot;, &quot;123123&quot;)\n .option(&quot;dbtable&quot;, &quot;user&quot;)\n .mode(SaveMode.Append)\n .save()\n \n \n&#x2F;&#x2F;方式 2：通过 jdbc 方法\nval props: Properties &#x3D; new Properties()\nprops.setProperty(&quot;user&quot;, &quot;root&quot;)\nprops.setProperty(&quot;password&quot;, &quot;123123&quot;)\nds.write\n.mode(SaveMode.Append)\n.jdbc(&quot;jdbc:mysql:&#x2F;&#x2F;linux1:3306&#x2F;spark-sql&quot;, &quot;user&quot;, props)\n\n&#x2F;&#x2F;释放资源\nspark.stop()\n<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n\n<h3 id=\"2-8-6-Hive\"><a href=\"#2-8-6-Hive\" class=\"headerlink\" title=\"2.8.6 Hive\"></a>2.8.6 Hive</h3><p>2）外部的 HIVE</p>\n<p>如果想连接外部已经部署好的 Hive，需要通过以下几个步骤： </p>\n<p>➢ Spark 要接管 Hive 需要把 hive-site.xml 拷贝到 spark目录下的conf&#x2F;目录下 </p>\n<p>➢ 把 Mysql 的驱动 copy 到 jars&#x2F;目录下 </p>\n<p>➢ 如果访问不到 hdfs，则需要把 core-site.xml 和 hdfs-site.xml 拷贝到 conf&#x2F;目录下 </p>\n<p>➢ 重启 spark-shell</p>\n<p>3）运行 Spark SQL CLI</p>\n<p>类似于Hive的交互命令行，但是很难看</p>\n<p><code>bin/spark-sql</code></p>\n<p>4）运行 Spark beeline</p>\n<p>比上面的要好看一些</p>\n<p>如果想连接 Thrift Server，需要通过以下几个步骤： </p>\n<p>➢ Spark 要接管 Hive 需要把 hive-site.xml 拷贝到 conf&#x2F;目录下 </p>\n<p>➢ 把 Mysql 的驱动 copy 到 jars&#x2F;目录下 </p>\n<p>➢ 如果访问不到 hdfs，则需要把 core-site.xml 和 hdfs-site.xml 拷贝到 conf&#x2F;目录下 </p>\n<p>➢ 启动 Thrift Server <code>sbin/start-thriftserver.sh</code></p>\n<p>➢ 使用 beeline 连接 Thrift Server  <code>bin/beeline -u jdbc:hive2://hadoop102:10000 -n atguigu</code></p>\n<p>5）代码操作 Hive</p>\n<p>1、导入依赖</p>\n<pre class=\"line-numbers language-xml\" data-language=\"xml\"><code class=\"language-xml\"><span class=\"token tag\"><span class=\"token tag\"><span class=\"token punctuation\">&lt;</span>dependency</span><span class=\"token punctuation\">></span></span>\n <span class=\"token tag\"><span class=\"token tag\"><span class=\"token punctuation\">&lt;</span>groupId</span><span class=\"token punctuation\">></span></span>org.apache.spark<span class=\"token tag\"><span class=\"token tag\"><span class=\"token punctuation\">&lt;/</span>groupId</span><span class=\"token punctuation\">></span></span>\n <span class=\"token tag\"><span class=\"token tag\"><span class=\"token punctuation\">&lt;</span>artifactId</span><span class=\"token punctuation\">></span></span>spark-hive_2.12<span class=\"token tag\"><span class=\"token tag\"><span class=\"token punctuation\">&lt;/</span>artifactId</span><span class=\"token punctuation\">></span></span>\n <span class=\"token tag\"><span class=\"token tag\"><span class=\"token punctuation\">&lt;</span>version</span><span class=\"token punctuation\">></span></span>3.0.0<span class=\"token tag\"><span class=\"token tag\"><span class=\"token punctuation\">&lt;/</span>version</span><span class=\"token punctuation\">></span></span>\n<span class=\"token tag\"><span class=\"token tag\"><span class=\"token punctuation\">&lt;/</span>dependency</span><span class=\"token punctuation\">></span></span>\n<span class=\"token tag\"><span class=\"token tag\"><span class=\"token punctuation\">&lt;</span>dependency</span><span class=\"token punctuation\">></span></span>\n <span class=\"token tag\"><span class=\"token tag\"><span class=\"token punctuation\">&lt;</span>groupId</span><span class=\"token punctuation\">></span></span>org.apache.hive<span class=\"token tag\"><span class=\"token tag\"><span class=\"token punctuation\">&lt;/</span>groupId</span><span class=\"token punctuation\">></span></span>\n <span class=\"token tag\"><span class=\"token tag\"><span class=\"token punctuation\">&lt;</span>artifactId</span><span class=\"token punctuation\">></span></span>hive-exec<span class=\"token tag\"><span class=\"token tag\"><span class=\"token punctuation\">&lt;/</span>artifactId</span><span class=\"token punctuation\">></span></span>\n <span class=\"token tag\"><span class=\"token tag\"><span class=\"token punctuation\">&lt;</span>version</span><span class=\"token punctuation\">></span></span>1.2.1<span class=\"token tag\"><span class=\"token tag\"><span class=\"token punctuation\">&lt;/</span>version</span><span class=\"token punctuation\">></span></span>\n<span class=\"token tag\"><span class=\"token tag\"><span class=\"token punctuation\">&lt;/</span>dependency</span><span class=\"token punctuation\">></span></span>\n<span class=\"token tag\"><span class=\"token tag\"><span class=\"token punctuation\">&lt;</span>dependency</span><span class=\"token punctuation\">></span></span>\n <span class=\"token tag\"><span class=\"token tag\"><span class=\"token punctuation\">&lt;</span>groupId</span><span class=\"token punctuation\">></span></span>mysql<span class=\"token tag\"><span class=\"token tag\"><span class=\"token punctuation\">&lt;/</span>groupId</span><span class=\"token punctuation\">></span></span>\n <span class=\"token tag\"><span class=\"token tag\"><span class=\"token punctuation\">&lt;</span>artifactId</span><span class=\"token punctuation\">></span></span>mysql-connector-java<span class=\"token tag\"><span class=\"token tag\"><span class=\"token punctuation\">&lt;/</span>artifactId</span><span class=\"token punctuation\">></span></span>\n <span class=\"token tag\"><span class=\"token tag\"><span class=\"token punctuation\">&lt;</span>version</span><span class=\"token punctuation\">></span></span>5.1.27<span class=\"token tag\"><span class=\"token tag\"><span class=\"token punctuation\">&lt;/</span>version</span><span class=\"token punctuation\">></span></span>\n<span class=\"token tag\"><span class=\"token tag\"><span class=\"token punctuation\">&lt;/</span>dependency</span><span class=\"token punctuation\">></span></span><span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n\n<p>2、将 hive-site.xml 文件拷贝到项目的 resources 目录中</p>\n<pre class=\"line-numbers language-none\"><code class=\"language-none\">&#x2F;&#x2F;创建 SparkSession\nval spark: SparkSession &#x3D; SparkSession\n .builder()\n .enableHiveSupport() &#x2F;&#x2F;启动hive支持\n .master(&quot;local[*]&quot;)\n .appName(&quot;sql&quot;)\n .getOrCreate()\n<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n\n<p>注意：在开发工具中创建数据库默认是在本地仓库，通过参数修改数据库仓库的地址:  config(“spark.sql.warehouse.dir”, “hdfs:&#x2F;&#x2F;linux1:8020&#x2F;user&#x2F;hive&#x2F;warehouse”) 如果在执行操作时，出现如下错误：</p>\n<p><img src=\"C:\\Users\\Cencus\\AppData\\Roaming\\Typora\\typora-user-images\\image-20220417235914329.png\" alt=\"image-20220417235914329\"></p>\n<p>可以代码最前面增加如下代码解决： <code>System.setProperty(&quot;HADOOP_USER_NAME&quot;, &quot;root&quot;)</code></p>\n<p>如：</p>\n<pre class=\"line-numbers language-none\"><code class=\"language-none\"><span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span></span></code></pre>\n\n<h1 id=\"第3章-SparkSQL-项目实战\"><a href=\"#第3章-SparkSQL-项目实战\" class=\"headerlink\" title=\"第3章 SparkSQL 项目实战\"></a>第3章 SparkSQL 项目实战</h1>","text":"第1章 SparkSQL 概述1.3 SparkSQL 特点1.3.1 易整合无缝的整合了 SQL 查询和 Spark 编程 1.3.2 统一的数据访问使用相同的方式连接不同的数据源 1.3.3 兼容Hive 在已有的仓库上直接运行 SQL 或者 HiveQL 1.3.4 标准数...","link":"","photos":[],"count_time":{"symbolsCount":"16k","symbolsTime":"15 mins."},"categories":[],"tags":[{"name":"spark","slug":"spark","count":2,"path":"api/tags/spark.json"}],"toc":"<ol class=\"toc\"><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#%E7%AC%AC1%E7%AB%A0-SparkSQL-%E6%A6%82%E8%BF%B0\"><span class=\"toc-text\">第1章 SparkSQL 概述</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#1-3-SparkSQL-%E7%89%B9%E7%82%B9\"><span class=\"toc-text\">1.3 SparkSQL 特点</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#1-3-1-%E6%98%93%E6%95%B4%E5%90%88\"><span class=\"toc-text\">1.3.1 易整合</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#1-3-2-%E7%BB%9F%E4%B8%80%E7%9A%84%E6%95%B0%E6%8D%AE%E8%AE%BF%E9%97%AE\"><span class=\"toc-text\">1.3.2 统一的数据访问</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#1-3-3-%E5%85%BC%E5%AE%B9\"><span class=\"toc-text\">1.3.3 兼容</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#1-3-4-%E6%A0%87%E5%87%86%E6%95%B0%E6%8D%AE%E8%BF%9E%E6%8E%A5\"><span class=\"toc-text\">1.3.4 标准数据连接</span></a></li></ol></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#1-4-DataFrame-%E6%98%AF%E4%BB%80%E4%B9%88\"><span class=\"toc-text\">1.4 DataFrame 是什么</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#1-5-DataSet-%E6%98%AF%E4%BB%80%E4%B9%88\"><span class=\"toc-text\">1.5 DataSet 是什么</span></a></li></ol></li><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#%E7%AC%AC2%E7%AB%A0-SparkSQL-%E6%A0%B8%E5%BF%83%E7%BC%96%E7%A8%8B\"><span class=\"toc-text\">第2章 SparkSQL 核心编程</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#2-1-%E6%96%B0%E7%9A%84%E8%B5%B7%E7%82%B9\"><span class=\"toc-text\">2.1 新的起点</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#2-2-DataFrame\"><span class=\"toc-text\">2.2 DataFrame</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#2-2-1-%E5%88%9B%E5%BB%BA-DataFrame\"><span class=\"toc-text\">2.2.1 创建 DataFrame</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#2-2-2-SQL-%E8%AF%AD%E6%B3%95\"><span class=\"toc-text\">2.2.2 SQL 语法</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#2-2-3-DSL-%E8%AF%AD%E6%B3%95\"><span class=\"toc-text\">2.2.3 DSL 语法</span></a></li></ol></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#2-3-DataSet\"><span class=\"toc-text\">2.3 DataSet</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#2-3-1-%E5%88%9B%E5%BB%BA-DataSet\"><span class=\"toc-text\">2.3.1 创建 DataSet</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#2-3-2-RDD-%E8%BD%AC%E6%8D%A2%E4%B8%BA-DataSet\"><span class=\"toc-text\">2.3.2 RDD 转换为 DataSet</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#2-3-3-DataSet-%E8%BD%AC%E6%8D%A2%E4%B8%BA-RDD\"><span class=\"toc-text\">2.3.3 DataSet 转换为 RDD</span></a></li></ol></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#2-4-DataFrame-%E5%92%8C-DataSet-%E8%BD%AC%E6%8D%A2\"><span class=\"toc-text\">2.4 DataFrame 和 DataSet 转换</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#2-5-RDD%E3%80%81DataFrame%E3%80%81DataSet-%E4%B8%89%E8%80%85%E7%9A%84%E5%85%B3%E7%B3%BB\"><span class=\"toc-text\">2.5 RDD、DataFrame、DataSet 三者的关系</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#2-5-1-%E4%B8%89%E8%80%85%E7%9A%84%E5%85%B1%E6%80%A7\"><span class=\"toc-text\">2.5.1 三者的共性</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#2-5-2-%E4%B8%89%E8%80%85%E7%9A%84%E5%8C%BA%E5%88%AB\"><span class=\"toc-text\">2.5.2 三者的区别</span></a></li></ol></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#2-6-IDEA-%E5%BC%80%E5%8F%91-SparkSQL\"><span class=\"toc-text\">2.6 IDEA 开发 SparkSQL</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#2-7-%E7%94%A8%E6%88%B7%E8%87%AA%E5%AE%9A%E4%B9%89%E5%87%BD%E6%95%B0\"><span class=\"toc-text\">2.7 用户自定义函数</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#2-7-1-UDF\"><span class=\"toc-text\">2.7.1 UDF</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#2-7-2-UDAF\"><span class=\"toc-text\">2.7.2 UDAF</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#1%E3%80%81%E5%BC%B1%E7%B1%BB%E5%9E%8B\"><span class=\"toc-text\">1、弱类型</span></a></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#2%E3%80%81%E5%BC%BA%E7%B1%BB%E5%9E%8B\"><span class=\"toc-text\">2、强类型</span></a></li></ol></li></ol></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#2-8-%E6%95%B0%E6%8D%AE%E7%9A%84%E5%8A%A0%E8%BD%BD%E5%92%8C%E4%BF%9D%E5%AD%98\"><span class=\"toc-text\">2.8 数据的加载和保存</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#2-8-2-Parquet\"><span class=\"toc-text\">2.8.2 Parquet</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#2-8-3-JSON\"><span class=\"toc-text\">2.8.3 JSON</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#2-8-4-CSV\"><span class=\"toc-text\">2.8.4 CSV</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#2-8-5-MySQL\"><span class=\"toc-text\">2.8.5 MySQL</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#2-8-6-Hive\"><span class=\"toc-text\">2.8.6 Hive</span></a></li></ol></li></ol></li><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#%E7%AC%AC3%E7%AB%A0-SparkSQL-%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%88%98\"><span class=\"toc-text\">第3章 SparkSQL 项目实战</span></a></li></ol>","author":{"name":"Cencus","slug":"blog-author","avatar":"https://gitee.com/cencus/blog-image/raw/master/blogImage/1648381409561.jpg","link":"/","description":"stay hungry, stay foolish.","socials":{"github":"","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{}}},"mapped":true,"prev_post":{"title":"maven安装配置","uid":"29fb3d0ec9f011ab95928547f0ea4133","slug":"maven安装配置","date":"2022-04-17T02:59:20.000Z","updated":"2022-04-17T03:23:00.337Z","comments":true,"path":"api/articles/maven安装配置.json","keywords":null,"cover":null,"text":"&lt;mirror> &lt;id>nexus-aliyun&lt;/id> &lt;mirrorOf>central&lt;/mirrorOf> &lt;name>Nexus aliyun&lt;/name> &lt;url>http://maven.aliyun.com/n...","link":"","photos":[],"count_time":{"symbolsCount":753,"symbolsTime":"1 mins."},"categories":[],"tags":[],"author":{"name":"Cencus","slug":"blog-author","avatar":"https://gitee.com/cencus/blog-image/raw/master/blogImage/1648381409561.jpg","link":"/","description":"stay hungry, stay foolish.","socials":{"github":"","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{}}},"feature":true},"next_post":{"title":"sqoop","uid":"2fc87cccb261708e4dc237f0341aa59b","slug":"sqoop","date":"2022-04-14T10:50:02.000Z","updated":"2022-04-14T11:01:14.779Z","comments":true,"path":"api/articles/sqoop.json","keywords":null,"cover":null,"text":"sql to hadoop 使用1 翻译成MapReduce对inputformat和outputformat进行定制 安装 下载：http://archive.apache.org/dist/sqoop/1.4.6/ tar -zxvf -C &#x2F;opt&#x2F;mo...","link":"","photos":[],"count_time":{"symbolsCount":808,"symbolsTime":"1 mins."},"categories":[],"tags":[],"author":{"name":"Cencus","slug":"blog-author","avatar":"https://gitee.com/cencus/blog-image/raw/master/blogImage/1648381409561.jpg","link":"/","description":"stay hungry, stay foolish.","socials":{"github":"","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{}}}}}