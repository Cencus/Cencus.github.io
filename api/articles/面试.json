{"title":"面试","uid":"b8cee04dc0368bcbafd67939edc1c0b9","slug":"面试","date":"2022-05-01T02:58:09.000Z","updated":"2022-05-03T08:45:05.182Z","comments":true,"path":"api/articles/面试.json","keywords":null,"cover":[],"content":"<p><strong>由大到小，由粗到细</strong></p>\n<h1 id=\"一、Linux-amp-Shell总结\"><a href=\"#一、Linux-amp-Shell总结\" class=\"headerlink\" title=\"一、Linux &amp; Shell总结\"></a>一、Linux &amp; Shell总结</h1><p>ps -ef</p>\n<p>只执行ps命令，默认是显示当前控制台下属于当前用户的进程；</p>\n<p>参数 -e 显示运行在系统上的所有进程</p>\n<p>参数 -f 扩展显示输出</p>\n<p>UID      启动进程的用户</p>\n<p>PID      进程的进程号</p>\n<p>PPID    父进程进程号</p>\n<p>C          cpu使用率</p>\n<p>STIME   进程启动时的系统时间</p>\n<p>TTY       进程启动时终端设备</p>\n<p>TIME     运行进程需要的累积CPU时间</p>\n<p>CMD   启动程序名称或命令</p>\n<p>df -h</p>\n<p>top</p>\n<p>iotop</p>\n<p>rpm -ivh</p>\n<p>netstat</p>\n<p>写过哪些脚本</p>\n<p>1、启动停止脚本、分发</p>\n<pre class=\"line-numbers language-shell\" data-language=\"shell\"><code class=\"language-shell\">#!bin&#x2F;bash\n\ncase $1 in\n&quot;start&quot;)&#123;\n    for i in hadoop102 hadoop103 hadoop104\n    do\n        ssh $i &quot;启动命令的绝对路径&quot;\n    done\n&#125;;;\n&quot;stop&quot;)&#123;\n    for i in hadoop102 hadoop103 hadoop104\n    do\n        ssh $i &quot;启动命令的绝对路径&quot;\n    done\n&#125;;;\nesac<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n\n\n\n\n\n<p>2、数仓层级内部导入</p>\n<p>ods &#x3D;&gt; dwd &#x3D;&gt; dws &#x3D;&gt; dwt &#x3D;&gt; ads</p>\n<pre class=\"line-numbers language-none\"><code class=\"language-none\">#!bin&#x2F;bash\n\n# 1. 定义变量 APP\n\n# 2. 获取时间\n\n# 3.\n# sql &#x3D; &quot;\n#     具体的sql；（先写一天的，然后在表名前面加上$APP,将时间换成$do_date）\n# &quot;\n\n# 4. 执行sql<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n\n\n\n\n\n<p>3、数仓与mysql的导入导出</p>\n<p>mysql sqoop hdfs</p>\n<p>shell 常用工具</p>\n<p>awk <a href=\"https://www.cnblogs.com/wwwdcsxudcom/p/15888073.html\">https://www.cnblogs.com/wwwdcsxudcom/p/15888073.html</a></p>\n<p>sed</p>\n<p>sort</p>\n<p>cut</p>\n<p>shell中单引号和双引号的区别</p>\n<p>单引号：’’在引号内部的变量不能解析里面变量对应的值，例如’$do_date’直接打印$do_date</p>\n<p>双引号：”$do_date”在双引号内部能够取出变量的值</p>\n<p>嵌套：看谁在最外层，”  ‘$do_date’  “可以取出。’ “$do_date” ‘不可以取出</p>\n<h1 id=\"Hadoop\"><a href=\"#Hadoop\" class=\"headerlink\" title=\"Hadoop\"></a>Hadoop</h1><h2 id=\"入门\"><a href=\"#入门\" class=\"headerlink\" title=\"入门\"></a>入门</h2><p>常用端口号</p>\n<table>\n<thead>\n<tr>\n<th>2.x端口</th>\n<th>3.x端口</th>\n<th>作用</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>50070</td>\n<td>9870</td>\n<td>HDFSWeb页面</td>\n</tr>\n<tr>\n<td></td>\n<td>19888</td>\n<td>历史服务器</td>\n</tr>\n<tr>\n<td></td>\n<td>8020&#x2F;9000</td>\n<td>客户端访问端口</td>\n</tr>\n<tr>\n<td></td>\n<td>8088</td>\n<td>MapReduce执行作业</td>\n</tr>\n</tbody></table>\n<p>安装配置文件</p>\n<p>2.x</p>\n<p>core-site.xml</p>\n<p>hdfs-site.xml</p>\n<p>mapred-site.xml</p>\n<p>yarn-site.xml</p>\n<p>slaves</p>\n<p>3.x</p>\n<p>core-site.xml</p>\n<p>hdfs-site.xml</p>\n<p>mapred-site.xml</p>\n<p>yarn-site.xml</p>\n<p>workers</p>\n<h2 id=\"HDFS\"><a href=\"#HDFS\" class=\"headerlink\" title=\"HDFS\"></a>HDFS</h2><p>1. </p>\n<p>2. </p>\n<ol start=\"3\">\n<li>HDFS默认有几个副本</li>\n</ol>\n<p>​            3个</p>\n<ol start=\"4\">\n<li>HDFS块大小</li>\n</ol>\n<table>\n<thead>\n<tr>\n<th>方面</th>\n<th>大小</th>\n<th></th>\n</tr>\n</thead>\n<tbody><tr>\n<td>2.x 3.x</td>\n<td>默认块 128m</td>\n<td></td>\n</tr>\n<tr>\n<td>2.x本地模式</td>\n<td>32m（看过源码）</td>\n<td></td>\n</tr>\n<tr>\n<td>1.x 块大小</td>\n<td>64m</td>\n<td></td>\n</tr>\n<tr>\n<td>hive的块大小</td>\n<td>256m</td>\n<td></td>\n</tr>\n<tr>\n<td>大仓企业块大小</td>\n<td>256m</td>\n<td></td>\n</tr>\n</tbody></table>\n<p>块大小取决于，服务器之间传输通信速度（速度越快，块大小越大）</p>\n<h2 id=\"MapReduce\"><a href=\"#MapReduce\" class=\"headerlink\" title=\"MapReduce\"></a>MapReduce</h2><p>shuffle及其优化</p>\n<p>shuffle是map方法之后，reduce方法之前，混洗的过程</p>\n<p>压缩</p>\n<p><img src=\"https://gitee.com/cencus/blog-image/raw/master/blogImage/1651377730092.png\"></p>\n<p>各项内存</p>\n<p><img src=\"https://gitee.com/cencus/blog-image/raw/master/blogImage/1651378355089.png\"></p>\n<h2 id=\"YARN\"><a href=\"#YARN\" class=\"headerlink\" title=\"YARN\"></a>YARN</h2><p>1、FIFO、容量调度器、公平调度器</p>\n<p>2、默认：</p>\n<ul>\n<li><p>Apache：默认容量</p>\n</li>\n<li><p>CDH：默认公平</p>\n</li>\n</ul>\n<p>3、</p>\n<p>FIFO：支持单队列，先进先出，同一时间只有一个任务执行，并发度非常低，在企业里面不会使用</p>\n<p>容量：支持多队列，底层由多个FIFO调度器组成，优先满足先进队列的任务，资源不够的时候会先其他队列借资源，其他队列需要该资源时会直接抢过来，自己不够找帮手，帮手没事还好，有事帮手就不会帮你干，并发度一般</p>\n<p>公平：支持多队列，同一时间把所有任务全部启动，所有任务公平享有获得资源的权利，雨露均沾，并发度最高，</p>\n<p>6、如何选择：电脑服务器性能较好，对并发度要求比较高选择公平调度器（上市公司，大仓），如果电脑服务器比较差，对并发度要求不是特别高，就可以选择容量调度器（中小型公司）</p>\n<p>7、在企业开发时如何创建队列</p>\n<p>容量调度器默认就一个default队列，其他队列需要自己去创建，如何创建</p>\n<ul>\n<li><p>按照执行任务的框架引擎：hive、spark、flink，等等队列</p>\n</li>\n<li><p>按照业务模块创建（较多使用）：登录注册、订单、物流…</p>\n</li>\n</ul>\n<p>降级使用：在某些极端场景下，例如双11，那时候登录注册的人会很少，但是订单物流都会很多，就可以直接不允许注册登录，把那个队列的资源分配给别人使用，优先保证核心任务的正常执行</p>\n<p>8、Yarn的工作机制（笔试）</p>\n<p>管借账的</p>\n<p>job，向集群提交一个applicationid，然后给你一个泰泰，然你准备材料，你放到台台上，然后告诉rm我提交好了，再申请运行appmaster整个job的老大，管理整个作业，然后就很多人都在提交，放在了队列上，刚刚说的，然后按照调度策略，进行调度，以容量为例，然后先发钱，发个nm的容器，作业就在容器里面开启applicationmaster，appmaseter去读刚刚提交的材料，发现有两个切片，于是向rm申请我要运行两个maptask，然后申请了两份资源（容器），来运行maptask，appmaster喊开始执行然后map &#x3D;&gt; shuffle &#x3D;&gt; 落盘到对应的分区里，然后appmaster跟rm说我要运行reducetask，又申请两个资源，每个reducetask拉取自己分区的数据，然后reduce执行结束，最后appmaster说我要释放资源，然后资源释放</p>\n<p>yarn.schem.</p>\n<h2 id=\"数据倾斜\"><a href=\"#数据倾斜\" class=\"headerlink\" title=\"数据倾斜\"></a>数据倾斜</h2><h1 id=\"三、zookeeper\"><a href=\"#三、zookeeper\" class=\"headerlink\" title=\"三、zookeeper\"></a>三、zookeeper</h1><p>安装多少台：奇数台</p>\n<table>\n<thead>\n<tr>\n<th>服务器台数</th>\n<th>zk的台数</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>10</td>\n<td>3</td>\n</tr>\n<tr>\n<td>20</td>\n<td>5</td>\n</tr>\n<tr>\n<td>50</td>\n<td>7</td>\n</tr>\n<tr>\n<td>100</td>\n<td>11</td>\n</tr>\n</tbody></table>\n<p>台数越多：提高了数据可靠性，但是增加了通信延迟（举手表决需要通信），降低了效率</p>\n<p>选举机制：</p>\n<p>半数机制</p>\n<p>常用命令：</p>\n<p>ls get create delete</p>\n<h1 id=\"四、flume\"><a href=\"#四、flume\" class=\"headerlink\" title=\"四、flume\"></a>四、flume</h1><p><strong>一、三个组成</strong></p>\n<p>source</p>\n<p>​    Q：选择什么？</p>\n<p>​    A：taildir source，</p>\n<p>​    Q：为什么选择</p>\n<p>​    A：支持断点续传，多目录</p>\n<p>​    于什么时候产生？</p>\n<p>​    该source于Apache1.7 cdh1.6 产生</p>\n<p>​    没有taildir source之前怎么实现断点续传？</p>\n<p>​    自定义source，</p>\n<p>​    具体实现？</p>\n<p>​    （Apache 1.7源码）</p>\n<p>​    taildir source挂了会有什么影响？</p>\n<p>​    记录了offset，数据不会丢，但是数据有可能重复（有可能是先写到出然后还没来得及写offset就挂掉）</p>\n<p>​    Q：重复数据怎么解决</p>\n<p>​    A：不处理提高传输效率，交给下一级处理（在hive的dwd层处理，sparkstreaming，使用groupby或者开窗处理，redis）。处理使用    事务（自定义source实现）</p>\n<p>​    该source是否支持递归遍历文件夹读取数据</p>\n<p>​    不支持，只能自定义source，分成两部分（一部分遍历文件夹 + taildir读取文件函数）</p>\n<p>channel</p>\n<p>有哪些channel？各自的特点？</p>\n<ul>\n<li><p>memory channel：基于内存，可靠性低，传输效率高</p>\n</li>\n<li><p>file channel：基于磁盘，可靠性高，传输效率低</p>\n</li>\n<li><p>kafka channel：数据存储在kafka里面，数据基于磁盘，可靠性高，传输性能高于memorychannel + kafkasink。写kafka最好这样用</p>\n</li>\n</ul>\n<p>kafkachannel什么时候产生的</p>\n<p>Apache1.6产生，但是当时没火起来，因为传输数据总是带着头信息加上内容，后续需要额外清洗，去掉头，虽然提供了参数可以不要头但是没有起作用，于Apache1.7问题解决了，火了。</p>\n<p>如何选择？</p>\n<p>如果下一级是kafka优先选择kafkachannel，</p>\n<p>如果不是的话：对可靠性要求比较高，如金融行业，可以使用file channel，如果只是普通的日志，对可靠性要求不高，对速度要求高的选择memory channel</p>\n<p>hdfs sink</p>\n<p>小文件问题，不做控制会产生大量的小文件，调整三个参数</p>\n<p>文件大小：128m，滚动一次形成一个文件</p>\n<p>文件时间：1个小时或者2个小时（取一个回答）</p>\n<p>event个数：0</p>\n<p><strong>二、三个器</strong></p>\n<p>1）拦截器</p>\n<p>ETL拦截器 判断json的完整性（是否是{开头哦，是否是}结尾）</p>\n<p>事件拦截器 event 和 start</p>\n<p>event看topic</p>\n<p>将source按类分，发到不同的topic</p>\n<p>2）自定义拦截器：</p>\n<p>定义类，实现interceptor接口，重写4个方法（初始化，关闭资源，单event处理，多event处理，builder）</p>\n<p>打包上传到flume的lib包下，在配置文件中关联拦截器（全类名 $builder）</p>\n<p>3）不用拦截器行不行</p>\n<p>可以，就在hive的dwd层或者sparkstreaming内部解析</p>\n<p>选择器</p>\n<p>有几种？</p>\n<p>两种</p>\n<p>replicating（默认）：把数据发往下一级所有通道</p>\n<p>multiplexing：把数据选择性发往指定通道</p>\n<p>3）监控器</p>\n<p>Ganglia监控source向channelput的时候尝试提交的次数远远大于channel实际收到的次数，说明出现了大量的重试，就需要优化flume</p>\n<p>怎么优化呢？</p>\n<p>自身：单台flume在flume-env.sh增加内存</p>\n<p>找兄弟：增加flume台数（怎么增加，先增加日志服务器，把后续服务打通，然后通过nginx引过来）</p>\n<p>日志服务器配置（阿里云）：16g&#x2F;32g&#x2F;64g，只是部署一个springboot程序和一个flume</p>\n<p>选择器</p>\n<p>监控器</p>\n<p><strong>三、优化</strong></p>\n<p>filechannel 能配置多磁盘就配置多磁盘，能够提高吞吐量</p>\n<p>hdfssink，小文件，文件大小，128m，文件时间1-2h，event个数0个</p>\n<p>监控</p>\n<p>自身：单台flume在flume-env.sh增加内存</p>\n<p>找兄弟：增加flume台数</p>\n<p>挂了怎么办</p>\n<p>memorychannel有可能丢数据（该channel默认100个event，而filechannel默认100万个event）</p>\n<p>如果是taildirsource不会丢数据，但是有可能重复数据</p>\n<h1 id=\"五、kafka-23件事\"><a href=\"#五、kafka-23件事\" class=\"headerlink\" title=\"五、kafka 23件事\"></a>五、kafka 23件事</h1><h2 id=\"1、基本信息\"><a href=\"#1、基本信息\" class=\"headerlink\" title=\"1、基本信息\"></a>1、基本信息</h2><p>1）组成：生产者，broker，消费者，zk</p>\n<p>2）zk中存储了哪些信息：broker的id，topic的相关信息，低版本存储有消费者信息，高版本没有</p>\n<p>3）安装多少台？2 *（v * n &#x2F; 100) +1 &#x3D;y(企业中大部分都是三个)</p>\n<p>​    v：生产者峰值生成速率</p>\n<p>​    n：副本个数</p>\n<p>​    y：最终结果</p>\n<p>​    副本和台数固定了的话，那么生成速率就有其上限</p>\n<p>4）</p>\n<p>​    生产者峰值生成速率：需要压测；</p>\n<p>​    副本：默认是1，企业中通常配置2-3个（为什么这样配，副本越多，可靠性越高，增加了磁盘io，效率低下了）</p>\n<p>5）kafka里面的数据量问题</p>\n<p>​    100万日活，每个人产生100条日志数据，一天可以产生1亿条日志数据，一条数据大概0.5k到2k之间，一般取1k</p>\n<p>​    平均速度：1亿 &#x2F; 24 * 3600s &#x3D; 1150条&#x2F;秒 &#x3D;&gt; 说成1千多条就行 &#x3D;&gt; 大概1m&#x2F;s</p>\n<p>​    峰值速度：平均速度的20倍以上，每秒大约2万条数据左右，数据量大小20m&#x2F;s左右，在3台机器和2个副本的条件下，生成速率上限为50m&#x2F;s</p>\n<p>6）kafka中数据保存多久</p>\n<p>​    默认一周，其实保存3天就可以了，当天数据当天就消费了最多第二天多一点点</p>\n<p>7）kafka磁盘预留多大</p>\n<p>​    100g（1亿条 * 1kb &#x3D; 100G），副本是2，保存3天，加上一定的余量（30%-20%）&#x3D;&gt; 100G * 2 * 3 &#x2F;0.7 &#x3D; 857G&#x3D;&gt;1T</p>\n<p>8）kafka可以做监控</p>\n<p>​    kafka manager或者kafkaEgale，</p>\n<p>9）kafka的分区</p>\n<p>​    先设置一个分区，然后对其压测，测生产者的峰值生成速率tp和消费者消费的峰值速率tc，同时要有一个kafka吞吐量的预期（希望的t） </p>\n<p>​    分区数 &#x3D; t &#x2F; min(tp, tc)，例如t &#x3D;100m&#x2F;s,tp&#x3D;40m&#x2F;s,tc&#x3D;50m&#x2F;s，则分区数 &#x3D; 100&#x2F;40 &#x3D;2.5 &#x3D;&gt; 3个分区。除较小的，得到的就是较大的，一般企业中设置3-10个分区。</p>\n<p>10）分区分配策略</p>\n<p>​    range（默认）：假设10个分区，三个消费者线程消费，</p>\n<p>​        c1: 0,1,2,3 &#x3D;&gt; 容易产生数据倾斜，除不尽的放在了低消费者</p>\n<p>​        c2: 4,5,6</p>\n<p>​        c3: 7,8,9</p>\n<p>​    </p>\n<p>​    roundrobin：全部随机使用hash的方式随机打散，按hash排序，然后再轮询</p>\n<p>11）ISR</p>\n<p>​    主要解决leader挂了谁当老大，在isr队列里的都有机会当老大。</p>\n<p>​        旧版本：延迟时间、延迟条数</p>\n<p>​        新版本：延迟时间</p>\n<h2 id=\"2、挂了\"><a href=\"#2、挂了\" class=\"headerlink\" title=\"2、挂了\"></a>2、挂了</h2><p>短时间，内会存储在flume channel里面</p>\n<p>长时间，日志服务器保存30天数据</p>\n<h2 id=\"3、丢了\"><a href=\"#3、丢了\" class=\"headerlink\" title=\"3、丢了\"></a>3、丢了</h2><p>ack</p>\n<ul>\n<li>0：发送过来就不管了，效率最高，可靠性最低，容易丢数据，生产环境基本不用</li>\n<li>1：等待leader写好，然后leader应答，效率一般，可靠性一般。</li>\n<li>-1：要leader和Follower共同应答，要全部都写好，效率最差，可靠性最高。</li>\n</ul>\n<p>生产环境下：1一般传输普通日志，-1跟钱相关或者金融企业。</p>\n<h2 id=\"4、重复了\"><a href=\"#4、重复了\" class=\"headerlink\" title=\"4、重复了\"></a>4、重复了</h2><p>自身：幂等性、事务、ack&#x3D;-1，通常用的都比较少，更注重效率。</p>\n<p>​    幂等性：单分区单会话内会有一个id</p>\n<p>​    事务：支持多分区，全局放了个id，每条比较是否重复了，每条都会进行判断，性能很低</p>\n<p>找兄弟：hive的dwd层进行处理，sparkstreaming&#x2F;redis</p>\n<p>不在自身处理一般，都是找兄弟处理</p>\n<h2 id=\"5、积压了\"><a href=\"#5、积压了\" class=\"headerlink\" title=\"5、积压了\"></a>5、积压了</h2><p>原因1：消费能力太低，</p>\n<p>解决方法：分区数太少了，增加分区同时增加消费者的cpu核数，增加下一级消费者消费速度（如spark和flume都有一个batchsize可以调大）。</p>\n<h2 id=\"6、优化\"><a href=\"#6、优化\" class=\"headerlink\" title=\"6、优化\"></a>6、优化</h2><p>1） Broker参数配置（server.properties)</p>\n<blockquote><span class=\"custom-blockquote-svg\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"\" xmlns=\"http://www.w3.org/2000/svg\" data-reactroot=\"\">\n<path fill=\"\" d=\"M22 12C22 6.5 17.5 2 12 2C6.5 2 2 6.5 2 12C2 17.5 6.5 22 12 22C13.8 22 15.5 21.5 17 20.6L22 22L20.7 17C21.5 15.5 22 13.8 22 12Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\" undefined=\"1\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M17 8.5C15.23 8.97 14.07 10.84 14.01 13.27C14 13.33 14 13.4 14 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M9 8.5C7.23 8.97 6.07 10.84 6.01 13.27C6 13.33 6 13.4 6 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\"></path>\n</svg>\n</span><p>1、日志保留策略配置 </p>\n<p>保留三天，也可以更短 （log.cleaner.delete.retention.ms） </p>\n<p>log.retention.hours&#x3D;72</p></blockquote>\n<blockquote><span class=\"custom-blockquote-svg\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"\" xmlns=\"http://www.w3.org/2000/svg\" data-reactroot=\"\">\n<path fill=\"\" d=\"M22 12C22 6.5 17.5 2 12 2C6.5 2 2 6.5 2 12C2 17.5 6.5 22 12 22C13.8 22 15.5 21.5 17 20.6L22 22L20.7 17C21.5 15.5 22 13.8 22 12Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\" undefined=\"1\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M17 8.5C15.23 8.97 14.07 10.84 14.01 13.27C14 13.33 14 13.4 14 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M9 8.5C7.23 8.97 6.07 10.84 6.01 13.27C6 13.33 6 13.4 6 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\"></path>\n</svg>\n</span><p>2、Replica 相关配置 </p>\n<p>default.replication.factor:1 # 默认副本1个</p>\n<p>3、网络通信延时</p>\n<p>replica.socket.timout.ms:30000 # 当集群之间网络不稳定时，可以调大该参数</p>\n<p>replica.lag.time.max.ms &#x3D; 600000 # 如果网络不好，或者kafka集群压力比较大，会出现副本丢失，然后会频繁复制副本，导致集群压力更大，此时可以调大该参数</p>\n<p>本来只是两个节点正常的交流，只是数据量太大或者风太大听不清，结果认为是有问题。</p></blockquote>\n<p>2）Producer优化（producer.properties）</p>\n<blockquote><span class=\"custom-blockquote-svg\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"\" xmlns=\"http://www.w3.org/2000/svg\" data-reactroot=\"\">\n<path fill=\"\" d=\"M22 12C22 6.5 17.5 2 12 2C6.5 2 2 6.5 2 12C2 17.5 6.5 22 12 22C13.8 22 15.5 21.5 17 20.6L22 22L20.7 17C21.5 15.5 22 13.8 22 12Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\" undefined=\"1\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M17 8.5C15.23 8.97 14.07 10.84 14.01 13.27C14 13.33 14 13.4 14 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M9 8.5C7.23 8.97 6.07 10.84 6.01 13.27C6 13.33 6 13.4 6 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\"></path>\n</svg>\n</span><p>compression.type:none gzip snappy lz4</p>\n<p>#默认发送不进行**压缩 **，推荐配置一种适合的压缩算法，可以大幅度的减缓网络压力和Broker的存储压力</p></blockquote>\n<p>3）Kafka内存调整</p>\n<p>默认一个G，生产环境可以调到4-6G：<code>export KAFKA_HEAP_OPTS=&quot;-Xms4g -Xmx4g&quot;</code></p>\n<h2 id=\"7、其他\"><a href=\"#7、其他\" class=\"headerlink\" title=\"7、其他\"></a>7、其他</h2><p>1）kafka为什么可以高效读写数据</p>\n<p>​    （1）kafka本身就是集群，又可以设置分区提高并行度，</p>\n<p>​    （2）底层采用顺序读写（600m&#x2F;s）（随机读写（100m&#x2F;s））</p>\n<p>​    （3）采用了零拷贝技术<img src=\"https://gitee.com/cencus/blog-image/raw/master/blogImage/1651546987897.png\"></p>\n<p>2）kafka单条日志传输大小</p>\n<p>超过1m的日志会卡死（生产者无法将消息推送到kafka或消费者无法从kafka消费数据），怎么解决呢？</p>\n<blockquote><span class=\"custom-blockquote-svg\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"\" xmlns=\"http://www.w3.org/2000/svg\" data-reactroot=\"\">\n<path fill=\"\" d=\"M22 12C22 6.5 17.5 2 12 2C6.5 2 2 6.5 2 12C2 17.5 6.5 22 12 22C13.8 22 15.5 21.5 17 20.6L22 22L20.7 17C21.5 15.5 22 13.8 22 12Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\" undefined=\"1\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M17 8.5C15.23 8.97 14.07 10.84 14.01 13.27C14 13.33 14 13.4 14 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M9 8.5C7.23 8.97 6.07 10.84 6.01 13.27C6 13.33 6 13.4 6 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\"></path>\n</svg>\n</span><p>replica.fetch.max.bytes:104876 broker课复制的消费的最大字节数，默认为1M</p>\n<p>message.max.bytes:100012 kafka会接受单个消息size的最大限制，默认为1M左右</p>\n<p>注意不要调整的太大</p></blockquote>\n<p><span style=\"color:red\">注意：message.max.bytes必须小于等于replica.feach.max.bytes，否则就会导致replica之间数据同步失败</span></p>\n<p>3）过期数据清理 &#x3D;&gt; 删除和压缩</p>\n<p>​    （1）保证数据没有被引用（没人消费他）</p>\n<blockquote><span class=\"custom-blockquote-svg\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"\" xmlns=\"http://www.w3.org/2000/svg\" data-reactroot=\"\">\n<path fill=\"\" d=\"M22 12C22 6.5 17.5 2 12 2C6.5 2 2 6.5 2 12C2 17.5 6.5 22 12 22C13.8 22 15.5 21.5 17 20.6L22 22L20.7 17C21.5 15.5 22 13.8 22 12Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\" undefined=\"1\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M17 8.5C15.23 8.97 14.07 10.84 14.01 13.27C14 13.33 14 13.4 14 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M9 8.5C7.23 8.97 6.07 10.84 6.01 13.27C6 13.33 6 13.4 6 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\"></path>\n</svg>\n</span><p>log.cleanup.policy &#x3D; delete 启用删除策略（建议使用）</p>\n<p>log.cleanup.policy &#x3D; compact 启用压缩策略</p></blockquote>\n<p>4）kafka可以按照时间消费数据</p>\n<p><code>Map&lt;TopicPartition, OffsetAndTimestamp&gt; startOffsetMap =  KafkaUtils.fetchOffsetsWithTimestamp(topic, sTime, kafkaProp)</code></p>\n<p>5）kafka消费者角度是拉取还是推送数据</p>\n<p>是消费者主动拉取</p>\n<p>6）kafka中数据是否是有序的吗</p>\n<p>单分区内有序，多分区，分区与分区间无序；</p>\n<p>如何保证有序，保证单分区，不然多分区还要拿出来排序，效率很低</p>\n<p>多分区如何保证有序，每个数据有一个编号，然后到了还要排序，完了还要校验编号是否连续。万一有一个数据迟迟不来很容易卡死</p>\n<h1 id=\"六、Hive\"><a href=\"#六、Hive\" class=\"headerlink\" title=\"六、Hive\"></a>六、Hive</h1><h2 id=\"1、hive的组成\"><a href=\"#1、hive的组成\" class=\"headerlink\" title=\"1、hive的组成\"></a>1、hive的组成</h2><p><img src=\"https://gitee.com/cencus/blog-image/raw/master/blogImage/1651548126722.png\"></p>\n<h2 id=\"2、与mysql等关系型数据库的区别\"><a href=\"#2、与mysql等关系型数据库的区别\" class=\"headerlink\" title=\"2、与mysql等关系型数据库的区别\"></a>2、与mysql等关系型数据库的区别</h2><table>\n<thead>\n<tr>\n<th></th>\n<th>Hive</th>\n<th>mysql</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>数据量</td>\n<td>大（上）</td>\n<td>小（千万）</td>\n</tr>\n<tr>\n<td>速度</td>\n<td>大数据量场景，hive快</td>\n<td>小数据量场景MySQL快</td>\n</tr>\n<tr>\n<td>擅长</td>\n<td>查询</td>\n<td>增删改查</td>\n</tr>\n</tbody></table>\n<h2 id=\"3、内部表和外部表的区别（高频）\"><a href=\"#3、内部表和外部表的区别（高频）\" class=\"headerlink\" title=\"3、内部表和外部表的区别（高频）\"></a>3、内部表和外部表的区别（高频）</h2><p>1）区别</p>\n<p>​    内部表删除数据：删除元数据和原始数据</p>\n<p>​    外部表删除数据：删除元数据但是不删除原始数据</p>\n<p>2）什么时候用内部表？什么时候用外部表？</p>\n<p>​    绝大多数表都是外部表，只有自己使用的临时表才是内部表</p>\n<h2 id=\"4、四个by\"><a href=\"#4、四个by\" class=\"headerlink\" title=\"4、四个by\"></a>4、四个by</h2><p>sort by：排序</p>\n<p>（常用）d by：分区，通常是sort + d 分区内排序</p>\n<p>（基本不用）order by：全局排序，容易产生数据倾斜（所有数据进入同一个reduce，最好提高reduce相关的内存）</p>\n<p>c by：sort 和 d字段相同时用classed by</p>\n<h2 id=\"5、函数系列\"><a href=\"#5、函数系列\" class=\"headerlink\" title=\"5、函数系列\"></a>5、函数系列</h2><p>系统函数</p>\n<p>​    日（date_add,date_sub）、周（next_day）、月(date_format,lastday)</p>\n<p>​    解析JSON(get_json_object)</p>\n<p>自定义函数</p>\n<p>​    UDF&#x2F;UDTF</p>\n<p>​    1）自定义UDF解决了什么问题？</p>\n<p>​    解析公共字段</p>\n<p>​    2）自定义UDTF解决了什么问题？</p>\n<p>​    解析了事件字段</p>\n<p>​    3）不用行不行？</p>\n<p>​    完全可以，用get_json_object做解析也可以。用的话方便定位bug，解析失败了可以在自定义函数中去查找问题，而且用自定义函数可以完成更多复杂的操作（例如给一个IP地址，解析对应的城市）</p>\n<p>​    4）自定义UDF函数的步骤</p>\n<p>​    定义一个类，继承UDF，重写其中的evaluate方法；</p>\n<p>​    5）自定义UDTF的步骤</p>\n<p>​    定义一个类，继承UDTF，重写三个方法（初始化（定义返回值的名称和类型），process（具体实现对应的解析），关闭资源），然后打包上传到hdfs路径，在hive的客户端创建永久函数</p>\n<p>窗口函数</p>\n<p>rank：三个排序</p>\n<p>over：开窗</p>\n<p>能够实现手写topn就可以了</p>\n<h2 id=\"6、优化-1\"><a href=\"#6、优化-1\" class=\"headerlink\" title=\"6、优化\"></a>6、优化</h2><p>1）mapjoin默认打开，不要关闭，能在map处理的就在map阶段处理掉</p>\n<p>2）行列过滤：join + where &#x3D;&gt; where + join，先聚合后过滤莫不如先过滤后聚合</p>\n<p>3）创建分区表，防止大面积的全局扫描</p>\n<p>4）小文件相关处理：</p>\n<p>​    （1）CombinehiveInputFormat &#x3D;&gt; 减少切片个数，进而减少maptask个数</p>\n<p>​    （2）JVM重用</p>\n<p>​    （3）merge功能，如果是maponly任务，默认打开，能在执行完任务后默认开启一个job，将产生的小于16M的小文件合并到256m，如是MapReduce任务，需要将该功能开启</p>\n<p>5）压缩</p>\n<p>6）列式存储</p>\n<p>​    行式存储：一行一行的存</p>\n<p>​    列式存储：一列一列的存</p>\n<p>​    select name from，则会顺序读写，速度超快</p>\n<p>7）替换引擎</p>\n<p>​    mr（基于磁盘，速度慢，可靠性高，效率低。在数据量比较大，计算时间比较长，例如周月年时可以使用） </p>\n<p>​    &#x3D;&gt; </p>\n<p>​        tez（基于内存，速度快，可靠性差，效率高。临时使用，即时查询，生产环境容易om|overmemory）</p>\n<p>​        spark（基于内存+磁盘，磁盘是因为shuffle的内容必须要落盘，还有checkpoint，可靠性与效率居中。每天的定时任务，）</p>\n<p>8）在map端提前开启combiner（不能影响最终业务逻辑的前提下）</p>\n<p>9）合理设置map个数<code>（max(0, min(块大小，Long的最大值))）</code>和reduce个数<code>（setnumberreducetask）</code>，原则上128m数据尽量使用1g内存。</p>\n<h2 id=\"7、数据倾斜\"><a href=\"#7、数据倾斜\" class=\"headerlink\" title=\"7、数据倾斜\"></a>7、数据倾斜</h2><p>1）怎么产生的数据倾斜</p>\n<p>情形，用户t1表达uid是int，而用户行为表t2的uid是string类型的，join时虽然会数据自动类型转换但是最好是手动显示的进行自动转换</p>\n<pre class=\"line-numbers language-sql\" data-language=\"sql\"><code class=\"language-sql\"><span class=\"token keyword\">select</span> <span class=\"token operator\">*</span>\n<span class=\"token keyword\">from</span> t1\n<span class=\"token keyword\">left</span> <span class=\"token keyword\">join</span> t2\n<span class=\"token keyword\">on</span> t1<span class=\"token punctuation\">.</span>sid <span class=\"token operator\">=</span> cast<span class=\"token punctuation\">(</span>t1<span class=\"token punctuation\">.</span>sid <span class=\"token keyword\">as</span> string<span class=\"token punctuation\">)</span><span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span></span></code></pre>\n\n\n\n<p>2）控制空值分布</p>\n<p>​    生产环境中经常有大量的null进入同一个reduce。解决方法：如果需要空值的话，就在后面拼接随机数，如果不需要直接where过滤就好</p>\n<p>3）解决数据倾斜的方法：</p>\n<p>​    （1）group by</p>\n<p>采用 sum() group by 的方式来替换count(distinct)完成计算</p>\n<p>​    （2）mapjoin，map阶段能够join一部分数据就join一部分数据</p>\n<p>​    （3）开启数据倾斜时的负载均衡<code>set hive.groupby.skewindata=true</code></p>\n<p>和前面空值手动完成一样的，先key随机打散计算，再把随机打散看key聚合</p>\n<h2 id=\"其他\"><a href=\"#其他\" class=\"headerlink\" title=\"其他\"></a>其他</h2><p>1）hive中默认分隔符是\\001，必须要求前端埋点和javaee传过来的后台传递过来的数据不能以\\t分割，前面先约束，约束不行后面做etl</p>\n<p>mysql元数据备份，mysql基于keepalived实现ha。</p>\n<p>2）mysql utf8字节数问题</p>\n<p>​    mysqlutf8最多存储3个字节，如果太大了要<code>set character_set_server = utf8mb4</code></p>\n<p>3）Union和Union all区别</p>\n<p>​    Union要去重</p>\n<p>​    Union all不去重，所以效率高</p>\n","feature":true,"text":"由大到小，由粗到细 一、Linux &amp; Shell总结ps -ef 只执行ps命令，默认是显示当前控制台下属于当前用户的进程； 参数 -e 显示运行在系统上的所有进程 参数 -f 扩展显示输出 UID 启动进程的用户 PID 进程的进程号 PPID 父进程进程号 C cp...","link":"","photos":[],"count_time":{"symbolsCount":"10k","symbolsTime":"9 mins."},"categories":[],"tags":[],"toc":"<ol class=\"toc\"><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#%E4%B8%80%E3%80%81Linux-amp-Shell%E6%80%BB%E7%BB%93\"><span class=\"toc-text\">一、Linux &amp; Shell总结</span></a></li><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#Hadoop\"><span class=\"toc-text\">Hadoop</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E5%85%A5%E9%97%A8\"><span class=\"toc-text\">入门</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#HDFS\"><span class=\"toc-text\">HDFS</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#MapReduce\"><span class=\"toc-text\">MapReduce</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#YARN\"><span class=\"toc-text\">YARN</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C\"><span class=\"toc-text\">数据倾斜</span></a></li></ol></li><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#%E4%B8%89%E3%80%81zookeeper\"><span class=\"toc-text\">三、zookeeper</span></a></li><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#%E5%9B%9B%E3%80%81flume\"><span class=\"toc-text\">四、flume</span></a></li><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#%E4%BA%94%E3%80%81kafka-23%E4%BB%B6%E4%BA%8B\"><span class=\"toc-text\">五、kafka 23件事</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#1%E3%80%81%E5%9F%BA%E6%9C%AC%E4%BF%A1%E6%81%AF\"><span class=\"toc-text\">1、基本信息</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#2%E3%80%81%E6%8C%82%E4%BA%86\"><span class=\"toc-text\">2、挂了</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#3%E3%80%81%E4%B8%A2%E4%BA%86\"><span class=\"toc-text\">3、丢了</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#4%E3%80%81%E9%87%8D%E5%A4%8D%E4%BA%86\"><span class=\"toc-text\">4、重复了</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#5%E3%80%81%E7%A7%AF%E5%8E%8B%E4%BA%86\"><span class=\"toc-text\">5、积压了</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#6%E3%80%81%E4%BC%98%E5%8C%96\"><span class=\"toc-text\">6、优化</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#7%E3%80%81%E5%85%B6%E4%BB%96\"><span class=\"toc-text\">7、其他</span></a></li></ol></li><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#%E5%85%AD%E3%80%81Hive\"><span class=\"toc-text\">六、Hive</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#1%E3%80%81hive%E7%9A%84%E7%BB%84%E6%88%90\"><span class=\"toc-text\">1、hive的组成</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#2%E3%80%81%E4%B8%8Emysql%E7%AD%89%E5%85%B3%E7%B3%BB%E5%9E%8B%E6%95%B0%E6%8D%AE%E5%BA%93%E7%9A%84%E5%8C%BA%E5%88%AB\"><span class=\"toc-text\">2、与mysql等关系型数据库的区别</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#3%E3%80%81%E5%86%85%E9%83%A8%E8%A1%A8%E5%92%8C%E5%A4%96%E9%83%A8%E8%A1%A8%E7%9A%84%E5%8C%BA%E5%88%AB%EF%BC%88%E9%AB%98%E9%A2%91%EF%BC%89\"><span class=\"toc-text\">3、内部表和外部表的区别（高频）</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#4%E3%80%81%E5%9B%9B%E4%B8%AAby\"><span class=\"toc-text\">4、四个by</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#5%E3%80%81%E5%87%BD%E6%95%B0%E7%B3%BB%E5%88%97\"><span class=\"toc-text\">5、函数系列</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#6%E3%80%81%E4%BC%98%E5%8C%96-1\"><span class=\"toc-text\">6、优化</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#7%E3%80%81%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C\"><span class=\"toc-text\">7、数据倾斜</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E5%85%B6%E4%BB%96\"><span class=\"toc-text\">其他</span></a></li></ol></li></ol>","author":{"name":"Cencus","slug":"blog-author","avatar":"https://gitee.com/cencus/blog-image/raw/master/blogImage/1648381409561.jpg","link":"/","description":"stay hungry, stay foolish.","socials":{"github":"","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{}}},"mapped":true,"prev_post":{"title":"重装系统","uid":"442c1730bf929d3d77849308e23e2567","slug":"重装系统","date":"2022-05-03T13:08:07.000Z","updated":"2022-05-03T13:08:53.656Z","comments":true,"path":"api/articles/重装系统.json","keywords":null,"cover":null,"text":"方式 1、在线 2、安装系统盘 准备 ","link":"","photos":[],"count_time":{"symbolsCount":19,"symbolsTime":"1 mins."},"categories":[],"tags":[],"author":{"name":"Cencus","slug":"blog-author","avatar":"https://gitee.com/cencus/blog-image/raw/master/blogImage/1648381409561.jpg","link":"/","description":"stay hungry, stay foolish.","socials":{"github":"","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{}}},"feature":true},"next_post":{"title":"serverlet","uid":"e090f92b1230f61edcb4c749584fc402","slug":"serverlet","date":"2022-04-27T04:28:43.000Z","updated":"2022-04-27T04:52:02.602Z","comments":true,"path":"api/articles/serverlet.json","keywords":null,"cover":null,"text":"ServerLet响应格式 响应行：版本，状态码，消息 头：附加信息 空行：就是空行 响应实体：正文 常见状态码： 第一个定义类型，后面没有分类作用，分为5中类型 1xx 信息 服务器收到请求 2xx 成功 操作被接受并处理 3xx 重定向 需要进一步的操作以完成请求 4xx 客...","link":"","photos":[],"count_time":{"symbolsCount":605,"symbolsTime":"1 mins."},"categories":[],"tags":[],"author":{"name":"Cencus","slug":"blog-author","avatar":"https://gitee.com/cencus/blog-image/raw/master/blogImage/1648381409561.jpg","link":"/","description":"stay hungry, stay foolish.","socials":{"github":"","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{}}},"feature":true}}