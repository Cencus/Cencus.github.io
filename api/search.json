[{"id":"e04902dcdb07e2ad1898c41f1efa8f40","title":"spark","content":"第3章 spark运行环境3.1 local模式3.2 standalone模式3.3 yarn模式（公司常用）3.4 k8s &amp; Mesos模式3.5 windows模式3.5.2 本地环境第4章 spark运行架构Driver 驱动器节点，执行spark任务中的main方法，负责实际的代码执行，任务的调度与管理监控\nExecutor：Worker中的一个JVM进程，负责运行具体的任务，任务的实际执行\nMaster：资源的调度与分配类似于resourceManager\nWorker：\nAM：解耦Driver 计算 和RM 资源\nRDD的并行度&amp;分区\nmakeRDD方法可以传递第二个参数，这个参数表示分区的数量\n第二个参数可以不传递，那么makeRDD会使用默认的数量\n//数据如何分配到分区\n\n\n\n\n\n\n\n第5章 spark核心编程5.1 RDD5.1.1 什么是RDD\n\n\n\n\n\n\n\n\n Resilient Distributed Dataset：弹性分布式数据集\n是 Spark 中最基本的数据 处理模型\n代码中是一个抽象类，它代表一个弹性的、不可变、可分区、里面的元素可并行计算的集合。\n简单一点就是数据+操作\n逐个解释\n➢ 弹性\n\n存储的弹性：内存与磁盘的自动切换；\n\n容错的弹性：数据丢失可以自动恢复； （有副本）\n\n计算的弹性：计算出错重试机制； \n\n分片的弹性：可根据需要重新分片。 （4个Executor可以改为4个分区）\n\n\n➢ 分布式：数据存储在大数据集群不同节点上 \n➢ 数据集：RDD 封装了计算逻辑，并不保存数据 （做完了就把数据发给下一步）\n➢ 数据抽象：RDD 是一个抽象类，需要子类具体实现 \n➢ 不可变：RDD 封装了计算逻辑，是不可以改变的，想要改变，只能产生新的 RDD，在 新的 RDD 里面封装计算逻辑 \n➢ 可分区、并行计算\n5.1.2 核心属性\n分区列表\n\n分区计算函数：每一个分区的计算方法\n\n与其他RDD的依赖关系\n\n分区器（可选）：数据放在哪个分区如我们有HashPartitioner\n\n首选位置（可选）：将任务发给Executor时，发向哪一个Executor（就近原则，移动数据不如移动计算）\n\n\n&#x2F;&#x2F; 完整 精简 准确 自己的理解\n5.1.3 执行原理1、启动 Yarn 集群环境\n\n2、Spark申请资源创建调度节点和计算节点（找人）\n\n3、Spark 框架根据需求将计算逻辑根据分区划分成不同的任务（指定任务）\n\n4、调度节点将任务根据计算节点状态发送到对应的计算节点进行计算（让员工执行任务）\n\n5.1.4 基础编程5.1.4.1 RDD创建1、从集合（内存）中创建RDD 常用\n两个方法parallelize和makeRDD\n从底层来讲makeRDD就是parallelize只是对其进行了封装，更好记了\ndef makeRDD[T: ClassTag](\n\tseq: Seq[T],\n\tnumSlices: Int = defaultParallelism): RDD[T] = withScope &#123;\n    parallelize(seq, numSlices)\n\t&#125;\n)\n\n\n\n2、从外部存储（文件）创建RDD 常用\ntextFile(&quot;path&quot;)方法\n支持的系统：本地FS，HDFS，HBase\nval fileRDD: RDD[String] = sc.textFile(\"path\") //所得是按照行分割的一个集合\n\n/*\n其中的path：\n* 目录(目录下的所有文件) 或者 具体文件\n* 通配符 textFile(\"datas/1*.txt\") 所有以1开头的.txt文件\n* 分布式存储路径： \"hdfs://hadoop102:8020/word.txt\"\n* wholeTextFiles() 以文件为单位读取数据，textFile是以行为单位，所得结果为一个turple (文件地址, 内容)\n*/\n\n\n\n3、从其他RDD创建\n…\n4、直接创建RDD\n…\n5.1.4.2 RDD并行度与分区并行度：能够并行计算的任务数量\ntodo：好像有些东西没有写上\n5.1.4.3 RDD转换算子(重点)1. value类型0、一些函数的解释\ncollect()\n\nforeach()\n\n\n\n01、map函数签名：\ndef map[U: ClassTag](f: T => U): RDD[U]\n\n粗解：传入一个函数，输入是RDD，输入依然是RDD\n模板：\nrdd.map(\n\telem =>&#123;\n        //dosomthing\n        ...\n        newElem\n    &#125;\n    \n)\n\n\n\n\n\n对于一个可遍历的rdd，逐个遍历并进行映射转换（就是操作啦）\nmap方法，对于集合中的每个元素执行操作    为什么老师说这里没有执行操作而是只是在逐层封装，只有collect才在执行操作，真的没有执行操作，    那说明这些操作是被他们带上了，只有在collect的时候才执行 正确    collect 返回rdd中的所有元素组成的数组，只能用于数据量小的时候，例如所有数据装入外存这种情况     *&#x2F;\n例如想要对rdd里面的每一个元素乘2，就可以像下面这样写\npackage com.atguigu.bigdata.spark.core.rdd.operator.transform\n\nimport org.apache.spark.rdd.RDD\nimport org.apache.spark.&#123;SparkConf, SparkContext&#125;\n\nobject Spark01_RDD_Operator_Transform &#123;\n  def main(args: Array[String]): Unit = &#123;\n    val sparkConf = new SparkConf().setMaster(\"local[*]\").setAppName(\"Operator\")\n    val sc = new SparkContext(sparkConf)\n\n    //TODO 算子-map\n    //需求：将[1,2,3,4]变为[2,4,6,8]\n    val rdd = sc.makeRDD(List(1, 2, 3, 4))\n\n    val mapRDD: RDD[Int] = rdd.map(_ * 2)\n\n    mapRDD.collect().foreach(println)\n\n    sc.stop()\n  &#125;\n\n&#125;\n\n\n\nrdd的计算是一个分区内的数据是一个一个的数据执行我们的逻辑，只有前面的一个数据逻辑执行完毕后，才会执行下一个数据，分区内数据的执行是有序的\npackage com.atguigu.bigdata.spark.core.rdd.operator.transform\n\nimport org.apache.spark.rdd.RDD\nimport org.apache.spark.&#123;SparkConf, SparkContext&#125;\n\nobject Spark01_RDD_Operator_Transform_Par &#123;\n  def main(args: Array[String]): Unit = &#123;\n    val sparkConf = new SparkConf().setMaster(\"local[*]\").setAppName(\"Operator\")\n    val sc = new SparkContext(sparkConf)\n\n    //TODO 算子-map\n    val rdd = sc.makeRDD(List(1, 2, 3, 4),2)\n\n    val mapRDD = rdd.map(\n      num => &#123;\n        println(\">>>>>>> \" + num)\n        num\n      &#125;\n    )\n    val mapRDD1 = mapRDD.map(\n      num => &#123;\n        println(\"####### \" + num)\n        num\n      &#125;\n    )\n    mapRDD1.collect()\n\n    sc.stop()\n  &#125;\n\n&#125;\n\n\n/*\n分区1：[1,2]\n分区2: [3,4]\n分区各自独立，可以保证，2不可能再1前面，4不能再3前面\n不同分区之间数据的执行是无序的\n>>>>>>> 3\n>>>>>>> 1\n####### 1\n####### 3\n>>>>>>> 2\n####### 2\n>>>>>>> 4\n####### 4\n\n\n\nrdd的计算是一个分区内的数据是一个一个的数据执行我们的逻辑，只有前面的一个数据逻辑执行完毕后，才会执行下一个数据，\n分区内数据的执行是有序的\nrdd.map(1) => mapRDD.map(1)\nrdd.map(2) => mapRDD.map(2)\nrdd.map(3) => mapRDD.map(3)\nrdd.map(4) => mapRDD.map(4)\n>>>>>>> 1\n####### 1\n>>>>>>> 2\n####### 2\n>>>>>>> 3\n####### 3\n>>>>>>> 4\n####### 4\n\n\n\n */\n\n\n\n\n\n02、mapPartitions函数签名：\ndef mapPartitions[U: ClassTag](\n \tf: Iterator[T] => Iterator[U],\n \tpreservesPartitioning: Boolean = false): RDD[U]\n)\n\n粗解：传入一个函数：函数的传入类型为迭代类型，返回类型为迭代器\n使用模板：\nrdd.mapPartitions(\n\titer => &#123;\n        ...\n        newIter // List().iterator\n    &#125;\n)\n\n我的一些理解：\n将待处理的数据以分区为单位发送到计算节点进行处理，这里的处理是指可以进行任意的处 理，哪怕是过滤数据。\nmappartition方法到底在干嘛 rdd是一个上述的那种数据集，将rdd以分区为单位进行处理，完了把处理好的返回\n下面是一个使用案例\npackage com.atguigu.bigdata.spark.core.rdd.operator.transform\n\nimport org.apache.spark.rdd.RDD\nimport org.apache.spark.&#123;SparkConf, SparkContext&#125;\n\nobject Spark02_RDD_Operator_Transform &#123;\n  def main(args: Array[String]): Unit = &#123;\n    val sparkConf = new SparkConf().setMaster(\"local[*]\").setAppName(\"Operator\")\n    val sc = new SparkContext(sparkConf)\n\n    //TODO 算子-map\n    val rdd = sc.makeRDD(List(1, 2, 3, 4), 2)\n\n    //高性能做法，将一个分区全部拿到以后再做操作，而不是一个一个做操作\n    val mapRDD = rdd.mapPartitions(\n      iter => &#123;\n        println(\">>>>>>>>>>>>>>>\")\n        iter.map(_ * 2)\n      &#125;\n    )\n\n    mapRDD.collect().foreach(println)\n\n    sc.stop()\n  &#125;\n\n&#125;\n\n\n/*\nmappartitions类似缓冲\n说明执行了两次，因为有两个分区，一次把一个分区拿过来，放在内存计算，类似于批处理，\n>>>>>>>>>>>>>>>\n>>>>>>>>>>>>>>>\n2\n4\n6\n8\n但是不好的地方：\n* 当一个分区没有计算完成，比如说 10000条数据计算了999条，这999条不会释放，因为存在对象的引用\n* 在内存较小，数据量较大的场景下容易出现内存移除，这种情况下还是用map\n */\n\n案例返回每个数据分区的最大值\npackage com.atguigu.bigdata.spark.core.rdd.operator.transform\n\nimport org.apache.spark.&#123;SparkConf, SparkContext&#125;\n\nobject Spark02_RDD_Operator_Transform_Test &#123;\n  def main(args: Array[String]): Unit = &#123;\n    val sparkConf = new SparkConf().setMaster(\"local[*]\").setAppName(\"Operator\")\n    val sc = new SparkContext(sparkConf)\n\n    //TODO 算子-map\n    val rdd = sc.makeRDD(List(1, 2, 3, 4), 2)\n      \n    //需求获取每个分区的最大值\n    val mapRDD = rdd.mapPartitions(\n      iter => &#123;\n        List(iter.max).iterator\n      &#125;\n    )\n\n    mapRDD.collect().foreach(println)\n      \n    sc.stop()\n  &#125;\n\n&#125;\n\n\n\nmap与mapPartitions的区别\n\n\n\n角度\nmap\nmapPartitions\n\n\n\n数据处理角度\nMap 算子是分区内一个数据一个数据的执行，类似于串行操作\nmapPartitions 算子 是以分区为单位进行批处理操作\n\n\n功能的角度\nMap 算子主要目的将数据源中的数据进行转换和改变。但是不会减少或增多数据\nMapPartitions 算子需要传递一个迭代器，返回一个迭代器，没有要求的元素的个数保持不变， 所以可以增加或减少数据\n\n\n性能的角度\nMap 算子因为类似于串行操作，所以性能比较低\nmapPartitions 算子类似于批处 理，所以性能较高但是 mapPartitions 算子会长时间占用内存，那么这样会导致内存可能 不够用，出现内存溢出的错误。所以在内存有限的情况下，不推荐使用。使用 map 操作替代\n\n\n03、mapPartitionsWithIndex04、flatMap扁平化处理，是不是听不懂，像这种list里面嵌套了list的结构，就可以用这种方式去做处理把List(List(1,2), List(3,4))变成 List(1,2,3,4)\nval rdd = sc.makeRDD(List(List(1,2), List(3,4)),1) //\n\nrdd.flatMap(\n list => &#123;\n     //do something\n     new_list\n &#125;\n)\n\n\n\n\n\n05、glom06、groupBy07、filter08、sample09、distinct10、coalesce11、repartition12、sortBy2. 双value类型01、intersection02、union03、subtract04、zip3. key-value类型01、partitionBypackage com.atguigu.bigdata.spark.core.rdd.operator.transform\n\nimport org.apache.spark.rdd.RDD\nimport org.apache.spark.&#123;HashPartitioner, SparkConf, SparkContext&#125;\n\nobject Spark14_RDD_Operator_Transform &#123;\n  def main(args: Array[String]): Unit = &#123;\n    val sparkConf = new SparkConf().setMaster(\"local[*]\").setAppName(\"Operator\")\n    val sc = new SparkContext(sparkConf)\n\n    //TODO Key-Value类型\n\n    val rdd = sc.makeRDD(List(1, 2, 3, 4))\n\n    val mapRDD: RDD[(Int, Int)] = rdd.map((_, 1)) //把数值变成了turple类型，对偶元组可以使用了\n\n    //该方法不是rdd的是pairrddfunctions方法\n    //RDD[T]中T的写法就直接写里面的元素的类型\n    //scala中的隐式转换，查找转换规则，可以简单理解为二次编译\n\n    /*\n    implicit def rddToPairRDDFunctions[K, V](rdd: RDD[(K, V)])\n    (implicit kt: ClassTag[K], vt: ClassTag[V], ord: Ordering[K] = null): PairRDDFunctions[K, V] = &#123;\n    new PairRDDFunctions(rdd)\n  &#125;\n  隐式函数，rdd转换为PairRDDFunctions 遵循了OCP开发原则开放封闭原则（OCP，Open Closed Principle）是所有 面向对象 原则的核心。 软件设计本身所追求的目标就是**封装变化、降低耦合**，而开放封闭原则正是对这一目标的最直接体现。 其他的设计原则，很多时候是为实现这一目标服务的，例如以Liskov替换原则实现最佳的、正确的继承层次，就能保证不会违反开放封闭原则。 软件实体应该是可扩展，而不可修改的。\n\n     */\n    //改变数据所在的分区\n    mapRDD.partitionBy(new HashPartitioner(2))\n      .saveAsTextFile(\"output\")\n\n    //ctrl + h查看当前所在类的实现类\n\n\n\n\n    //打开之后24一个分区，13一个分区，为什么呢？\n    /*\n    def getPartition(key: Any): Int = key match &#123;\n    case null => 0\n    case _ => Utils.nonNegativeMod(key.hashCode, numPartitions)\n  &#125;\n  def nonNegativeMod(x: Int, mod: Int): Int = &#123;\n    val rawMod = x % mod\n    rawMod + (if (rawMod &lt; 0) mod else 0)\n  &#125;\n     */\n\n\n\n    sc.stop()\n  &#125;\n\n&#125;\n\n02、reduceByKey粗解：\n相同的key分在同一个组，对其value做两两聚合\n如下述情况，聚合方式写在了注释中\n格式：\n//一般形式\nrdd.reduceByKey(\n    (x, y) => &#123;\n        //将xy聚合的操作，如x + y\n        x + y\n    &#125;\n)\n//简写只需要写两个数的操作方式\nrdd.reduceByKey(_+_)\n\n\n\npackage com.atguigu.bigdata.spark.core.rdd.operator.transform\n\nimport org.apache.spark.rdd.RDD\nimport org.apache.spark.&#123;HashPartitioner, SparkConf, SparkContext&#125;\n\nobject Spark15_RDD_Operator_Transform &#123;\n  def main(args: Array[String]): Unit = &#123;\n    val sparkConf = new SparkConf().setMaster(\"local[*]\").setAppName(\"Operator\")\n    val sc = new SparkContext(sparkConf)\n\n    //TODO Key-Value类型\n    val rdd = sc.makeRDD(List(\n      (\"a\", 1), (\"a\", 2), (\"a\", 3), (\"b\", 4)\n    ))\n\n    //需求：相同的key分在同一个组，对其value做聚合\n    //相同的key的数据进行value的聚合操作 又涉及到了分组的概念\n    //scala中聚合操作一般都是两两聚合，spark也是\n    //[1,2,3]\n    //[3,3]\n    //[6]\n    //reduceByKey中，如果key的数据只有一个，是不会参与运算的\n\n    //底层用了hashpartition\n    val reduceRDD = rdd.reduceByKey(\n      (x, y) => &#123;\n        println(s\"x=$x,y=$y\")\n        x + y\n      &#125;\n    )\n\n    reduceRDD.collect().foreach(println)\n\n\n    sc.stop()\n  &#125;\n\n&#125;\n\n03、groupByKey将数据源中相同key的数据分在一个组中，形成一个对偶元组 (key, 相同key的value的可迭代集合)\n格式：\nrdd.groupByKey() //直接按照key来分组，底层用到了HashPartitioner\n\n\n\npackage com.atguigu.bigdata.spark.core.rdd.operator.transform\n\nimport org.apache.spark.&#123;SparkConf, SparkContext&#125;\n\nobject Spark16_RDD_Operator_Transform &#123;\n  def main(args: Array[String]): Unit = &#123;\n    val sparkConf = new SparkConf().setMaster(\"local[*]\").setAppName(\"Operator\")\n    val sc = new SparkContext(sparkConf)\n\n    //TODO Key-Value类型\n    val rdd = sc.makeRDD(List(\n      (\"a\", 1), (\"a\", 2), (\"a\", 3), (\"b\", 4)\n    ))\n\n    //将数据源中相同key的数据分在一个组中，形成一个对偶元组\n    //(key, 相同key的value的可迭代集合)\n    //(a,CompactBuffer(1, 2, 3))\n    //(b,CompactBuffer(4))\n\n    val groupRDD = rdd.groupByKey()\n\n\n    //groupBy\n    //\n    //val groupRDD = rdd.groupBy(_._1)\n\n    groupRDD.collect().foreach(println)\n\n    sc.stop()\n  &#125;\n\n&#125;\n/*\n使用什么分组不确定，groupby要传进去，groupbykey不用\n得到的结果而言：一个是(key,value的集合)\n一个是(key,(key,value)的集合)\n */\n\n/*\ngroupByKey会导致数据打乱重组，涉及shuffle\nspark中的shuffle操作必须要落盘处理，不能在内存中数据等待，否则会越积越多，导致内存溢出\n落盘涉及磁盘I/O，会影响性能，所以shuffle的性能很低\n */\n\n/*\nreduceByKey支持分区内预聚合功能，可以有效减少shuffle时的落盘量，提高性能\n */\n\n与groupBy的区别\npackage com.atguigu.bigdata.spark.core.rdd.operator.transform\n\nimport org.apache.spark.&#123;SparkConf, SparkContext&#125;\n\nobject Spark16_RDD_Operator_Transform &#123;\n  def main(args: Array[String]): Unit = &#123;\n    val sparkConf = new SparkConf().setMaster(\"local[*]\").setAppName(\"Operator\")\n    val sc = new SparkContext(sparkConf)\n\n    //TODO Key-Value类型\n    val rdd = sc.makeRDD(List(\n      (\"a\", 1), (\"a\", 2), (\"a\", 3), (\"b\", 4)\n    ))\n      \n    val groupRDD = rdd.groupBy(_._1)\n\n    groupRDD.collect().foreach(println)\n\n    sc.stop()\n  &#125;\n\n&#125;\n\n\n\n\n\n角度\ngroupBy\ngroupByKey\n\n\n\n分组方式\n传参\n不用\n\n\n结果\n(key,value的集合)\n(key,(key,value)的集合)\n\n\n\n\n\n\n\n面试题：reduceByKey和groupByKey的区别\n\n\n\n\n\n\n\n\n\n\nspark中的shuffle操作必须要落盘处理，不能在内存中数据等待，否则会越积越多，导致内存溢出落盘涉及磁盘I&#x2F;O，会影响性能，所以shuffle的性能很低\nreduceByKey支持分区内预聚合功能，可以有效减少shuffle时的落盘量，提高性能\n\n\n\n\n角度\nreduceByKey\ngroupByKey\n\n\n\n从 shuffle 的角度\nreduceByKey 和 groupByKey 都存在 shuffle 的操作，但是 reduceByKey 可以在 shuffle 前对分区内相同 key 的数据进行预聚合（combine）功能，这样会减少落盘的 数据量，reduceByKey 性能比较高。\n而 groupByKey 只是进行分组，不存在数据量减少的问题\n\n\n从功能的角度\nreduceByKey 其实包含分组和聚合的功能所以在分组聚合的场合下，推荐使用 reduceByKey\nGroupByKey 只能分组，不能聚合如果仅仅是分组而不需要聚合那么还是只能使用 groupByKey\n\n\n04、aggregateByKey05、foldByKey06、combineByKey07、sortByKey08、join09、leftOuterJoin10、cogroup5.1.4.4 案例实操5.1.4.5 行动算子1、reduce2、collect3、count4、first5、take6、takeOrderd7、aggregate8、fold9、countByKey10、save相关算子11、foreach5.1.4.6 RDD序列化5.1.4.7 RDD依赖关系5.1.4.8 RDD持久化一些报错及解决1、报错奇奇怪怪的：����: �Ҳ������޷���������在maven的runner中\n\n","slug":"spark","date":"2022-04-08T12:55:34.000Z","categories_index":"","tags_index":"spark","author_index":"Cencus"},{"id":"a5d8f8aa75a85d4533d622d4e92bd3f1","title":"scala","content":"第1章  入门基于JVM，与Java关系密切\nJava是一门先编译后解释的语言\nScala与java一样\n特点：融合怪\n面向对象 函数式编程 静态类型（类型提前声明指定）编程语言\n\n多范式  结合了面对对象与函数式编程\n.scala会被编译为java字节码.class，然后运行在JVM之上，并可以调用现有的Java类库，实现两种语言的无缝对接\n\n环境搭建步骤\n\n确保有Java\nhttps://www.scala-lang.org/download/2.12.1.html 下载scala2.12.1.zip文件\n解压配置环境变量\n\n\n\n\n\n\n\n\n\n\n编译：scalac xxx.scala\n执行：scala xxx\n编译生成了两个文件xxx.class和xxx$.class\n第2章 变量和数据类型2.1注释变量与常量\n能够用常量就不要用变量\n```\n\n## 2.4 字符串输出\n\n```scala\n//全代码\npackage chapter02\n\nobject Test04_String &#123;\n  def main(args: Array[String]): Unit = &#123;\n    val age = 18\n    val name = \"欧婉华\"\n    //printf，占位符，%d,%s，例如printf(\"%d岁的%s在尚硅谷学习\",age,name)\n\n\n    //字符串模板(+插值字符串，通过$获取变量值)，上面的太麻烦了，直接把变量引用到字符串内容中\n    //格式：print(s\"$&#123;var&#125;\")\n    println(s\"$&#123;age&#125;岁的$&#123;name&#125;喜欢小狗\")\n\n    val num:Double = 3.1415\n    println(s\"The num is $&#123;num&#125;\") //格式化模板字符串\n    println(f\"The num is $&#123;num&#125;%2.2f\") //格式化输出\n    println(raw\"The num is $&#123;num&#125;%2.2f\") //将变量填进来，后面的字符串原来输出\n\n    //三引号表示字符串，保持多行字符串的原格式输出\n    var str = s\"\"\"\n       |I\n       |Love\n       |Oliva\n       |\"\"\".stripMargin\n    println(str)\n  &#125;\n&#125;\n\n\n\n\n\nprintf+占位符printf(\"%d岁的%s在尚硅谷学习\",age,name)\n\n\n\n插值表达式println(s\"$&#123;age&#125;岁的$&#123;name&#125;喜欢小狗\")\n\n\n\n格式化输出println(f\"The num is $&#123;num&#125;%2.2f\") //格式化输出\n\n\n\n不对字符串内的特殊字符做解析println(raw\"The num is $&#123;num&#125;%2.2f\") //将变量填进来，后面的字符串原来输出\n\n多行字符串的原格式输出var str = s\"\"\"\n       |I\n       |Love\n       |Oliva\n       |\"\"\".stripMargin\n\n不会有那个|\n第3章  忘了是什么章节了和Java没差，没什么可写的\n第4章  流程控制4.1 if else和Java相同的部分就不说了，说一个不一样的，ifelse的返回值，看代码\nvar a: String = if (age >= 18)&#123;\n      \"成年\"\n    &#125;else&#123;\n      \"未成年\"\n    &#125;\n\n返回条件分支的最后一行语句，可以用做三目运算符\nval res = if (age > 18) \"成年\" else \"未成年\"\n\n\n\n\n\n4.2 for循环语法for (i &lt;- start to end)&#123;\n    //循环体\n    /*\n    i：循环变量，和java里面的i\n    &lt;-：将后面生成的东西依次赋值给i\n    start to end 一种方法调度，start.to(end)，生成一个从start到end的一个集合\n    */\n    ...\n&#125;\n\n范围遍历for (i &lt;- start to end)&#123;&#125;\n\n\n\n不想包含结束位置 untilfor(i &lt;- start until end)&#123;&#125;\n\n这样就不会包含end的集合\nforeach的集合遍历//foreach，集合遍历\n   for (i &lt;- Array(1, 2, 3)) &#123;\n     println(i)\n   &#125;\n   for (i &lt;- List(1, 2, 3)) &#123;\n     println(i)\n   &#125;\n   for (i &lt;- Set(1, 2, 3)) &#123;\n     println(i)\n   &#125;\n\n\n\n\n\n循环守卫 if听起来很高大上其实就是java里的continue\nfor (i &lt;- 1 to 10 if i != 5) &#123;\n\tprintln(i)\n&#125;\n\n解释：如果i不等于5就执行循环体，否则就执行等价于\nfor (int i = 0; i &lt; 10; i++)&#123;\n    if (i == 5) continue;\n    ...\n    \n&#125;\n\n\n\n循环步长 by//by的类型：正数，负数，小数，但就是不能是0\n//循环步长\n    for (i &lt;- 1 to 10 by 2) &#123;\n      println(i)\n    &#125;\n    //本质\n    /*\n        for (i &lt;- (1 to 10).by(2))&#123;\n\n        &#125;*/\n\n    println(\"=================\")\n    //步长小于0\n    for (i &lt;- 20 to 1 by -2) &#123;\n      println(i)\n    &#125;\n    //反转遍历 10到1\n    for (i &lt;- 10 to 1 by -1) &#123;\n\n    &#125;\n\n    //或者\n    for (i &lt;- 1 to 10 reverse) &#123;\n\n    &#125;\n\n    //by的类型：正数，负数，小数，不能是0\n    for (i &lt;- 1.0 to 10.0 by 0.5) &#123;\n\n    &#125;\n\n\n\n循环嵌套java里面遍历二维数组时，我们需要用到循环嵌套\nfor (int i = 0; i &lt; rows; i++)&#123;\n\tfor(int j = 0; j &lt; colums; j++)&#123;\n        //do something\n        \n    &#125;\n\n&#125;\n\n在scala中我们这样写\nfor (i &lt;- 0 to rows; j &lt;- 0 to colums)&#123;\n    //do something\n&#125;\n\n简洁的极致了属于是\nfor的返回值这玩意可有意思了，一般情况下for的返回值就是unit，说白了就是空值，但是我们可以用yield关键字去指定返回值，注意，循环有那么多元素，返回值也不是一个单值，而是一个vector\nval b = for (i &lt;- 1 to 10) yield i*i\n\n有什么用呢，这个过程就是相当于对于数组的每一个元素做了操作然后返回，用java怎么写呢\nfor (int i = 0; i &lt; arr.length; i++)&#123;\n    doSomething(arr[i]);\n&#125;\n\n那看到scala更有趣一点，直接将操作完的赋值，天生就适合大数据场景，非常的nice\n4.3 while与dowhile和java里一样的就不说了，注意在scala中能用for就不要用while\n同时while也没有返回值\n循环中断scala中没有break和continue那肯定有替换方案\n对于continue之前的循环守卫已经讲了\n对于break，scala中用类似抛出异常的方式，去做break，或者用Breaks（本质上还是封装了那个抛出，只是还会执行接下来的代码）\nBreaks.breakable(\n      for (i &lt;- 0 until 5) &#123;\n        if (i == 3 )\n          Breaks.break()\n        println(i)\n      &#125;\n\n或者在导包的时候直接：import scala.util.control.Breaks._然后代码中省略Breaks.\nbreakable(\n      for (i &lt;- 0 until 5) &#123;\n      if (i == 3 )\n        break()\n      println(i)\n     &#125;\n    )\n\n多重循环案例：输出九重妖塔\n&#96;&#96;&#96;\n\n\n\n# 第5章 函数式编程\n\n## 5.1 函数基础\n什么是函数式编程，刚打完游戏脑子蒙的也不想动脑子，反正这个函数就和数学里面的函数一样 y &#x3D; f(x)，传进来一个x，返回一个结果y，那和面向过程不是没差别嘛，我也不知道。感觉至少这玩意说是给数学好的人用的，他们用的习惯。\n\n\n\n### 5.1.1 函数基础\n\n基本语法\n\n![](https:&#x2F;&#x2F;gitee.com&#x2F;cencus&#x2F;blog-image&#x2F;raw&#x2F;master&#x2F;blogImage&#x2F;1649501424975.png)\n\n### 5.1.2 函数与方法的区别\n\n不晓得\n\n### 5.1.3 函数定义\n\n前面有，就那样定义的\n\n无非就是这样变一下那样变一下\n\n### 5.1.4 函数参数\n\n（1）可变参数  &#96;参数名: 类型*&#96;\n\n&#96;&#96;&#96;scala\ndef f1(str: String*): Unit &#x3D; &#123;\n\tprintln(str)\n&#125;\n\nf1(&quot;alice&quot;)\nf1(&quot;aaa&quot;, &quot;b&quot;, &quot;c&quot;)\n\n\n\n（2）如果参数列表中存在多个参数，那么可变参数一般放置在最后 \n//多个参数，可变参数要放到最后\n    def f2(str1: String, str2: String*): Unit =&#123;\n      println(s\"str1=$str1, str2=$str2\")\n    &#125;\n\n    f2(\"alice\")\n    f2(\"a\", \"b\", \"c\")\n\n\n\n（3）参数默认值，一般将有默认值的参数放置在参数列表的后面\n有没有想过为什么，如果不放最后，那就会歧义嘛 \n//给参数默认值，也放在最后\ndef f3(age: Int, name: String = \"atguigu\"): Unit =&#123;\n\t...\n&#125;\n\n（4）带名参数\n//带名参数，和默认值连用，就不用考虑顺序了\ndef f4(name: String = \"atguigu\", age: Int): Unit =&#123;\n  println(s\"$&#123;age&#125;岁的$&#123;name&#125;爱刘熙\")\n&#125;\nf4(\"Oliva\",18)\nf4(name = \"lx\",age = 50)\nf4(age = 123)\n\n\n注释里说了，和默认值连用，就不要考虑顺序，之前写程序不是老有很多默认值，就是这样的，只需要对需要改变的值操作就行\n5.1.5 函数至简原则这玩意简称鬼都不认识原则，是真的化简到了极致，情况还比较多，后面有精力总结一下吧哎 todo\n\n\n\n\n\n\n\n\n\n函数至简原则：能省则省\n 1）至简原则细节 \n（1）return 可以省略，Scala 会使用函数体的最后一行代码作为返回值 \n（2）如果函数体只有一行代码，可以省略花括号 \n（3）返回值类型如果能够推断出来，那么可以省略（:和返回值类型一起省略） \n（4）如果有 return，则不能省略返回值类型，必须指定 \n（5）如果函数明确声明 unit，那么即使函数体中使用 return 关键字也不起作用 \n（6）Scala 如果期望是无返回值类型，可以省略等号 \n（7）如果函数无参，但是声明了参数列表，那么调用时，小括号，可加可不加 \n（8）如果函数没有参数列表，那么小括号可以省略，调用时小括号必须省略 \n（9）如果不关心名称，只关心逻辑处理，那么函数名（def）可以省略\npackage chapter05\n\n//函数至简原则\nobject Test04_Simplify &#123;\n  def main(args: Array[String]): Unit = &#123;\n    def f0(name: String): String = &#123;\n      return name\n    &#125;\n\n    println(f0(\"atguigu\"))\n\n    //    （1）return 可以省略，Scala 会使用函数体的最后一行代码作为返回值\n    def f1(name: String): String = &#123;\n      name\n    &#125;\n\n    println(f1(\"atguigu\"))\n\n    //    （2）如果函数体只有一行代码，可以省略花括号\n    def f2(name: String): String = name\n\n    println(f2(\"atguigu\"))\n\n    //    （3）返回值类型如果能够推断出来，那么可以省略（:和返回值类型一起省略）\n    def f3(name: String) = name //已经有了f(x) = x\n\n    println(f3(\"atguigu\"))\n\n    //    （4）如果有 return，则不能省略返回值类型，必须指定\n    //    def f0(name: String) = &#123;\n    //      return name\n    //    &#125;\n    //    （5）如果函数明确声明 unit，那么即使函数体中使用 return 关键字也不起作用\n    def f5(name: String): Unit = &#123;\n      return name\n    &#125;\n\n    println(f5(\"atguigu\"))\n\n    //    （6）Scala 如果期望是无返回值类型，可以省略等号\n    def f6(name: String) &#123;\n      println(name)\n    &#125;\n\n    f6(\"atguigu\")\n\n    //    （7）如果函数无参，但是声明了参数列表，那么调用时，小括号，可加可不加\n    def f7(): Unit = &#123;\n      println(\"atguigu\")\n    &#125;\n\n    f7()\n    f7\n\n    //    （8）如果函数没有参数列表，那么小括号可以省略，调用时小括号必须省略\n    def f8: Unit = &#123;\n      println(\"atguigu\")\n    &#125;\n\n    f8\n    //    f8() error\n    //    （9）如果不关心名称，只关心逻辑处理，那么函数名（def）可以省略匿名函数\n\n    //    (name: String) => &#123;println(name)&#125;\n\n\n  &#125;\n\n&#125;\n\n5.2 函数高级5.2.1 高阶函数—不是很高级的函数，就是函数的高级一点的用法todo直接不想写了，太累了，加个todo有时间再写\n1、函数作为值传递\n2、函数作为参数传递\n3、函数作为函数的返回值传递\n5.2.2 匿名函数这玩意也不是人，虽然之前学过\n(num: Int) => &#123;\n    //dosomething\n&#125;\n\n就张这样，没有名字，就是一个输入然后操作\n关键在于下面的特性：\n\n\n\n\n\n\n\n\n\n传递匿名函数至简原则： \n（1）参数的类型可以省略，会根据形参进行自动的推导 \n（2）类型省略之后，发现只有一个参数，则圆括号可以省略；其他情况：没有参数和参 数超过 1 的永远不能省略圆括号\n（3）匿名函数如果只有一行，则大括号也可以省略 \n（4）如果参数只出现一次，则参数省略且后面参数可以用_代替\n参数可以不写类型，只有一个参数可以不写圆括号，函数体只有一行可以不写大括号，参数可以用_作为代替\n(x,y) &#x3D;&gt; x*y\n&#x2F;&#x2F;直接可以简写成\n_*_\n&#x2F;&#x2F;相当于一个占位符那种感觉，然后按照参数的顺序往里面填，但是注意只能对于那种只用一次的参数\n\n\n\n5.2.3 高阶函数案例","slug":"scala","date":"2022-04-05T07:52:16.000Z","categories_index":"","tags_index":"语言，Scala，大数据","author_index":"Cencus"},{"id":"9db7d7b85a5cd901f6957d349aad6c0c","title":"思辨","content":"2020年7月2日21:15:20\n从今日思考：局外人是不是一定比局内人看得更加清楚？为什么问题分析：这个问题的本质是一个不同的角度看待同一个问题会有不同的结果，局内人就是事件的参与者，局外人可能是一个跟他毫不相干的人，可以是他的亲人，可以是他想成为的人，等等，看得更加清楚的意思是在以他的角度看来这样是更合理的，一个毫无干系的人来看更容易给出一个中肯的答案。\n不同的角度看待同一个问题会有不同的结果明日：每日思考有什么用？\n越想做好越做不好？是诅咒还是什么？不是想做好就能做好，想做好是很重要的事情也不是说你连想法都没有，而是我觉得应该是要耐心地告诉自己：”亲爱的，我知道这个结果对你来说很重要你真的很想要达到。我觉得这是很棒的，追求着一些东西，同时要相信自己，做成一件事永远都是很简单的，不用想的太复杂，保持平心静气地去做就好，自然就知道该做什么，该怎么做“，此刻我们再次提及卡夫卡的那段话放在这里。\n\n\n\n\n\n\n\n\n\n努力想要得到什么东西，其实只要沉着镇静，实事求是，就可以轻易地，神不知鬼不觉地达到目的。而如果过于使劲，闹得太凶，太幼稚，太没有经验，就哭啊，抓啊，拉啊，像一个小孩扯桌布，结果却是一无所获，只不过把桌上的好东西都扯到地上，永远也得不到了。卡夫卡《城堡》\n不用担心，我相信你我爱你\n映象情绪，心，头脑，冥想，《内部运作》，原生家庭\n情境1：早上老师讲笑话然后无论任何方面效率开心\n\n头脑暴政\n\n事物感知的颗粒度\n\n内部运作：头脑指挥与心的放飞，有节奏的音符\n\n当你有心的参与，事情才刚刚开始\n\n“你想要推开我”，“是的”（心的奇怪的感觉）\n\n“你是不是你要我了”，”是的，我害怕伤害到你“（又一次心的奇怪的感觉，如浪潮）\n\n心的奇怪的感觉我大概知道是什么了，堵在心上的一块石头，将心的的情感泛滥都堵住了，平时就是无感，一旦像上述两条事情，浪潮会变大，石头会堵不住，会流进心里一点。那种奇怪的感觉就是流进心里的那些加上石头堵住的感觉\n\n好像那种好的大脑与心的状态对我来说会更难一些\n\n先去冥想一会儿吧\n\n\nstep1：接纳自己心里有一块大石头，接纳他的存在\nstep2：与自己的内心反复的共舞——听起来可能是最不好理解最难的\nstep3：量变引起质变\n关于宝的所有好像很乱，很多东西，有没有可能，我们不用去解决旧的东西，直接去创造新的东西。\n这样好像是最方便的\n值得一试\n关心则乱越想要解决问题，问题就会越顽固\n不是不解决问题，而是放下与之对抗的想法，合作，甚至允许自己失眠，并从中找到力量&#x3D;&gt;提前解决明天的问题\nSomething做自己\n所有的路都是走向自己\n","slug":"思辨","date":"2022-03-31T13:03:51.125Z","categories_index":"","tags_index":"thinking","author_index":"Cencus"},{"id":"cfa64a171d4a3ced7e8787ba1d28f1d6","title":"小程序知识","content":"15 数组和对象循环数组定义：\nArrayListName:[数组项1,数组项2],\n\n\n\n\n数组项的类型可以不相同\n\n数组项可以是对象\n\n\n循环\nwx:for&#x3D;&quot;&#123;&#123;数组或者对象&#125;&#125;&quot;\nwx:for-item&#x3D;&quot;循环项的名称&quot;\nwx:for-index&#x3D;&quot;循环项的索引&quot;\n\nwx:key提高性能：\nwx:key&#x3D;&quot;唯一值&quot;\n谓一致\n\n唯一值：\n1.绑定string，那么string’s name\n嵌套循环：\n16 block\n占位符\n写代码能看到\n页面渲染会移除掉block只留下里面的内容和标签\n\n17 条件渲染\nwx:if&#x3D;”Infinity“\n\ntrue：显示\nfalse：隐藏\n&lt;view wx:if&#x3D;&quot;&#123;&#123;&#125;&#125;&quot;&gt;hidden&lt;&#x2F;view&gt;\n&lt;view wx:elif&#x3D;&quot;&#123;&#123;&#125;&#125;&quot;&gt;hidden&lt;&#x2F;view&gt;\n...\n&lt;view wx:else&gt;hidden&lt;&#x2F;view&gt;\n\n\n\n\n\nhidden\n\n隐藏：\n&lt;view hidden&gt;hidden&lt;&#x2F;view&gt; 等效于\n&lt;view hidden&#x3D;&quot;&#123;&#123;true&#125;&#125;&quot;&gt;hidden&lt;&#x2F;view&gt;\n显示：\n&lt;view hidden&#x3D;&quot;&#123;&#123;false&#125;&#125;&quot;&gt;hidden&lt;&#x2F;view&gt;\n\n\n什么场景用什么\n\n标签不会频繁切换显示 优先用：wx:if\n\n\n\n\n\n\n\n\n\n\n直接把标签从当前页面结构给移除掉\n\n当标签频繁切换显示 优先用：hidden\n\n\n\n\n\n\n\n\n\n\n通过添加样式的方式来切换显示，本质是加一个display：none，所以不要和display一起使用\n\n\n18 事件绑定changeInput(e)&#123;\n    this.setData(&#123;\n      num:e.detail.value,\n    &#125;)\n  &#125;\n\nbindtap:相当于click事件\n19 样式wxss20样式导入@import导入，只支持相对路径\n@import &quot;filepath&quot;;\n\n21选择器不支持通配符*\n*&#123;\n\tm0;\n\tp0;\n&#125;\n\n解法：罗列全部\n支持的选址器\n和css类似\n22 less把wxss后缀名先改成.less最后会自动编译成wxss\n\n定义使用less变量\n嵌套\n天生支持导入样式表\n\n如何安装less\n\n编辑器是vscode\n安装插件easy less】\n在vs code设置中加入如下配置：点击设置\n\n\"less.compile\":&#123;\n\t\"outExt\": \".wxss\"\n&#125;\n\n23 view textviewtext\n只能嵌套text：\n长按可以复制（只有哦该标签有此功能）\n可以对 空格（@nbsp;） 回车（真回车）等等 进行编码\n\n\n\n\n属性名\n类型\n默认值\n说明\n\n\n\nselectable\n布尔\nfalse\n文本是否可选\n\n\ndecode\n布尔\nfalse\n是否解码\n\n\nimage默认宽高\n\nweight:320px\nheight:240px\n\nlazy-load：直接支持懒加载会自己判断 当图片出现在饰扣上下三屏的高度之内时，自己开始加载图片\nswiper  1.轮播图的外层容器就是swiper\n  2.轮播项就是swiper-item\n  3.swiper存在默认样式\n​    height：150px，高度不能被图片撑开\n​    weight：100%，\n  4.图片也是默认的值：320*240\n  5.先找原图的宽高 等比例 找swiper的宽高\n  swiper的宽 &#x2F; swiper的高 &#x3D; 原图的宽 &#x2F; 原图的高\n  swiper的高 &#x3D; swiper的宽 * 原图的高 &#x2F;原图的宽(一般都是宽高，所以是750rpx第二个数&#x2F;第一个数)\nnavigator导航组件：navigator：超链接\n\n相当于一个块级元素 可以直接添加宽度和高度\n\nurl：要跳转的路径（绝对或者相对）\n\ntarget：要跳转到当前的小程序（self默认），还是其他小程序(miniProgram)\n\nopertype\n​    1.navigate：默认值 保留当前页面 跳转到应用内的某个不是tabbar的页面，如果是tabbar那么不会报错但是不能跳转\n​    2.redirect：保留当前页面 跳转到应用内的某个不是tabbar的页面\n​    3.switchTab：跳转到tabbar并关闭其他非tabbar页面\n​    4.reLaunch：关闭所有页面，跳到一个任意页面\n​    5.exit：退出小程序，最好配合target&#x3D;miniprogram\n\n\nrich-text富文本标签：rich-text\n\n通过nodes属性来实现\n​    1. 接收标签字符串（就是一段标签构成的字符串）\n2. 接收对象数组\n\n&lt;rich-text nodes=\"&#123;&#123;item.goods_intro&#125;&#125;\">&lt;/rich-text>\n\n其中的goods_intro部分：\n&#x2F;&#x2F;goods_intro\n&quot;&lt;div class&#x3D;&quot;123&quot;&gt;我是内容&lt;&#x2F;div&gt;&quot;\n\niconicon\n\ntype：success|success_no_circle|info|warn|waiting|cancel|download|search|clear\n\nsize：表示图标大小\n\ncolor：同css的颜色\n\n\nradio单选框 radio\n\n颜色\n\n配合父元素radio-group使用\n\nvalue 选中单选框的值\n\n绑定change事件\n\n\ncheckbox复选框自定义组件需求\n\n我写一个abc就有一个椭圆形的图片框\n\ntaobao导航模块&#x3D;&#x3D;代码存在公用的时候，提取出来成为自己的组件&#x3D;&#x3D;\n\n创建：\n\n\n\n\n\n\n\n\n\n\n\ncomponents -&gt; 标签名 -&gt; 新建component\n\n声明\n\n\n\n\n\n\n\n\n\n\n哪个页面要用，就在那个页面的json中用\n&#123;\n  \"usingComponents\": &#123;\n    \"tabs\":\"../../components/tabs/tabs\"\n  &#125;\n&#125;\n\n\n使用\n\n&lt;tabs&gt;&lt;&#x2F;tabs&gt;\n\n\n\n\n\n父向子传参\n父组件（页面） 通过标签方式 向子组件 传递数据\n​    属性名：aaa；\n​    属性值：123\n\n在子组件上进行接收properties中接收，然后在子组件的wxml中用\n\n\neg:\n//父页面中：\n&lt;tabs  tabs=\"&#123;&#123;tabs&#125;&#125;\">&lt;/tabs>\n//属性名tabs，属性值tabs\n\n//父JS中\n    tabs:[&#123;\n      id:0,\n      name:\"首页\",\n      isActive:true,\n    &#125;,\n    &#123;\n      id:1,\n      name:\"原创\",\n      isActive:false,\n    &#125;,\n    &#123;\n      id:0,\n      name:\"分类\",\n      isActive:false,\n    &#125;,\n    &#123;\n      id:0,\n      name:\"关于\",\n      isActive:false,\n    &#125;\n  \n  ]\n子JS中：\nproperties: &#123;//注意是properties而不是什么data\n      tabs:&#123;\n        type:Array,//类型String，Array\n        value:[]//值，都一般是空的\n\n      &#125;\n  &#125;,\n  子页面中\n  &lt;view \n        class=\"title_item &#123;&#123;item.isActive?'active':''&#125;&#125;\" \n        wx:for=\"&#123;&#123;tabs&#125;&#125;\" \n        wx:key=\"id\"\n        data-index=\"&#123;&#123;index&#125;&#125;\"\n        bindtap=\"handleItemTap\"\n        >\n        &#123;&#123;item.name&#125;&#125;\n        &lt;/view>   \n\n\n\n\n\n子向父传递\n//子获取到数据\n//自定义组件的method函数中\nfunc()&#123;\n    let value = index;\n    this.triggerEvent(\"父组件的自定义事件名\",要传递的参数);//eg：this.triggerEvent(\"itemChange\",index);\n&#125;\n\n//父组件中：\n&lt;tab binditemChange=\"handleItemChange\">&lt;/tab>\njs：\nhandleItemChange(e)&#123;\n\te.detai即为传过来的值\n&#125;\n\n动态渲染需要实时更新data数据\nthis.data(&#123;\n\tkey:value,\n&#125;);\n\n\n\n\n\n\n\n\n\nflex布局（弹性布局）原始浮动：块级元素同行显示\n\n\n\n优势\n劣势\n\n\n\n兼容性好\n复杂，局限\n\n\n移动端比较适合用flex，PC端可能兼容性不多好\n用于 替换浮动\n两个概念：\n\nflex container：包裹\n\nflex布局父项常见属性\n\nflex-direction：设置主轴的方向\njustify-content：设置主轴上子元素的排列方式\nflex-wrap：设置子元素是否换行\nalign-content：设置侧轴上子元素的排列方式（多行）\nalign-items：设置侧轴上子元素的排列方式（单行）\nflex-flow：复合属性，相当于同时设置了flex-direction和flex-wrap\n\njustify-content：设置主轴上子元素的排列方式\n注意：使用前一定要确定主轴方向\n\n\n\n属性\n说明\n\n\n\nflex-start\n默认值 从头部开始 如果主轴是x轴 则从左到右\n\n\nflex-end\n从尾部开始排列\n\n\ncenter\n在主轴居中对齐（如果主轴是x轴，则水平居中）\n\n\nspace-around\n平分剩余空间\n\n\nspace-between\n*先两边贴边 再平分剩余空间\n\n\nalign-items：设置侧轴上子元素的排列方式（单行）\n该属性是控制子项在侧轴（默认y轴）上的排列方式 在子项单行时使用\n\n\n\n属性\n说明\n\n\n\nflex-start\n默认值 从上到下\n\n\nflex-end\n从下到上\n\n\ncenter\n挤在一起居中（垂直居中）\n\n\nstretch\n拉伸\n\n\nalign-content 设置侧轴上的子元素的排列方式（多行）\n设置子项在侧轴上的排列方式并且只能用于子项出现换行的情况，在单行下是没有用的\n\n\n\n\n\n\n\n\nflex-start\n默认值在侧轴的头部开始排列\n\n\nflex-end\n在侧轴的尾部开始排列\n\n\ncenter\n在侧轴中间显示\n\n\nspace-around\n子项在侧轴平分剩余空间\n\n\nspace-between\n子项在侧轴先分布在两头 在平分剩余空间\n\n\nstretch\n设置子项元素高度平分父元素高度\n\n\nalign-content与align-items区别\n\nalign-items适用于单行情况，只有上对齐，下对齐，居中和拉伸\nalign-content适应于换行（多行）的情况，单行无效，可以设置上对齐，下对齐，居中，拉伸，以及平均分配剩余空间等属性值\n单行用items多行用content\n\n子项常见属性：\n\nflex：表示所占的份数，用于布局\nalign-self：指定某一个^子元素在&#x3D;&#x3D;侧轴上的&#x3D;&#x3D;的排列\norder:指定元素在主轴上的顺序，越小越靠前\n\nheimaGo商城（微小项目）打开Ali字体图标库\nhttps://www.iconfont.cn/?spm=a313x.7781069.1998910419.d4d0a486a\n搜索框中搜素需要的字体图标-&gt;鼠标移至想要的图标上再点击购物车-&gt;点击右上方购物车图标-&gt;添加至项目-&gt;font_class-&gt;复制链接到浏览器-&gt;复制全部到style下的新建的iconfont.wxss-&gt;然后在类中引入class&#x3D;”index gouwuche”\n自定义组件\n复习\n轮播图相关知识：swiper及其相关属性 swiper-item data数组 循环渲染 定义全局主题颜色\n相关知识复盘\nswiper\n\nswiper存在默认属性:\nwidth:100%\nheight:150px\n\n问题：在屏幕更大一点的时候height不够把图片显示完全，此时需要让swiper的高度去适应图片高度\n方法：①查看原图宽高\n​            ②(750&#x2F;image_width)*image_height\nswiper的常用属性：\nautoplay\nindicator-dots\ncircular\n\n\n\n\n\n\n\n\n\n自动播放\nautoplay\n\n\n显示小圆点\nindicator-dots\n\n\n循环播放让切换更加自然\ncircular\n\n\n参见：https://developers.weixin.qq.com/miniprogram/dev/component/swiper.html\n\nswiper-item\n\n这个具体没有什么特别需要说的，嵌套在swiper中，作为放置image和navigator的一个容器，只是有个循环渲染的问题\n&lt;swiper>\n    &lt;swiper-item\n\twx:for=\"&#123;&#123;data中定义的数组名&#125;&#125;\"\n\twx:key=\"&#123;&#123;数组中定义的关键字&#125;&#125;\"\n\t>\n    &lt;/swiper-item>\n&lt;/swiper>\n\n\n\ndata中的数组\n\n定义：数组名:[数组元素]\n数组元素的类型：所有类型，其中对象类型需要特别说明\n//对象用花括号框起来，就像结构体一样的\n&#123;\n\timage_src:\"...\",\n\tgoods_id:xxx\n&#125;,\n\n\nimage\n\nimage标签存在默认值\nwidth:320px;\nheight:240px\n问题在于很多图片都不是这个样子，所以image一般需要自己定义属性值，宽 100% 高自适应，这就涉及到mode属性\nmode属性：本次用到的widthFix：宽度固定高度自适应，其他的详见：\nhttps://developers.weixin.qq.com/miniprogram/dev/component/image.html\n另外，写样式表的时候的步骤\n①改对应页面的wxss为less\n②写一种层叠结构\n.index_swiper&#123;\n    /*index_swiper的样式区*/\n\tswiper&#123;\n        /*swiper的样式区*/\n\t\timage&#123;\n\t\t\t/*image的样式区*/\n\t\t&#125;\n\t&#125;\n&#125;\n\n③在对应的区域写对应的样式\n写全局主题颜色的方法：\n\n在app.wxss中的page里，定义统一的主题颜色\n\n//app.wxss\npage&#123;\n\t--themeColor:颜色值\n&#125;\n\n\n使用\n\ncolor:var(--themeColor);\n\n还存在的问题\n 老师用的是网页上request来获取图片，我是用的本地，需要学习一下怎么从网页拿东西\n\nonLoad: function(options)&#123;\n    //发送异步请求获取轮播图数据\n    wx.request(&#123;\n      url: 'https://api-hmugo-web.itheima.net/api/public/v1/home/swiperdata',\n      success: (result)=>&#123;//成功的回调的函数\n        console.log(result)\n      &#125;\n    &#125;);\n&#125;\n\n\n 导航栏不能用主题颜色\n\nrequest相关的部分：\nhttps://developers.weixin.qq.com/miniprogram/dev/api/network/request/wx.request.html\n改request为promise\n&#x2F;&#x2F;在request中放置promise代码\n&#x2F;&#x2F;或者在util里面放也可\nexport const 方法名&#x3D;()&#x3D;&gt;&#123;\n    return new Promise((resolve,reject)&#x3D;&gt;&#123;\n    &#x2F;&#x2F;要采用Promise的方法 可以是小程序原生的方法\n        wx.getSetting(&#123;\n            success: (result)&#x3D;&gt;&#123;\n                resolve(result);\n            &#125;,\n            fail: (err)&#x3D;&gt;&#123;\n                reject(err);\n            &#125;\n        &#125;);\n    &#125;)\n&#125;\n\n大胆猜测这个params是一个对象，在传参的时候会串一个对象，然后...params解析一下，然后就可以拿里面的东西用了\nlet ajaxTimes &#x3D; 0;\nexport const request&#x3D;(params)&#x3D;&gt;&#123;\n    &#x2F;&#x2F;显示加载中效果\n    ajaxTimes++;\n    wx.showLoading(&#123;\n        title: &#39;加载中&#39;,\n        mask: true,&#x2F;&#x2F;没法进行其他操作\n    &#125;);\n    const baseUrl&#x3D;&quot;https:&#x2F;&#x2F;api-hmugo-web.itheima.net&#x2F;api&#x2F;public&#x2F;v1&quot;;\n    return new Promise((resolve,reject)&#x3D;&gt;&#123;\n        wx.request(&#123;\n           ...params,\n           url:baseUrl+params.url,\n           success:(result)&#x3D;&gt;&#123;\n               resolve(result.data.message);\n           &#125;,\n           fail:(err)&#x3D;&gt;&#123;\n               reject(err);\n           &#125;,\n           complete:()&#x3D;&gt;&#123;\n               ajaxTimes--;\n               if(ajaxTimes&#x3D;&#x3D;&#x3D;0)&#123;\n                wx.hideLoading();\n               &#125;\n           &#125;\n        &#125;);\n    &#125;)\n&#125;\n&#x2F;&#x2F;在需要用的地方引入\nimport &#123; request &#125; from &quot;..&#x2F;..&#x2F;request&#x2F;index.js&quot;;\nimport &#123; getSetting,chooseAddress,openSetting,showModel &#125; from &quot;..&#x2F;..&#x2F;utils&#x2F;asyncWx.js&quot;;\nasync fun_name()&#123;\n\tconst 参数 &#x3D; await 引入的方法名()\n&#125;\n\n\n\n es6\n分类导航知识点：弹性布局，padding与marging\n楼层知识点：嵌套循环渲染，less的使用，选择器的使用，浮动与浮动清除\nimage放在navigator的里面\n\n嵌套循环渲染\n\n问题：为什么这里会用到嵌套循环渲染？\n答：因为这里有三个楼层（女装，运动，箱包），每个楼层里面分了很多种类型，这就需要嵌套循环\n&lt;view class=\"floor_group\"\nwx:for=\"&#123;&#123;floorList&#125;&#125;\"\nwx:for-item=\"item1\"\nwx:for-index=\"index1\">//来循环楼层\n    &lt;navigator\n     wx:for=\"&#123;&#123;item1.productList&#125;&#125;\"\n     wx:for-item=\"item2\"\n     wx:for-index=\"index2\">\n        &lt;image src=\"...\">&lt;/image>\n    &lt;/navigator>\n&lt;/view>\n\n\n\nless的使用\n本次实例用到的：直接计算宽高\n单位：vw，类似于百分比\n选择器的使用：\n本节中涉及的：\n&amp;:nth-last-chile(-n+4)&#123;&#x2F;&#x2F;选中后四个\n&#125;\n&amp;:nth-child(序号)[,&amp;nth-child()]&#123;&#125;\n\n.floor_list&#123;\n            overflow: hidden;\n            navigator&#123;\n                float: left;\n                width: 30%;\n                    &#x2F;&#x2F; 后四个链\n                &amp;:nth-last-child(-n+4)&#123;\n                    &#x2F;**&#x2F;\n                    &#x2F;&#x2F; 232&#x2F;386&#x3D;33.33vw&#x2F;height\n                    height:30vw*386&#x2F;232&#x2F;2;\n                    border-left:10rpx solid #fff;\n                &#125;\n                &amp;:nth-child(3),\n                &amp;:nth-child(4)&#123;\n                    border-bottom: 10rpx solid #fff;\n                &#125;\n                image&#123;\n                    width: 100%;\n                    height: 100%;\n                &#125;\n            &#125;\n        &#125;\n\n浮动：\nfloat:lef\nhttps://www.w3school.com.cn/cssref/css_selectors.asp\n分类页面：知识点：\n\nscroll-view的使用，\n用flex布局来居中，\n用类来实现动态效果，\njs编写函数的方式，\n绑定事件的方式，\nwx:key&#x3D;”*this”\n\nscroll-view作为父组件来让子组件可以滑动\n&lt;scroll-view>\n    &lt;view class=\"menu_item\"\n     wx:for=\"&#123;&#123;leftMenuList&#125;&#125;\"\n     wx:key=\"*this\">&lt;/view>\n&lt;/scroll-view>\n\nwx:key=&quot;*this&quot;:key等于循环项自身\nflex布局来让子元素完全居中\n&#123;\n    display:flex;\n    justify-content:center;\n    align-item:center;\n&#125;\n\n\n\n有关flex的总结详见：flex布局\n\n父向常见属性\n子项常见属性\n\n使用async await来替代.then\n全局变量写在data后面函数前面：\ndata&#123;\n\n&#125;,\n&#x2F;&#x2F;全局变量区\n\n\n\nonLoad:function()&#123;\n\n&#125;\n\n\n\n准备：勾选es6转es5，勾选增强编译，在函数前面加上async\nasync getCates()&#123;\n\tconst res = await request(&#123;url:\"...\"&#125;);//用常量res来存储获取接口后的数据\n    this.Cates = res.message.data;//存入本地的全局变量\n    \n    \n&#125;\n\n\n\n编写函数的一般步骤\n\n找到要绑定事件的元素\n给元素加上bind^xxx&#x3D;”function_name”，考虑是否传参，如果需要加上：data-参数名&#x3D;”“\n在js中书写函数\n\nfunction_name(e)&#123;\n&#125;\n\n4. \n还存在的问题\n\n async await\n\n商品列表页面\n子组件向父组件传参，\n\n让父组件监听事件\n\nforEach方法\n\n解构\n\nslot\n\n\n&lt;image mode=\"widthFix\" src=\"&#123;&#123;item.goods_small_logo?item.goods_small_logo:'../../images/empty_img.jpg'&#125;&#125;\" />\n\n商品列表动态渲染request带参请求：\nconst res = await request(&#123;url:\"/goods/search\",data:this.QueryParams&#125;);\n\n加载下一页效果Math.ceil(value)函数\n例如：Math.ceil(2.3)&#x3D;3，此处用于计算总页数（数据条数&#x2F;页容量）\n\n 可以尝试了解一下其他的math函数，这个函数时向上取整\n\nsetData中的数组拼接\nthis.setData(&#123;\n\tArray:[...原数组名,...要添加的数组名];&#x2F;&#x2F;eg:goodsList:[...this.data.goodsList,...res.goods]\n&#125;)\n\n\n\nwx-showToast方法：展示一个过会儿就消失的对话框\nwx.showToast(&#123;\n   title: '没有下一页数据了'\n&#125;);\n\n下拉刷新效果json中加属性的方法：不需要加什么window直接写属性名:属性值\n&#123;\n\t\"\":\"dark\"\n&#125;\n\n2.下拉刷新的逻辑结构：本质是清除所有数据，重新请求数据，请求的时候要把页码置1\n\n下拉刷新事件:需要再json中开启一个配置项\n\n&#123;\n\t\"enablePullDownRefresh\":true,//是否打开下拉刷新\n\t\"backgroundTextStyle\":\"dark\"//是否出现下拉刷新的那三个点点样式\n&#125;\n\n\n\n 找到下拉刷新的事件onPullDownRefresh，书写逻辑：\n\n重置商品数组\n\nthis.setData(&#123;\n\tgoodsList:[]\n&#125;)\n\n\n页码置1\n\n数据请求好了关闭刷新效果\n\n\n两个地方都可以加：\n①在getGoodsList()函数的最后\n②在onPullDownRefresh()函数的最后\n加上：\nwx.stopPullDownRefresh();\n\n添加全局的正在加载的图标发送请求前显示正在加载中的图标，加载完毕后关闭图标\nconst、let、var的区别let 的用法类似于 var，但是 let 只在所在的代码块内有效（局部变量），所以我们一般使用 let 替代 var。而 const 用来声明常量。\n总结：\n\n\n\n声明\n用途\n\n\n\nlet\n变量\n\n\nconst\n常量\n\n\n添加的位置，request里面的index.js\n&#123;\n\t&#x2F;&#x2F;\t开头\n    wx.showLoading(&#123;\n      title: &#39;加载中&#39;,\n      mask:true&#x2F;&#x2F;不让页面能被操作\n    &#125;)\n\n\tcomplete()&#x3D;&gt;&#123;\n\t\twx.hideLoading()\n\t&#125;\n&#125;\n\n此时注意index页面发送了很多请求，这些请求几乎是同时发生，也就是说，展示”加载中“的对话框会一起展示但是在数据回来的时候，可能一个先回来，然后就关闭页面，此时其他的数据没有回来，没有被渲染，影响体验，此时我们加一个计数器，进来几个退出的时候就要全部退出我才关闭对话框\n//优化\n&#123;\n\t//\t开头\n\tlet ajaxTimes = 0;\n\t&#123;\n\t\tajaxTimes++;\n\t\twx.showLoading(&#123;\n          title: '加载中',\n          mask:true//不让页面能被操作\n        &#125;)\n\n        complete()=>&#123;\n            ajaxTimes--;\n            if(ajaxTimes===0)&#123;\n            \twx.hideLoading()\n            &#125;\n        &#125;\n\t&#125;\n    \n&#125;\n\n30.商品详情\nnavigator传参：\n\n&lt;navigator url=\"/pages/good_list/index?cid=&#123;&#123;item2.cat_id&#125;&#125;\">&lt;/navigator>\n\n发请求就要用到：\nimport &#123;request&#125; from \"../../request/index.js\"\n\n\n\n\n\n31.商品详情-接口数据和页面分析富文本\n像素单位：\n\n\n\n书写\n用途\n\n\n\npx\n\n\n\nem\n\n\n\npt\n\n\n\nvw\n表示相对视口宽度（viewport width)，1vw&#x3D;1%*视口宽度\n\n\nvh\n表示相对视口宽度（viewport height)，1vh&#x3D;1%*视口宽度\n\n\nrpx\nresoponse px\n\n\nfont-weights\n格式化：shift+alt+F\n商品详情——优化动态渲染只拿需要的数据\nthis.setData(&#123;\n          goodsDetail:&#123;\n            goods_name:goodsInfo.goods_name,\n            goods_price:goodsInfo.goods_price,\n            pics:goodsInfo.pics,\n            goods_introduce:goodsInfo.goods_introduce\n          &#125;\n        &#125;);\n\n\n\n在富文本中存在：https://image.suning.cn/uimg/sop/commodity/152418403963754690151350_x.jpg?from=mobile&amp;format=80q.webp\n这种webp格式，但是iPhone不能识别这种格式，所以如何临时自己改\n确保后台存在：1.webp&#x3D;&gt;1.jpg\n前端简单的字符串替换：使用正则表达式\ngoods_introduce:goodsInfo.goods_introduce.replace(/\\.webp/g,'.jpg')\n\n商品详情——放大预览图片步骤分析\n\n给swiper-item加点击事件：bindtap\n放大预览：利用小程序的api \n给去预览图片的数组\n获取被点击的索引，显示对应的预览图\n\n&#x2F;&#x2F;wxml\n&lt;swiper-item data-index&#x3D;&quot;&#123;&#123;index&#125;&#125;&quot;&gt;\n&#x2F;&#x2F;js\nconst urls &#x3D; 对应的图片列表;&#x2F;&#x2F;eg:const urls &#x3D; this.data.goodsObj.pics.map(v&#x3D;&gt;v.pics_mid);\nconst i &#x3D; index;\nwx.previewImage(&#123;\n  current: urls[i], &#x2F;&#x2F; 当前显示图片的http链接\n  urls: urls &#x2F;&#x2F; 需要预览的图片http链接列表\n&#125;)\n\n\n\n相关细节：https://developers.weixin.qq.com/miniprogram/dev/api/media/image/wx.previewImage.html\n.map(v&#x3D;&gt;v.xxx)\n商品详情——底部工具栏position有关的知识\n此处用到了fix，relative，absolute，父相子绝\nhttps://developer.mozilla.org/zh-CN/docs/Web/CSS/position\nview.iconfont利用style里面的类名来设置图标\n底部工具栏会挡到内容，此时需要把内容向上提：\npage{\n​    padding-bottom:导航栏高度\n}\n障眼法来解决让view像button一样\n方法：让button继承view的宽高，再让他的透明度为0\n&#x2F;&#x2F;父组件：\n.tool_item&#123;\n\tposition:relative;\n\tbutton&#123;\n\t\twidth:100%;\n\t\theight:100%;\n\t\tposition:absolute;\n\t\tleft:0;\n\t\ttop:0;\n\t\topacity:0;\n\t&#125;\n&#125;\n\n\n\n购物车在tabbar里面，要想用navigator跳转就要注意open-type的参数应该是switchtab，详见navigator\n商品详情——加入购物车逻辑：\n点击事件：在事件中处理，假如购物车的本质应该是商品id，也就是说购物车页面动态渲染一个商品id的数组\n通过id获得对应的名称和价格，本质是要加到购物车页面的数组里，这里就要用到拼接了\n\n绑定点击事件\n\n获取本地的购物车缓存\n\nwx.getStorgeSync(“key”)\n\n\n查看缓存里有没有这个商品\nfindIndex函数\n数组名.findIndex(v&#x3D;&gt;v.goods_id&#x3D;&#x3D;&#x3D;this.data.goodsDetail.goods_id);\n函数作用：返回第一个满足条件的项的索引，如果没有就返回-1\n\n\n\n3.1 如果有那么点加入购物车就让这个商品的数量变多(自增)\n3.2 如果没有(index&#x3D;-1)那么久存到购物车数组里,\ncart.push(数组名)\n\n把缓存存进去\n\nwx.setStorgeSync(“key”,value)\n\n\n显示弹窗\n\n\nwx.showToast()\n购物车button按钮的属性：\nwx-choooseAddress\nwx-getSetting发现属性名很怪异的时候都要使用[]来获取属性值\n\n\n//这里要获取scope[]\nconst scopeAddr = authSetting[\"scope.address\"]\n\nwx-openSetting\n代码优化，如何导入多个异步同步函数\nimport {1,2,3} from “..&#x2F;.&#x2F;util&#x2F;async.js”\n对象哪怕是一个空对象的bool类型都是true但是可以拿属性那做\n:nth-child(){}\n购物车——列表静态样式快速生成html树\n工具：vscode、插件css tree\n用法：选中需要写less的标签结构，按住ctrl+shift+p输入gen，选择就可以，然后cv操作\ndisplay: -webkit-box;\noverflow: hidden;\n-webkit-box-orient: vertical;\n -webkit-line-clamp: 1;\n\n购物车为空使用\nblock\n问题：怎么给block里面的元素加样式\n格式化代码:Shift + Alt + F.\nshowtoast的icon参数\n","slug":"MiniPrograme","date":"2022-03-31T13:03:51.115Z","categories_index":"","tags_index":"","author_index":"Cencus"},{"id":"b4a715d3920019ff67db4366ba3b8790","title":"JQuery","content":"dom对象与jQuery对象\ndom对象\n\nvar dom = document.getElementById();//原生js获取到的对象就是dom对象\n\n特点：只能调用dom的方法与属性，不能调用jQuery的方法与属性\ndom.style.backgroundcolor = 'red';//调用dom的属性与方法\ndom.css('backgroundcolor','red');//不能调用dom的方法属性\n\n\njQuery对象\n\n利用jQuery获取的对象就是…\nvar $div01 = $('#one');//通过id\n$div01.css()\n\n\n\n特点：不能互通的\njQuery对象是一个伪数组，有下标有length但是不是数组\n证明：\nconsole.log($div1.__proto__ === Array.__proto__);\n\ndom对象与jQuery对象的互相转换\ndom -&gt; jQuery ：用$把dom对象包起来\nvar $obj = $(dom对象);\n\njQuery -&gt; dom ：1.下标，2.get()；\n//method1 ：下标\nvar div1 = $jQuery对象[下标]//下标0序\n//method2 ：get()方法\nvar div1 = $jQuery对象.get(下标)\n\ntext()\n获取文本\n$('obj').text();\n\n注：假如obj有子代，那么也会获取到子代的文本（div下的p）\n\n设置文本\n$(&#39;obj&#39;).text(&quot;String&quot;);\n\n注：\n\nString会覆盖原来的内容，&#x3D;&#x3D;如果设置的文本中包换标签，是不会解析该标签的&#x3D;&#x3D;\n如果选择器选择了多个,那么都会被设置，并且不会保留子代文本\n\n\n\ncss()\n获取样式\n\n格式：$('obj').css('想要获取的样式名');\n\n\n\n\n\n设置样式\n\n设置单个样式\n\n\n\n格式：$('obj').css('想要设置的样式名'，'样式值');\n\n\n设置多个样式\n\n   格式:$('obj').css(&#123;\n\txxx:xxx,\n\txxx:xxx\n  &#125;);   \n\n基本选择器\n\n\n选择器\n用途\n\n\n\n$(‘#id’)\nid选择器\n\n\n$(.class’)\n类选择器\n\n\n$(‘TagName’)\n标签名选择器\n\n\n$(‘obj1,obj2,obj3’)\n并集选择器\n\n\n$(‘obj1 obj2 obj3’)\n交集选择器\n\n\n层次选择器\n\n\n选择器\n用途\n\n\n\n$(‘#father &gt; #son’)\n获取ID为father的所有子son\n\n\n\n\n\n\n\n\n\n\n过滤选择器\n这类选择器都带冒号\n\n\n\n\n名称\n用法\n描述\n\n\n\n:eq(index)\n$(“li:eq(2)”).css(‘color’,’red);\n从获取的li元素中，选择索引号为2的元素，索引从0开始\n\n\n:odd\n$(“li:odd”).css(‘color’,’red);\n奇数\n\n\n:even\n$(“li:even”).css(‘color’,’red);\n偶数\n\n\n筛选选择器（方法）\n和过滤选择器类似，但是这个主要是方法\n\n\n\n\n名称\n用法\n描述\n\n\n\nchildren(selector)\n$(‘ul’).children(‘li’)\n相当于$(‘ul-li’)，子类选择器\n\n\nfind(selector)\n$(‘ul’).find(‘li’)\n相当于$(‘ul li’)，后代选择器\n\n\nsiblings(selector)\n$(‘#first’).siblings(‘li’);\n查找兄弟节点，不包括自己本身\n\n\nparent()\n$(‘#first’).parent()\n查找父亲\n\n\neq(index)\n$(‘li’).eq(2)\n相当于$(‘li:eq(2)’),index从0开始\n\n\nnext()\n$(‘li’).next()\n找下一个兄弟\n\n\nprev()\n$(‘li’)prev()\n找上一个兄弟\n\n\n案例：下拉菜单\nmouseover 事件在鼠标移动选取的元素及其子元素上时触发\nmouseenter 鼠标只在鼠标移动到选取元素上时触发\n//案例：突出显示“\n../ 代表当前文件的上一级目录\n./” 代表当前文件所在目录\n&lt;script src=\"jquery-3.4.1.js\">&lt;/script>\n&lt;script>\n    //获取所在的li\n    $(function() &#123;\n        //需求1：给小人物加一个鼠标移入时间，当前(this)li的透明度为1，其他兄弟的透明度为0.4\n        //需求2：鼠标离开大盒子，所有li的透明度改成1\n\n        $('.wrap').find('li').mouseenter(function() &#123;\n            $(this).css('opacity', 1).siblings('li').css('opacity', 0.4);\n        &#125;)\n        $('ul').mouseleave(function() &#123;\n            $('ul').find('li').css('opacity', 1);\n        &#125;)\n\n    &#125;);\n&lt;/script>\n\n.show：显示（相当于display：block）\n.hidden：隐藏（相当于display：none）\njquery：隐式迭代，链式编程（在于一个方法返回的是一个jquery对象，就可以继续点jquery方法）\n淘宝服饰精品$(‘li’).index()方法获取索引，从0开始\n$(&#39;.center&gt;li&#39;).eq(index)\n$(&#39;.center&gt;li:eq(&#39;+index+&#39;)&#39;)&#x2F;&#x2F;做字符串拼接\n\n\n\n.show()有返回值，返回本身\n.show().siblings().hidden()\n.attr(src,’src’)\n&lt;!DOCTYPE html>\n&lt;html lang=\"en\">\n\n&lt;head>\n    &lt;meta charset=\"UTF-8\">\n    &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    &lt;title>Document&lt;/title>\n&lt;/head>\n&lt;style>\n    *&#123;\n        margin: 0;\n        padding: 0;\n    &#125;\n    .wrapper&#123;\n        width: 500px;\n        height: 200px;\n        margin: 100px auto;\n        display: flex;\n        justify-content: space-around;\n        border: 1px solid pink;\n\n\n    &#125;\n    .left,.right,.center&#123;\n        list-style: none;\n    &#125;\n    .left&#123;\n        flex:1;\n    &#125;\n    .center&#123;\n        flex:3;\n\n    &#125;\n    .right&#123;\n        flex:1;\n    &#125;\n&lt;/style>\n\n&lt;body>\n    &lt;div class=\"wrapper\">\n        &lt;ul class=\"left\">\n            &lt;li>&lt;a href=\"\">链接&lt;/a>&lt;/li>\n            &lt;li>&lt;a href=\"\">链接&lt;/a>&lt;/li>\n            &lt;li>&lt;a href=\"\">链接&lt;/a>&lt;/li>\n            &lt;li>&lt;a href=\"\">链接&lt;/a>&lt;/li>\n            &lt;li>&lt;a href=\"\">链接&lt;/a>&lt;/li>\n            &lt;li>&lt;a href=\"\">链接&lt;/a>&lt;/li>\n            &lt;li>&lt;a href=\"\">链接&lt;/a>&lt;/li>\n            &lt;li>&lt;a href=\"\">链接&lt;/a>&lt;/li>\n        &lt;/ul>\n        &lt;ul class=\"center\">\n            &lt;li>&lt;a href=\"\">1&lt;/a>&lt;/li>\n            &lt;li>&lt;a href=\"\">2&lt;/a>&lt;/li>\n            &lt;li>&lt;a href=\"\">3&lt;/a>&lt;/li>\n            &lt;li>&lt;a href=\"\">4&lt;/a>&lt;/li>\n            &lt;li>&lt;a href=\"\">5&lt;/a>&lt;/li>\n            &lt;li>&lt;a href=\"\">6&lt;/a>&lt;/li>\n            &lt;li>&lt;a href=\"\">7&lt;/a>&lt;/li>\n            &lt;li>&lt;a href=\"\">8&lt;/a>&lt;/li>\n            &lt;li>&lt;a href=\"\">1&lt;/a>&lt;/li>\n            &lt;li>&lt;a href=\"\">2&lt;/a>&lt;/li>\n            &lt;li>&lt;a href=\"\">3&lt;/a>&lt;/li>\n            &lt;li>&lt;a href=\"\">4&lt;/a>&lt;/li>\n            &lt;li>&lt;a href=\"\">5&lt;/a>&lt;/li>\n            &lt;li>&lt;a href=\"\">6&lt;/a>&lt;/li>\n            &lt;li>&lt;a href=\"\">7&lt;/a>&lt;/li>\n            &lt;li>&lt;a href=\"\">8&lt;/a>&lt;/li>\n        &lt;/ul>\n        &lt;ul class=\"right\">\n            &lt;li>&lt;a href=\"\">链接&lt;/a>&lt;/li>\n            &lt;li>&lt;a href=\"\">链接&lt;/a>&lt;/li>\n            &lt;li>&lt;a href=\"\">链接&lt;/a>&lt;/li>\n            &lt;li>&lt;a href=\"\">链接&lt;/a>&lt;/li>\n            &lt;li>&lt;a href=\"\">链接&lt;/a>&lt;/li>\n            &lt;li>&lt;a href=\"\">链接&lt;/a>&lt;/li>\n            &lt;li>&lt;a href=\"\">链接&lt;/a>&lt;/li>\n            &lt;li>&lt;a href=\"\">链接&lt;/a>&lt;/li>\n        &lt;/ul>\n    &lt;/div>\n\n&lt;/body>\n\n&lt;/html>\n&lt;script src=\"jquery-3.4.1.js\">&lt;/script>\n&lt;script>\n    $(function() &#123;\n        //需求1：给左边的li设置鼠标移入事件，让对应的li显示，其他的隐藏\n        //需求2：给右边的li设置鼠标移入事件，让对应的li显示，其他的隐藏\n        //解决需求1:为什么不给a标签设置鼠标移入时间，而要给li标签\n        $('.left>li').mouseenter(function()&#123;\n            //获取当前li的索引\n            let index = $(this).index();\n            console.log($('.center>li').eq(index).show().siblings('li').hide());\n        &#125;);\n\n        $('.right>li').mouseenter(function()&#123;\n            let index = $(this).index();\n            index+=$('.left>li').length;\n            console.log($('.center>li').eq(index).show().siblings('li').hide());\n\n        &#125;)\n\n    &#125;);\n&lt;/script>\n\n\n\nClass 类操作_01没有button只能用input来实现type\n添加类添加单个类addClass(类名1)\n添加多个类addClass(类名1 类名2)\n移除类移除单个.removeClass(类名);\n移除多个移除所有.removeClass();\n判断类hasClass()：\n切换类.toggleClass()：没有这个类就添加，有就删除\ntab栏切换&lt;!DOCTYPE html>\n&lt;html lang=\"en\">\n&lt;head>\n    &lt;meta charset=\"UTF-8\">\n    &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    &lt;title>Document&lt;/title>\n&lt;/head>\n&lt;style>\n    ul&#123;\n        width: 500px;\n        height: 30px;\n        list-style: none;\n        display: flex;\n        justify-content: space-between;\n        margin:20px auto;\n        background-color: #ccc;\n\n    &#125;\n    ul li&#123;\n        width: 50px;\n        height: 100%;\n        background-color: aqua;\n    &#125;\n    .main&#123;\n    \tdisplay:none;\n    &#125;\n    .active&#123;\n        border-top: 3px solid red;\n    &#125;\n    .selected&#123;\n    \tdisplay:block;\n\t&#125;\n&lt;/style>\n&lt;body>\n    &lt;ul>\n        &lt;li>1&lt;/li>\n        &lt;li>2&lt;/li>\n        &lt;li>3&lt;/li>\n    &lt;/ul>\n&lt;/body>\n&lt;/html>\n&lt;script src=\"jquery-3.4.1.js\">&lt;/script>\n&lt;script>\n    $(function()&#123;\n        $('ul>li').mouseenter(function()&#123;\n            console.log($(this).addClass('active').siblings('li').removeClass('active'))\n            let index = $(this).index();\n            $('.main').eq(index).addClass('selected').sibling('.main').removeClass('selected')\n        &#125;)\n\n        $('ul').mouseleave(function()&#123;\n            $('li').removeClass('active')\n        &#125;)\n    &#125;);\n&lt;/script>\n\n\n\n\n\njquery动画\n显示：\n\n显示和隐藏\n\n显示\n\n$('#div1').show('slow')\n\n\n如果show没有参数就没有动画效果\n\n如果想要动画效果就要给他参数：\n\n参数1：代表执行动画的时长可以是毫秒数，也可以是代表时长的三个字符串（fast:200ms normal:400ms slow:600ms）写错了，也行此时相当于Normal\n参数2：动画执行完后的回调函数\n\n\n隐藏\n $('#div1').hide\n\n$(‘#div1’).hide(2000);\n\n切换\n\n\n$('#div1').toggle(1000)//\n\n\n\n如果元素隐藏就动画显示，否则反之\n\n 有没有回调函数\n\n&lt;!DOCTYPE html>\n&lt;html lang=\"en\">\n&lt;head>\n    &lt;meta charset=\"UTF-8\">\n    &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    &lt;title>Document&lt;/title>\n&lt;/head>\n&lt;style>\n    div&#123;\n        width: 200px;\n        height: 200px;\n        background-color: red;\n        display: none;\n    &#125;\n&lt;/style>\n&lt;body>\n    &lt;input type=\"button\" value=\"隐藏\" id=\"hide\">\n    &lt;input type=\"button\" value=\"显示\" id=\"show\">\n    &lt;input type=\"button\" value=\"切换\" id=\"toggle\">\n    &lt;div id=\"div1\">&lt;/div>\n&lt;/body>\n&lt;/html>\n&lt;script src=\"jquery-3.4.1.js\">&lt;/script>\n&lt;script>\n    $(function()&#123;\n        //1.显示\n        $('#show').click(function()&#123;\n            //给div1显示出\n            //  1.1如果show没有参数就没有动画效果\n            //  1.2如果想要动画效果就要给他参数\n            /*  参数1：代表执行动画的时长可以是毫秒数，也可以是代表时长的三个字符串（fast:200ms normal:400ms slow:600ms）写错了，也行此时相当于Normal\n                参数2：动画执行完后的回调函数\n                */\n            $('#div1').show('slow');//200ms\n\n            \n\n\n        &#125;)\n\n        //2.隐藏\n        $('#hide').click(function()&#123;\n            //$('#div1').hide(2000);//\n            //回调函数\n            $('#div1').hide(2000,function()&#123;\n                alert();\n            &#125;)\n        &#125;)\n\n\n        //3.toggle\n        //如果元素隐藏就动画显示，否则反之\n        $('#toggle').click(function()&#123;\n            //$('#div1').hide(2000);//\n            //回调函数\n            $('#div1').toggle(1000)//\n        &#125;)\n\n\n        \n    &#125;);\n&lt;/script>\n\n\n\n\n滑入\n\nslideDown()方法\n\n向下滑动元素\n参数1：speed：”slow”,”fast”,ms数，可省略\n参数2：回调函数\n\nslideup()方法\n\nslideToggle()方法\n\n\n淡入\n\n\nfadeIn\n\n自定义动画\n\n.click(function()&#123;\n\t$('#div1').animate(&#123;prop:value&#125;,speed)\n&#125;)\n\n.animate(prop,time,speed,callback)\n\n参数1：必选 需要做动画的属性，\n参数2：时长，可选\n参数3：easing：可选的，代表缓动还是匀速：linear swing  linear:匀速  swing：慢-快-慢  默认：swing\n参数4：动画执行完毕后的回调函数：可以在里面又做动画定义\n\n动画队列以及stop参数如果你去鬼畜一个动画，他当前做不了就会加入动画队列，然后就会很鬼畜\n方法：stop方法\n解法：在同一个元素上执行多个动画，后面的动画会被放到动画队列中，前面的执行完了才会执行\nstop(chearQueue,jumpToEnd)\n\nchearQueue：是否清除队列\njumpToEnd:是否跳转到最终效果\n\n不写参数默认两个false\nJquery节点操作\n原生js创建节点\n\n\ndocument.write( )：打开标准流-&gt;添加-&gt;关闭\ninnerHtml：设置内容（包含标签）\ndocument.createElement(‘div’)\n\nJQuery中：\n\nhtml\n$()\n\n\nhtml()\n设置&#x2F;获取内容\n\" data-language=\"<>\">\">&lt;input &#x2F;&gt;\n&lt;input &#x2F;&gt;\n&lt;div id&#x3D;&#39;div1&#39;&gt;\n    &lt;p&gt;&lt;&#x2F;p&gt;\n&lt;&#x2F;div&gt;\n\n获取到元素的所有内容：\n$(&#39;#div1&#39;).html()&#x3D;&gt;&lt;p&gt;&lt;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;p&gt;\n\n设置内容：\n$(&#39;#div1&#39;).html(&quot;我是设置的内容&quot;)\n\n会覆盖\n\n$()方法\nvar $link&#x3D;$(&#39;&lt;a&gt;&lt;&#x2F;a&gt;&#39;)\n\n记住：\n\n$()里面的参数是字符串，同时像class&#x3D;”div”这样的里面的””都可以省略\n同时这样创建的元素只存在于内存中，如果要在页面上显示，就一定要追加\n\n$(&quot;#div1&quot;).append($link)\n\n清空节点empty(),remove()  &lt;!DOCTYPE html>\n&lt;html lang=\"en\">\n&lt;head>\n    &lt;meta charset=\"UTF-8\">\n    &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    &lt;title>Document&lt;/title>\n    &lt;script src=\"./jquery-3.4.1.js\">&lt;/script>\n&lt;/head>\n&lt;body>\n    &lt;input type=\"button\" value=\"按钮\" id=\"btn\" />\n    &lt;ul id=\"ul1\">\n        &lt;li>我是第1个li&lt;/li>\n        &lt;li>我是第2个li&lt;/li>\n        &lt;li id=\"li3\">我是第3个li&lt;/li>\n        &lt;li>我是第4个li&lt;/li>\n        &lt;li>我是第5个li&lt;/li>\n    &lt;/ul>\n&lt;/body>\n&lt;/html>\n\n 清空ul里面的所有元素\n$(&quot;#ul1&quot;).html(&quot;&quot;);\n\n 不推荐使用，不安全，li清掉了，但是事件还在，所以可能造成内存泄漏\n推荐使用\n$(&quot;#ul1&quot;).empty();\n\n不仅移除了元素。还清空了元素绑定的事件\n 移除某一个元素\n$(&quot;#li3&quot;).remove();\n\n\n\n看着像自杀,本质还是找到父亲，然后从父亲那边移除孩子\n克隆节点clone()&lt;!DOCTYPE html>\n&lt;html lang=\"en\">\n&lt;head>\n    &lt;meta charset=\"UTF-8\">\n    &lt;title>轮播案例-念龙&lt;/title>\n    &lt;link rel=\"stylesheet\" href=\"css/banner.css\">\n    &lt;script src=\"js/jquery-1.12.4.min.js\">&lt;/script>\n    &lt;script src=\"js/banner.js\">&lt;/script>\n    &lt;link rel=\"stylesheet\" href=\"http://at.alicdn.com/t/font_1903309_36ckhpth40m.css\">\n&lt;/head>\n&lt;body>\n    &lt;div class=\"banner\">\n        &lt;ul class=\"ul_list\">\n            &lt;li>&lt;img src=\"img/1.jpg\" alt=\"\">&lt;/li>\n            &lt;li>&lt;img src=\"img/2.jpg\" alt=\"\">&lt;/li>\n            &lt;li>&lt;img src=\"img/3.jpg\" alt=\"\">&lt;/li>\n            &lt;li>&lt;img src=\"img/4.jpg\" alt=\"\">&lt;/li>\n            &lt;li>&lt;img src=\"img/5.jpg\" alt=\"\">&lt;/li>\n        &lt;/ul>\n        &lt;div class=\"num\">\n            &lt;span class=\"on\">&lt;/span>\n            &lt;span>&lt;/span>\n            &lt;span>&lt;/span>\n            &lt;span>&lt;/span>\n            &lt;span>&lt;/span>\n        &lt;/div>\n        &lt;a href=\"javascript:;\" class=\"prev\">&lt;i class=\"iconfont icon-qiehuanqizuo\">&lt;/i>&lt;/a>\n        &lt;a href=\"javascript:;\" class=\"next\">&lt;i class=\"iconfont icon-qiehuanqiyou\">&lt;/i>&lt;/a>\n    &lt;/div>\n&lt;/body>\n&lt;/html>\n\n\n\n*&#123;\n    margin: 0;\n    padding: 0;\n&#125;\na&#123;\n    text-decoration: none;\n&#125;\n.banner&#123;\n    overflow: hidden;\n    width: 790px;\n    height: 340px;\n    margin: 100px auto;\n    position: relative;\n&#125;\n.banner ul&#123;\n    width: 3950px;/*5张图片的宽度，也可以写成500%*/\n    height: 340px;\n    position: absolute;\n&#125;\n.banner li&#123;\n    list-style-type: none;\n    float: left;\n    width: 790px;\n    height: 340px;\n&#125;\n/*如果图片大小有问题*/\n/*.banner img&#123;\n    width: 100%;\n    height: 100%;\n&#125;*/\n.banner .num&#123;\n    width: 100%;\n    height: 20px;\n    font-size: 0px;\n    position: absolute;\n    left: 0;\n    bottom: 10px;\n    text-align: center;\n&#125;\n.banner .num span&#123;\n    display: inline-block;\n    width: 15px;\n    height: 15px;\n    margin: 0 8px;\n    background-color: #fff;\n    border-radius: 50%;\n    cursor: pointer;\n    transition: all .3s;\n&#125;\n/*向左向右箭头*/\n.banner .prev&#123;\n    display: block;\n    width: 40px;\n    height: 74px;\n    background-color: rgba(0,0,0,0.2);\n    color: rgba(255,255,255,0.4);\n    position: absolute;\n    top: 50%;\n    margin-top: -37px;\n    left: 0;\n    text-align: center;\n    transition: background-color .3s;\n&#125;\n.banner .next&#123;\n    display: block;\n    width: 40px;\n    height: 74px;\n    background-color: rgba(0,0,0,0.2);\n    color: rgba(255,255,255,0.4);\n    position: absolute;\n    top: 50%;\n    margin-top: -37px;\n    right: 0;\n    text-align: center;\n    transition: background-color .3s;\n&#125;\n.banner .prev i,\n.banner .next i&#123;\n    font-size: 30px;\n    line-height: 74px;\n    transition: background-color .3s;\n&#125;\n.banner .prev:hover,\n.banner .next:hover&#123;\n    background-color: rgba(0,0,0,0.5);\n    color: #fff;\n&#125;\n.banner .num span:hover&#123;\n    background-color: #ccc;\n&#125;\n.banner .num span.on&#123;\n    width: 30px;\n    height: 15px;\n&#125;\n\n\n\n/*轮播图js*/\n/*封装play函数，利用定时器自动向左播放下一张*/\n$(function () &#123;\n    var timer = null;\n    var liW = $(\".banner ul li:first\").innerWidth();//求一个li的宽度\n    var n = 0;//用来管理给第几个span添加类.on\n    timer = setInterval(function () &#123;\n        /*\n            1.先将ul向左移动一个li的宽度，再将第一个li带出去\n            2.在执行完1后(形成一个动画队列)，将ul的left回归到0，再将带出去的li追加回ul的最后\n        */\n        play();\n    &#125;,2000)\n    function play() &#123;\n        $(\".banner ul\").animate(&#123;left: -liW&#125;,function () &#123;/*回调函数*/\n            $(this).css(\"left\",0).find(\"li:first\").appendTo(this);\n        &#125;)\n        n++;\n        if(n>4)&#123;/*if(n>$(\".banner ul li\").length)-1*/   /*$(\".banner ul li\").length)表示li的个数*/\n            n = 0;\n        &#125;\n        $(\".banner .num span\").eq(n).addClass(\"on\").siblings().removeClass(\"on\");\n    &#125;\n    /*当鼠标移动到轮播图区域，轮播暂停*/\n    $(\".banner\").hover(function () &#123;\n        clearInterval(timer);\n    &#125;,function () &#123;\n        timer = setInterval(function () &#123;\n            play();\n        &#125;,2000)\n    &#125;)\n    /*点击next向右箭头*/\n    $(\".next\").click(function () &#123;\n        play();\n    &#125;)\n    /*点击prev向左箭头*/\n    $(\".prev\").click(function () &#123;\n        /*\n            看当前显示的图片前面的\n            1.先把ul往左移出一个li的宽度，同时，将最后一个li追加到ul最前头\n            2.执行动画：ul的left变为0，将最后一个li带回\n        */\n        /*$(\".banner ul\").css(\"left\",-liW).find(\"li:last\").prependTo(this);*//*此处不能用this,当前this指的是prev按钮*/\n        $(\".banner ul\").css(\"left\",-liW).find(\"li:last\").prependTo(\".banner ul\");\n        $(\".banner ul\").animate(&#123;left:0&#125;)\n        /*解决点击prev时，span跟着一起变化*/\n        n--;\n        if(n&lt;0)&#123;\n            n = 4;/*n = $(\".banner ul li\").length)-1*/\n        &#125;\n        $(\".banner .num span\").eq(n).addClass(\"on\").siblings().removeClass(\"on\");\n    &#125;)\n    /*点击span进行切换\n       当前轮播到第几张，放在n中存储，当点击span时，会得到span的index值\n       1.判断得到的index值是大于n还是小于n\n       如果小于n：通过for循环执行n到index的次数(不包括index,++)的向左循环切换效果\n       如果大于n：通过for循环执行n到index的次数(不包括index,--)的向右循环切换效果\n       2.将index赋值给n，进行span的切换\n    */\n    $(\".banner .num span\").each(function (index) &#123;/*用each遍历*/\n        $(this).click(function () &#123;\n            if(n &lt; index)&#123;\n                for(var i = n;i&lt;index;i++)&#123;\n                    $(\".banner ul\").animate(&#123;left: -liW&#125;,100,function () &#123;/*回调函数*/\n                        $(this).css(\"left\",0).find(\"li:first\").appendTo(this);\n                    &#125;)\n                &#125;\n            &#125;else if(n>index)&#123;\n                for(var i = n;i>index;i--)&#123;\n                    $(\".banner ul\").css(\"left\",-liW).find(\"li:last\").prependTo(\".banner ul\");\n                    $(\".banner ul\").animate(&#123;left:0&#125;,100);\n                &#125;\n            &#125;\n            n = index;\n            $(\".banner .num span\").eq(n).addClass(\"on\").siblings().removeClass(\"on\");\n        &#125;)\n\n    &#125;)\n\n&#125;)\n\n","slug":"JQuery","date":"2022-03-31T13:03:51.098Z","categories_index":"","tags_index":"前端,编程","author_index":"Cencus"},{"id":"1e07bc0caee6be4abcaacceaca4d25ab","title":"java知识","content":"Java更新日志Day2&#x2F;14\n集合学习目标\n\n 使用集合存储数据\n 能遍历集合，把数据取出来\n 掌握每种集合的特性\n集合框架介绍avi\n\n\n\n注：集合框架的学习方式使用底层，底层应该是实例类\nCollection中的共性方法public boolean add(E e);\npublic void clear();\npublic boolean remove();\npublic boolean contains();\npublic boolean isEmpty();\npublic int size();\npublic Object[] toArray();\n\n\n\n\nIterator接口两个方法：\nhasNext()\nnext()\n\n\nwhile(it.hasNext())&#123;\n\tit.next()\n&#125;\n\n​    \n数据结构\n栈：FILO\n\n队列：先进先出：FIFO\n\n数组：查询快，增删慢\n\n链表：\n\n红黑树\n\n\nSet（java.util.Set)\n特点：\n\n不允许存储重复的元素\n\n没有索引，也不能使用普通for遍历\n\n\nHashSet(java.util.HashSet) implements Set\n\nset的特点\n无序集合，存取顺序可能不同\n\nList集合ArrayListArrayList：查询快，增删慢\nList&lt;String> lists = new ArrayList&lt;>();//只可以装String\n//在i索引位置插入元素\nlists.add(i,e);\n//根据索引删除元素，返回被删除元素\nlists.remove(i);\n//修改第i个元素为value\nlists.set(i,value)\n\nl\nLinkList特点：添加的元素 有序 可重复 有索引\n底层基于链表\nLinkedList&lt;E>;\n//\npublic void addFirst(E e);\npublic void addLast();\npublic E getFirst();\npublic E getLast();\npublic E removeFirst();\npublic E removeLast();\npublic E pop();\npublic void push(E e)\n\n\n\n\n\n\n\n\n\nvectorHashMap\n","slug":"Java更新日志","date":"2022-03-31T13:03:51.094Z","categories_index":"","tags_index":"编程,java","author_index":"Cencus"},{"id":"dc3ddb3c3e294cfbfa2182d222982083","title":"Java","content":"写在前面第一章 开发前言1.1 Java语言概述什么是JavaSun公司（Stanford University Netword）、高级编程语言\n\n发展史Java可以做什么\n网站\n电商\n快递\n…\n大数据（Hadoop）\n\n\n\n1.2计算机基础知识二进制\n字节cmd+notepad：打开一个记事本\nbit：0&#x2F;1byte：8个bit，数据存储的最小单位\n命令提示符MS-DOS（Microsoft Disk Operating System）：微软磁盘操作系统\nwindows中的命令提示符cmd，继承了大多数ms——dos\nwin+r -&gt; cmdcommandC:\\Users\\User：表示位置，在这个文件夹下进行操作\n常用命令\n\n\n\n命令\n解释\n\n\n\n切换盘符\n盘符:\n\n\n进入子文件夹\ncd 想进入的文件夹（如果开头唯一，那后面的可以省略，然后按tab键）\n\n\n返回上一级\ncd ..（根目录再往上找，就没有此电脑了，只能到根目录）\n\n\n一次性进入文件夹\ncd 路径1\\路径2\\路径3\\路径4\n\n\n一次性返回根目录\ncd \\\n\n\n查看当前目录有哪些文件（）\ndir\n\n\n清理屏幕\ncls(clear screen)\n\n\n关闭cmd\nexit\n\n\n此电脑是一个假想\n\n第二章\nJVM：实现跨平台，翻译官\nwindows版的JVM，linux版的JVM\n\n\n2.2JRE和JDK\nJRE：运行环境，包含JVM与核心类库\nJDK：Java Development KIt：开发工具包，包含JRE和开发人员使用的工具\n\n运行一个已有的Java程序，只需要JRE\n开发一个全新的Java程序，必须要JDK\n\nwww.oracle.com\nhttps://www.oracle.com/technetwork/cn/java/javase/downloads/java-archive-javase9-3934878-zhs.html\n双击图标安装\n注意两点\n\n","slug":"java","date":"2022-03-31T13:03:51.091Z","categories_index":"","tags_index":"java","author_index":"Cencus"},{"id":"00c0454f1d8e3895f071dd0779512fe7","title":"Hive","content":" 尚硅谷大数据技术之Hive\n第 1 章 Hive基本概念1 .1 什么是 Hive1 ） hive简介Hive：由Facebook开源用于解决海量结构化日志的数据统计工具。\nHive是基于Hadoop的一个数据仓库工具，可以将结构化的数据文件映射为一张表，并提供类SQL查询功能。\n2 ） Hive本质：将HQL转化成MapReduce程序（ 1 ）Hive处理的数据存储在HDFS（ 2 ）Hive分析数据底层的实现是MapReduce（ 3 ）执行程序运行在Yarn上1.2 Hive 的优缺点1.2.1 优点（ 1 ）操作接口采用类SQL语法，提供快速开发的能力（简单、容易上手）。—————————————————————————————————————（ 2 ）避免了去写MapReduce，减少开发人员的学习成本。（ 3 ）Hive的执行延迟比较高，因此Hive常用于数据分析，对实时性要求不高的场合。（ 4 ）Hive优势在于处理大数据，对于处理小数据没有优势，因为Hive的执行延迟比较高。（ 5 ）Hive支持用户自定义函数，用户可以根据自己的需求来实现自己的函数。1 .2.2 缺点1 ） Hive 的 HQL 表达能力有限（ 1 ）迭代式算法无法表达（ 2 ）数据挖掘方面不擅长，由于MapReduce数据处理流程的限制，效率更高的算法却无法实现。2 ） Hive 的效率比较低（ 1 ）Hive自动生成的MapReduce作业，通常情况下不够智能化（ 2 ）Hive调优比较困难，粒度较粗1.3 Hive 架构原理HDFSMapReduceMeta storeSQLParser解析器PhysicalPlan编译器 Execution执行器QueryOptimizer优化器DriverCLI JDBCClientHive 架构1 ）用户接口： ClientCLI（command-line interface）、JDBC&#x2F;ODBC(jdbc访问hive)、WEBUI（浏览器访问hive）—————————————————————————————————————2 ）元数据： Metastore元数据包括：表名、表所属的数据库（默认是default）、表的拥有者、列&#x2F;分区字段、表的类型（是否是外部表）、表的数据所在目录等；默认存储在自带的derby数据库中，推荐使用MySQL存储Metastore3 ） Hadoop使用HDFS进行存储，使用MapReduce进行计算。4 ）驱动器： Driver（ 1 ）解析器（SQL Parser）：将SQL字符串转换成抽象语法树AST，这一步一般都用第三方工具库完成，比如antlr；对AST进行语法分析，比如表是否存在、字段是否存在、SQL语义是否有误。（ 2 ）编译器（Physical Plan）：将AST编译生成逻辑执行计划。（ 3 ）优化器（Query Optimizer）：对逻辑执行计划进行优化。（ 4 ）执行器（Execution）：把逻辑执行计划转换成可以运行的物理计划。对于Hive来说，就是MR&#x2F;Spark。Hive的运行机制\n用户创建table\ncreate\ntable ...\n用户针对数据表\n进行数据分析：\nselect ...from\ntable where ...\n用户只需要创建表，将\n表与数据建立映射关系，\n编写SQL分析语句\nHive中的\n元数据库\nMetastore着表对应文件的中记录 MetaStore\npath\n解析器查询\n输入文件的\npath\nHive中的解析器\n将SQL语言解析成对应的\nMapReduce程序，并生成\n相应的jar包\nMapReduce体系架构 result\nHadoop jar xxx.jar&#x2F;path\n数据仓库\n数据文件\n通过映射\n关系向表\n中导数据\nhdfs\nHive通过给用户提供的一系列交互接口，接收到用户的指令(SQL)，使用自己的Driver，结合元数据(MetaStore)，将这些指令翻译成MapReduce，提交到Hadoop中执行，最后，将执行返回的结果输出到用户交互接口。1.4 Hive 和数据库比较由于 Hive 采用了类似SQL 的查询语言 HQL(Hive Query Language)，因此很容易将 Hive 理—————————————————————————————————————解为数据库。其实从结构上来看，Hive 和数据库除了拥有类似的查询语言，再无类似之处。本文将从多个方面来阐述 Hive 和数据库的差异。数据库可以用在 Online 的应用中，但是Hive 是为数据仓库而设计的，清楚这一点，有助于从应用角度理解 Hive 的特性。1 .4.1 查询语言由于SQL被广泛的应用在数据仓库中，因此，专门针对Hive的特性设计了类SQL的查询语言HQL。熟悉SQL开发的开发者可以很方便的使用Hive进行开发。1 .4.2 数据更新由于Hive是针对数据仓库应用设计的，而数据仓库的内容是读多写少的。因此，Hive中不建议对数据的改写，所有的数据都是在加载的时候确定好的。而数据库中的数据通常是需要经常进行修改的，因此可以使用 INSERT INTO … VALUES 添加数据，使用 UPDATE … SET修改数据。1 .4.3 执行延迟Hive 在查询数据的时候，由于没有索引，需要扫描整个表，因此延迟较高。另外一个导致 Hive 执行延迟高的因素是 MapReduce框架。由于MapReduce 本身具有较高的延迟，因此在利用MapReduce 执行Hive查询时，也会有较高的延迟。相对的，数据库的执行延迟较低。当然，这个低是有条件的，即数据规模较小，当数据规模大到超过数据库的处理能力的时候，Hive的并行计算显然能体现出优势。1 .4.4 数据规模由于Hive建立在集群上并可以利用MapReduce进行并行计算，因此可以支持很大规模的数据；对应的，数据库可以支持的数据规模较小。第 2 章 Hive安装2.1 Hive 安装地址1 ） Hive 官网地址http://hive.apache.org/2 ）文档查看地址https://cwiki.apache.org/confluence/display/Hive/GettingStarted3 ）下载地址—————————————————————————————————————http://archive.apache.org/dist/hive/4 ） github 地址https://github.com/apache/hive2.2 Hive 安装部署2 .2.1 安装 Hive1 ）把 apache-hive-3.1.2-bin.tar.gz 上传到 linux 的 &#x2F;opt&#x2F;software 目录下2 ）解压 apache-hive-3.1.2-bin.tar.gz 到 &#x2F;opt&#x2F;module&#x2F; 目录下面[atguigu@hadoop102 software]$ tar -zxvf &#x2F;opt&#x2F;software&#x2F;apache-hive-3.1.2-\nbin.tar.gz -C &#x2F;opt&#x2F;module&#x2F;\n3 ）修改 apache-hive-3.1.2-bin.tar.gz 的名称为 hive[atguigu@hadoop102 software]$ mv &#x2F;opt&#x2F;module&#x2F;apache-hive-3.1.2-bin&#x2F;\n&#x2F;opt&#x2F;module&#x2F;hive\n4 ）修改 &#x2F;etc&#x2F;profile.d&#x2F;my_env.sh ，添加环境变量[atguigu@hadoop102 software]$ sudo vim &#x2F;etc&#x2F;profile.d&#x2F;my_env.sh\n5 ）添加内容#HIVE_HOME\nexport HIVE_HOME&#x3D;&#x2F;opt&#x2F;module&#x2F;hive\nexport PATH&#x3D;$PATH:$HIVE_HOME&#x2F;bin\n6 ）解决日志 Jar 包冲突[atguigu@hadoop102 software]$ mv $HIVE_HOME&#x2F;lib&#x2F;log4j-slf4j-impl-\n2.10.0.jar $HIVE_HOME&#x2F;lib&#x2F;log4j-slf4j-impl-2.10.0.bak\n7 ）初始化元数据库[atguigu@hadoop102 hive]$ bin&#x2F;schematool -dbType derby -initSchema\n2 .2.2 启动并使用 Hive1 ）启动 Hive[atguigu@hadoop102 hive]$ bin&#x2F;hive\n2 ）使用 Hivehive&gt; show databases;\nhive&gt; show tables;\nhive&gt; create table test(id int);\nhive&gt; insert into test values(1);\nhive&gt; select * from test;\n3 ）在 CRT 窗口中开启另一个窗口开启 Hive ，在 &#x2F;tmp&#x2F;atguigu 目录下监控 hive.log 文件Caused by: ERROR XSDB6: Another instance of Derby may have already booted\nthe database &#x2F;opt&#x2F;module&#x2F;hive&#x2F;metastore_db.\nat\norg.apache.derby.iapi.error.StandardException.newException(Unknown\nSource)\nat\norg.apache.derby.iapi.error.StandardException.newException(Unknown\n\n—————————————————————————————————————Source)\nat\norg.apache.derby.impl.store.raw.data.BaseDataFileFactory.privGetJBMSLockO\nnDB(Unknown Source)\nat\norg.apache.derby.impl.store.raw.data.BaseDataFileFactory.run(Unknown\nSource)\n...\n原因在于Hive默认使用的元数据库为derby，开启Hive之后就会占用元数据库，且不与其他客户端共享数据，所以我们需要将Hive的元数据地址改为MySQL。2.3 MySQL 安装1 ）检查当前系统是否安装过MySQL[atguigu@hadoop102 ~]$ rpm -qa|grep mariadb\nmariadb-libs-5.5.56-2.el7.x86_\n&#x2F;&#x2F;如果存在通过如下命令卸载\n[atguigu @hadoop102 ~]$ sudo rpm -e --nodeps mariadb-libs\n2 ）将 MySQL 安装包拷贝到 &#x2F;opt&#x2F;software 目录下[atguigu @hadoop102 software]# ll\n总用量 528384\n\nrw-r–r–. 1 root root 609556480 3月 21 15:41 mysql-5.7.28-\n\n1.el7.x86_64.rpm-bundle.tar\n3 ）解压 MySQL 安装包[atguigu @hadoop102 software]# tar -xf mysql-5.7.28-1.el7.x86_64.rpm-\nbundle.tar\n4 ）在安装目录下执行rpm安装[atguigu @hadoop102 software]$\nsudo rpm -ivh mysql-community-common-5.7.28-1.el7.x86_64.rpm\nsudo rpm -ivh mysql-community-libs-5.7.28-1.el7.x86_64.rpm\nsudo rpm -ivh mysql-community-libs-compat-5.7.28-1.el7.x86_64.rpm\nsudo rpm -ivh mysql-community-client-5.7.28-1.el7.x86_64.rpm\nsudo rpm -ivh mysql-community-server-5.7.28-1.el7.x86_64.rpm\n注意:按照顺序依次执行如果Linux是最小化安装的，在安装mysql-community-server-5.7.28-1.el7.x86_64.rpm时可能会出现如下错误[atguigu@hadoop 1 02 software]$ sudo rpm -ivh mysql-community-server-\n5.7.28-1.el7.x86_64.rpm\n警告：mysql-community-server-5.7.28-1.el7.x86_64.rpm: 头V3 DSA&#x2F;SHA\nSignature, 密钥 ID 5072e1f5: NOKEY\n\n—————————————————————————————————————错误：依赖检测失败：\nlibaio.so.1()(64bit) 被 mysql-community-server-5.7.28-1.el7.x86_\n需要\nlibaio.so.1(LIBAIO_0.1)(64bit) 被 mysql-community-server-5.7.28-\n1.el7.x86_64 需要\nlibaio.so.1(LIBAIO_0.4)(64bit) 被 mysql-community-server-5.7.28-\n1.el7.x86_64 需要\n通过yum安装缺少的依赖,然后重新安装mysql-community-server-5.7.28-1.el7.x86_64即可[atguigu@hadoop 1 02 software] yum install -y libaio\n5 ）删除 &#x2F;etc&#x2F;my.cnf 文件中 datadir 指向的目录下的所有内容 , 如果有内容的情况下 :查看datadir的值：[mysqld]\ndatadir&#x3D;&#x2F;var&#x2F;lib&#x2F;mysql\n删除&#x2F;var&#x2F;lib&#x2F;mysql目录下的所有内容:[atguigu @hadoop102 mysql]# cd &#x2F;var&#x2F;lib&#x2F;mysql\n[atguigu @hadoop102 mysql]# sudo rm -rf .&#x2F;* &#x2F;&#x2F;注意执行命令的位置\n6 ）初始化数据库[atguigu @hadoop102 opt]$ sudo mysqld --initialize --user&#x3D;mysql\n7 ）查看临时生成的 root 用户的密码[atguigu @hadoop102 opt]$ sudo cat &#x2F;var&#x2F;log&#x2F;mysqld.log\n8 ）启动 MySQL 服务[atguigu @hadoop102 opt]$ sudo systemctl start mysqld\n9 ）登录 MySQL 数据库[atguigu @hadoop102 opt]$ mysql -uroot -p\nEnter password: 输入临时生成的密码\n登录成功.10 ）必须先修改 root 用户的密码 , 否则执行其他的操作会报错mysql&gt; set password &#x3D; password(&quot;新密码&quot;);\n11 ）修改 mysql 库下的 user 表中的 root 用户允许任意 ip 连接mysql&gt; update mysql.user set host&#x3D;&#39;%&#39; where user&#x3D;&#39;root&#39;;\nmysql&gt; flush privileges;\n\n—————————————————————————————————————2.4 Hive 元数据配置到 MySQL2. 4 .1 拷贝驱动将MySQL的JDBC驱动拷贝到Hive的lib目录下[atguigu@hadoop102 software]$ cp &#x2F;opt&#x2F;software&#x2F;mysql-connector-java-\n5.1. 37 .jar $HIVE_HOME&#x2F;lib\n2. 4 .2 配置 Metastore 到 MySQL1 ）在 $HIVE_HOME&#x2F;conf 目录下新建 hive-site.xml 文件[atguigu@hadoop102 software]$ vim $HIVE_HOME&#x2F;conf&#x2F;hive-site.xml\n添加如下内容&lt;?xml version&#x3D;&quot;1.0&quot;?&gt;\n&lt;?xml-stylesheet type&#x3D;&quot;text&#x2F;xsl&quot; href&#x3D;&quot;configuration.xsl&quot;?&gt;\n&lt;configuration&gt;\n&lt;!-- jdbc连接的URL --&gt;\n&lt;property&gt;\n&lt;name&gt;javax.jdo.option.ConnectionURL&lt;&#x2F;name&gt;\n&lt;value&gt;jdbc:mysql:&#x2F;&#x2F;hadoop102:3306&#x2F;metastore?useSSL&#x3D;false&lt;&#x2F;value&gt;\n&lt;&#x2F;property&gt;\n&lt;!-- jdbc连接的Driver--&gt;\n&lt;property&gt;\n&lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;&#x2F;name&gt;\n&lt;value&gt;com.mysql.jdbc.Driver&lt;&#x2F;value&gt;\n&lt;&#x2F;property&gt;\n&lt;!-- jdbc连接的username--&gt;\n&lt;property&gt;\n&lt;name&gt;javax.jdo.option.ConnectionUserName&lt;&#x2F;name&gt;\n&lt;value&gt;root&lt;&#x2F;value&gt;\n&lt;&#x2F;property&gt;\n&lt;!-- jdbc连接的password --&gt;\n&lt;property&gt;\n&lt;name&gt;javax.jdo.option.ConnectionPassword&lt;&#x2F;name&gt;\n&lt;value&gt; 000000 &lt;&#x2F;value&gt;\n&lt;&#x2F;property&gt;\n&lt;!-- Hive元数据存储版本的验证 --&gt;\n&lt;property&gt;\n&lt;name&gt;hive.metastore.schema.verification&lt;&#x2F;name&gt;\n&lt;value&gt;false&lt;&#x2F;value&gt;\n&lt;&#x2F;property&gt;\n&lt;!--元数据存储授权--&gt;\n&lt;property&gt;\n&lt;name&gt;hive.metastore.event.db.notification.api.auth&lt;&#x2F;name&gt;\n&lt;value&gt;false&lt;&#x2F;value&gt;\n&lt;&#x2F;property&gt;\n&lt;!-- Hive默认在HDFS的工作目录 --&gt;\n&lt;property&gt;\n\n—————————————————————————————————————&lt;name&gt;hive.metastore.warehouse.dir&lt;&#x2F;name&gt;\n&lt;value&gt;&#x2F;user&#x2F;hive&#x2F;warehouse&lt;&#x2F;value&gt;\n&lt;&#x2F;property&gt;\n&lt;&#x2F;configuration&gt;\n2 ）登陆 MySQL[atguigu@hadoop102 software]$ mysql -uroot -p\n3 ）新建 Hive 元数据库mysql&gt; create database metastore;\nmysql&gt; quit;\n4 ） 初始化 Hive 元数据库[atguigu@hadoop102 software]$ schematool -initSchema -dbType mysql -\nverbose\n2.4.3 再次启动 Hive1 ）启动 Hive[atguigu@hadoop102 hive]$ bin&#x2F;hive\n2 ）使用 Hivehive&gt; show databases;\nhive&gt; show tables;\nhive&gt; create table test (id int);\nhive&gt; insert into test values(1);\nhive&gt; select * from test;\n3 ）在 CRT 窗口中开启另一个窗口开启 Hivehive&gt; show databases;\nhive&gt; show tables;\nhive&gt; select * from aa;\n2. 5 使用元数据服务的方式访问 Hive1 ）在 hive-site.xml 文件中添加如下配置信息&lt;!-- 指定存储元数据要连接的地址 --&gt;\n&lt;property&gt;\n&lt;name&gt;hive.metastore.uris&lt;&#x2F;name&gt;\n&lt;value&gt;thrift:&#x2F;&#x2F;hadoop102:9083&lt;&#x2F;value&gt;\n&lt;&#x2F;property&gt;\n2 ）启动 metastore[atguigu@hadoop202 hive]$ hive --service metastore\n2020 - 04 - 24 16:58:08: Starting Hive Metastore Server\n注意: 启动后窗口不能再操作，需打开一个新的shell窗口做别的操作\n3 ）启动 hive[atguigu@hadoop202 hive]$ bin&#x2F;hive\n2. 6 使用 JDBC 方式访问 Hive1 ）在 hive-site.xml 文件中添加如下配置信息&lt;!-- 指定hiveserver2连接的host --&gt;\n&lt;property&gt;\n&lt;name&gt;hive.server2.thrift.bind.host&lt;&#x2F;name&gt;\n\n—————————————————————————————————————&lt;value&gt;hadoop102&lt;&#x2F;value&gt;\n&lt;&#x2F;property&gt;\n&lt;!-- 指定hiveserver2连接的端口号 --&gt;\n&lt;property&gt;\n&lt;name&gt;hive.server2.thrift.port&lt;&#x2F;name&gt;\n&lt;value&gt; 10000 &lt;&#x2F;value&gt;\n&lt;&#x2F;property&gt;\n2 ）启动 hiveserver 2[atguigu@hadoop102 hive]$ bin&#x2F;hive --service hiveserver\n3 ）启动 beeline 客户端（需要多等待一会）[atguigu@hadoop102 hive]$ bin&#x2F;beeline -u jdbc:hive2:&#x2F;&#x2F;hadoop102:10000 -n\natguigu\n4 ）看到如下界面Connecting to jdbc:hive2:&#x2F;&#x2F;hadoop102:\nConnected to: Apache Hive (version 3.1.2)\nDriver: Hive JDBC (version 3.1.2)\nTransaction isolation: TRANSACTION_REPEATABLE_READ\nBeeline version 3.1.2 by Apache Hive\n0: jdbc:hive2:&#x2F;&#x2F;hadoop102:10000&gt;\n5 ）编写 hive 服务启动脚本（了解）（ 1 ） 前台启动的方式导致需要打开多个shell窗口，可以使用如下方式后台方式启动nohup: 放在命令开头，表示不挂起,也就是关闭终端进程也继续保持运行状态&#x2F;dev&#x2F;null：是Linux文件系统中的一个文件，被称为黑洞，所有写入改文件的内容都会被自动丢弃2 &gt;&amp;1 : 表示将错误重定向到标准输出上&amp;: 放在命令结尾,表示后台运行一般会组合使用: nohup [xxx命令操作]&gt; file 2&gt;&amp;1 &amp;，表示将xxx命令运行的结果输出到file中，并保持命令启动的进程在后台运行。如上命令不要求掌握。[atguigu@hadoop202 hive]$ nohup hive --service metastore 2&gt;&amp;1 &amp;\n[atguigu@hadoop202 hive]$ nohup hive --service hiveserver2 2&gt;&amp;1 &amp;\n（ 2 ） 为了方便使用，可以直接编写脚本来管理服务的启动和关闭[atguigu@hadoop102 hive]$ vim $HIVE_HOME&#x2F;bin&#x2F;hiveservices.sh\n内容如下：此脚本的编写不要求掌握。直接拿来使用即可。#!&#x2F;bin&#x2F;bash\nHIVE_LOG_DIR&#x3D;$HIVE_HOME&#x2F;logs\nif [! -d $HIVE_LOG_DIR ]\nthen\nmkdir -p $HIVE_LOG_DIR\nfi\n#检查进程是否运行正常，参数 1 为进程名，参数 2 为进程端口\nfunction check_process()\n\n—————————————————————————————————————&#123;\npid&#x3D;$(ps -ef 2&gt;&#x2F;dev&#x2F;null | grep -v grep | grep -i $1 | awk &#39;&#123;print\n$2&#125;&#39;)\nppid&#x3D;$(netstat -nltp 2&gt;&#x2F;dev&#x2F;null | grep $2 | awk &#39;&#123;print $7&#125;&#39; | cut -\nd &#39;&#x2F;&#39; -f 1)\necho $pid\n[[ &quot;$pid&quot; &#x3D;~ &quot;$ppid&quot; ]] &amp;&amp; [ &quot;$ppid&quot; ] &amp;&amp; return 0 || return 1\n&#125;\nfunction hive_start()\n&#123;\nmetapid&#x3D;$(check_process HiveMetastore 9083)\ncmd&#x3D;&quot;nohup hive --service metastore &gt;$HIVE_LOG_DIR&#x2F;metastore.log 2&gt;&amp;\n&amp;&quot;\n[ -z &quot;$metapid&quot; ] &amp;&amp; eval $cmd || echo &quot;Metastroe服务已启动&quot;\nserver2pid&#x3D;$(check_process HiveServer2 10000)\ncmd&#x3D;&quot;nohup hiveserver2 &gt;$HIVE_LOG_DIR&#x2F;hiveServer2.log 2&gt;&amp;1 &amp;&quot;\n[ -z &quot;$server2pid&quot; ] &amp;&amp; eval $cmd || echo &quot;HiveServer2服务已启动&quot;\n&#125;\nfunction hive_stop()\n&#123;\nmetapid&#x3D;$(check_process HiveMetastore 9083)\n[ &quot;$metapid&quot; ] &amp;&amp; kill $metapid || echo &quot;Metastore服务未启动&quot;\nserver2pid&#x3D;$(check_process HiveServer2 10000)\n[ &quot;$server2pid&quot; ] &amp;&amp; kill $server2pid || echo &quot;HiveServer2服务未启动&quot;\n&#125;\ncase $1 in\n&quot;start&quot;)\nhive_start\n;;\n&quot;stop&quot;)\nhive_stop\n;;\n&quot;restart&quot;)\nhive_stop\nsleep 2\nhive_start\n;;\n&quot;status&quot;)\ncheck_process HiveMetastore 9083 &gt;&#x2F;dev&#x2F;null &amp;&amp; echo &quot;Metastore服务运行\n正常&quot; || echo &quot;Metastore服务运行异常&quot;\ncheck_process HiveServer2 10000 &gt;&#x2F;dev&#x2F;null &amp;&amp; echo &quot;HiveServer2服务运\n行正常&quot; || echo &quot;HiveServer2服务运行异常&quot;\n;;\n*)\necho Invalid Args!\necho &#39;Usage: &#39;$(basename $0)&#39; start|stop|restart|status&#39;\n;;\nesac\n3 ）添加执行权限[atguigu@hadoop102 hive]$ chmod +x $HIVE_HOME&#x2F;bin&#x2F;hiveservices.sh\n4 ）启动 Hive 后台服务[atguigu@hadoop102 hive]$ hiveservices.sh start\n\n—————————————————————————————————————2. 7 Hive 常用交互命令[atguigu@hadoop102 hive]$ bin&#x2F;hive -help\nusage: hive\n\nd,–define &lt;key&#x3D;value&gt; Variable subsitution to apply to hivecommands. e.g. -d A&#x3D;B or –define A&#x3D;B\n\n–database  Specify the database to use\n\ne  SQL from command line\nf  SQL from files\nH,–help Print help information\n\n–hiveconf &lt;property&#x3D;value&gt; Use value for given property–hivevar &lt;key&#x3D;value&gt; Variable subsitution to apply to hivecommands. e.g. –hivevar A&#x3D;B\n\ni  Initialization SQL file\nS,–silent Silent mode in interactive shell\nv,–verbose Verbose mode (echo executed SQL to theconsole)\n\n1 ）“ - e ”不进入 hive 的交互窗口执行 sql 语句[atguigu@hadoop102 hive]$ bin&#x2F;hive -e &quot;select id from student;&quot;\n2 ）“ - f ”执行脚本中 sql 语句（ 1 ）在&#x2F;opt&#x2F;module&#x2F;hive&#x2F;下创建datas目录并在datas目录下创建hivef.sql文件[atguigu@hadoop102 datas]$ touch hivef.sql\n（ 2 ）文件中写入正确的sql语句select *from student;\n（ 3 ）执行文件中的sql语句[atguigu@hadoop102 hive]$ bin&#x2F;hive -f &#x2F;opt&#x2F;module&#x2F;hive&#x2F;datas&#x2F;hivef.sql\n（ 4 ）执行文件中的sql语句并将结果写入文件中[atguigu@hadoop102 hive]$ bin&#x2F;hive -f &#x2F;opt&#x2F;module&#x2F;hive&#x2F;datas&#x2F;hivef.sql &gt;\n&#x2F;opt&#x2F;module&#x2F;datas&#x2F;hive_result.txt\n2. 8 Hive 其他命令操作1 ）退出 hive 窗口：hive(default)&gt;exit;\nhive(default)&gt;quit;\n2 ）在 hive cli 命令窗口中如何查看 hdfs 文件系统hive(default)&gt;dfs -ls &#x2F;;\n3 ）查看在 hive 中输入的所有历史命令（ 1 ）进入到当前用户的根目录 &#x2F;root或&#x2F;home&#x2F;atguigu（ 2 ）查看. hivehistory文件[atguig 2 u@hadoop102 ~]$ cat .hivehistory\n\n—————————————————————————————————————2. 9 Hive 常见属性配置2. 9 .1 Hive 运行日志信息配置1 ） Hive 的 log 默认存放在 &#x2F;tmp&#x2F;atguigu&#x2F;hive.log 目录下（当前用户名下）2 ）修改 hive 的 log 存放日志到 &#x2F;opt&#x2F;module&#x2F;hive&#x2F;logs（ 1 ）修改&#x2F;opt&#x2F;module&#x2F;hive&#x2F;conf&#x2F;hive-log4j 2 .properties.template文件名称为hive-log4j 2 .properties[atguigu@hadoop102 conf]$ pwd\n&#x2F;opt&#x2F;module&#x2F;hive&#x2F;conf\n[atguigu@hadoop102 conf]$ mv hive-log4j 2 .properties.template hive-\nlog4j 2 .properties\n（ 2 ）在hive-log4j 2 .properties文件中修改log存放位置hive.log.dir&#x3D;&#x2F;opt&#x2F;module&#x2F;hive&#x2F;logs\n2. 9 .2 打印 当前库 和 表头在hive-site.xml中加入如下两个配置:&lt;property&gt;\n&lt;name&gt;hive.cli.print.header&lt;&#x2F;name&gt;\n&lt;value&gt;true&lt;&#x2F;value&gt;\n&lt;&#x2F;property&gt;\n&lt;property&gt;\n&lt;name&gt;hive.cli.print.current.db&lt;&#x2F;name&gt;\n&lt;value&gt;true&lt;&#x2F;value&gt;\n&lt;&#x2F;property&gt;\n2. 9. 3 参数配置方式1 ）查看当前所有的配置信息hive&gt;set;\n2 ）参数的配置三种方式（ 1 ）配置文件方式默认配置文件：hive-default.xml用户自定义配置文件：hive-site.xml注意：用户自定义配置会覆盖默认配置。另外，Hive也会读入Hadoop的配置，因为Hive是作为Hadoop的客户端启动的，Hive的配置会覆盖Hadoop的配置。配置文件的设定对本机启动的所有Hive进程都有效。（ 2 ）命令行参数方式启动Hive时，可以在命令行添加-hiveconf param&#x3D;value来设定参数。—————————————————————————————————————例如：[atguigu@hadoop103 hive]$ bin&#x2F;hive -hiveconf mapred.reduce.tasks&#x3D;10;\n注意：仅对本次hive启动有效查看参数设置：hive (default)&gt; set mapred.reduce.tasks;\n（ 3 ）参数声明方式可以在HQL中使用SET关键字设定参数例如：hive (default)&gt; set mapred.reduce.tasks&#x3D;100;\n注意：仅对本次hive启动有效。查看参数设置hive (default)&gt; set mapred.reduce.tasks;\n上述三种设定方式的优先级依次递增。即配置文件&lt;命令行参数&lt;参数声明。注意某些系统级的参数，例如log4j相关的设定，必须用前两种方式设定，因为那些参数的读取在会话建立以前已经完成了。第 3 章 Hive数据类型3 .1 基本数据类型Hive数据类型 Java数据类型 长度 例子\nTINYINT byte 1byte有符号整数 20\nSMALINT short 2byte有符号整数 20\nINT int 4byte有符号整数 20\nBIGINT long (^) 8byte有符号整数 20BOOLEAN boolean (^) 布尔类型，true或者falseTRUE FALSEFLOAT float 单精度浮点数 3.DOUBLE double 双精度浮点数 3.STRING string (^) 字符系列。可以指定字符集。可以使用单引号或者双引号。‘now is the time’“for all good men”TIMESTAMP 时间类型BINARY 字节数组\n对于Hive的String类型相当于数据库的varchar类型，该类型是一个可变的字符串，不过它不能声明其中最多能存储多少个字符，理论上它可以存储2GB的字符数。—————————————————————————————————————3 .2 集合数据类型数据类型 描述 语法示例\nSTRUCT 和c语言中的struct类似，都可以通过“点”符号访\n问元素内容。例如，如果某个列的数据类型是STRUCT&#123;first\nSTRING, last STRING&#125;,那么第 1 个元素可以通过字段.first来\n引用。\nstruct()\n例如struct&lt;street:string,\ncity:string&gt;\nMAP MAP是一组键-值对元组集合，使用数组表示法可以\n访问数据。例如，如果某个列的数据类型是MAP，其中键\n\n\n\n\n\n\n\n\n\n\n值对是’first’-&gt;’John’和’last’-&gt;’Doe’，那么可以通过字段名[‘last’]获取最后一个元素\n\n\nmap()\n例如map&lt;string, int&gt;\nARRAY 数组是一组具有相同类型和名称的变量的集合。这些\n变量称为数组的元素，每个数组元素都有一个编号，编号从\n零开始。例如，数组值为[‘John’, ‘Doe’]，那么第 2 个\n元素可以通过数组名[1]进行引用。\nArray()\n例如array&lt;string&gt;\nHive有三种复杂数据类型ARRAY、MAP 和 STRUCT。ARRAY和MAP与Java中的Array和Map类似，而STRUCT与C语言中的Struct类似，它封装了一个命名字段集合，复杂数据类型允许任意层次的嵌套。1 ）案例实操（ 1 ）假设某表有如下一行，我们用JSON格式来表示其数据结构。在Hive下访问的格式为&#123;\n&quot;name&quot;: &quot;songsong&quot;,\n&quot;friends&quot;: [&quot;bingbing&quot; , &quot;lili&quot;] , &#x2F;&#x2F;列表Array,\n&quot;children&quot;: &#123; &#x2F;&#x2F;键值Map,\n&quot;xiao song&quot;: 18 ,\n&quot;xiaoxiao song&quot;: 19\n&#125;\n&quot;address&quot;: &#123; &#x2F;&#x2F;结构Struct,\n&quot;street&quot;: &quot;hui long guan&quot;,\n&quot;city&quot;: &quot;beijing&quot;\n&#125;\n&#125;\n（ 2 ）基于上述数据结构，我们在Hive里创建对应的表，并导入数据。创建本地测试文件test.txtsongsong,bingbing_lili,xiao song:18_xiaoxiao song:19,hui long\nguan_beijing\nyangyang,caicai_susu,xiao yang:18_xiaoxiao yang:19,chao yang_beijing\n注意：MAP，STRUCT和ARRAY里的元素间关系都可以用同一个字符表示，这里用“_”。（ 3 ）Hive上创建测试表testcreate table test(\nname string,\nfriends array&lt;string&gt;,\n\n—————————————————————————————————————children map&lt;string, int&gt;,\naddress struct&lt;street:string, city:string&gt;\n)\nrow format delimited fields terminated by &#39;,&#39;\ncollection items terminated by &#39;_&#39;\nmap keys terminated by &#39;:&#39;\nlines terminated by &#39;\\n&#39;;\n字段解释：row format delimited fields terminated by ‘,’ – 列分隔符collection items terminated by ‘_’ –MAP STRUCT 和 ARRAY 的分隔符(数据分割符号)map keys terminated by ‘:’ – MAP中的key与value的分隔符lines terminated by ‘\\n’; – 行分隔符（ 4 ）导入文本数据到测试表load data local inpath ‘&#x2F;opt&#x2F;module&#x2F;hive&#x2F;datas&#x2F;test.txt’ into table test;（ 5 ）访问三种集合列里的数据，以下分别是ARRAY，MAP，STRUCT的访问方式hive (default)&gt; select friends[1],children[&#39;xiao song&#39;],address.city from\ntest\nwhere name&#x3D;&quot;songsong&quot;;\nOK\n_c0 _c1 city\nlili 18 beijing\nTime taken: 0.076 seconds, Fetched: 1 row(s)\n3 .3 类型转化Hive的原子数据类型是可以进行隐式转换的，类似于Java的类型转换，例如某表达式使用INT类型，TINYINT会自动转换为INT类型，但是Hive不会进行反向转化，例如，某表达式使用TINYINT类型，INT不会自动转换为TINYINT类型，它会返回错误，除非使用CAST操作。1 ）隐式类型转换规则如下（ 1 ）任何整数类型都可以隐式地转换为一个范围更广的类型，如TINYINT可以转换成INT，INT可以转换成BIGINT。（ 2 ）所有整数类型、FLOAT和STRING类型都可以隐式地转换成DOUBLE。（ 3 ）TINYINT、SMALLINT、INT都可以转换为FLOAT。（ 4 ）BOOLEAN类型不可以转换为任何其它的类型。2 ）可以使用 CAST 操作显示进行数据类型转换例如CAST(‘1’ AS INT)将把字符串’1’ 转换成整数 1 ；如果强制类型转换失败，如执行CAST(‘X’ AS INT)，表达式返回空值 NULL。—————————————————————————————————————0: jdbc:hive2:&#x2F;&#x2F;hadoop102:10000&gt; select &#39;1&#39;+2, cast(&#39;1&#39;as int) + 2;\n+------+------+--+\n| _c0 | _c1 |\n+------+------+--+\n| 3.0 | 3 |\n+------+------+--+\n第 4 章 DDL数据定义4 .1 创建数据库CREATE DATABASE [IF NOT EXISTS] database_name\n[COMMENT database_comment]\n[LOCATION hdfs_path]\n[WITH DBPROPERTIES (property_name&#x3D;property_value, ...)];\n1 ）创建一个数据库，数据库在 HDFS 上的默认存储路径是 &#x2F;user&#x2F;hive&#x2F;warehouse&#x2F;*.db 。hive (default)&gt; create database db_hive;\n2 ）避免要创建的数据库已经存在错误，增加 if not exists 判断。（标准写法）hive (default)&gt; create database db_hive;\nFAILED: Execution Error, return code 1 from\norg.apache.hadoop.hive.ql.exec.DDLTask. Database db_hive already exists\nhive (default)&gt; create database if not exists db_hive;\n3 ）创建一个数据库，指定数据库在 HDFS 上存放的位置hive (default)&gt; create database db_hive2 location &#39;&#x2F;db_hive2.db&#39;;\n4 .2 查询数据库4 .2.1 显示数据库1 ）显示数据库hive&gt; show databases;\n2 ）过滤显示查询的数据库hive&gt; show databases like &#39;db_hive*&#39;;\nOK\ndb_hive\ndb_hive_\n4 .2.2 查看数据库详情1 ）显示数据库信息hive&gt; desc database db_hive;\nOK\ndb_hive hdfs:&#x2F;&#x2F;hadoop102: 9820 &#x2F;user&#x2F;hive&#x2F;warehouse&#x2F;db_hive.db\natguiguUSER\n2 ）显示数据库详细信息， extendedhive&gt; desc database extended db_hive;\nOK\ndb_hive hdfs:&#x2F;&#x2F;hadoop102: 9820 &#x2F;user&#x2F;hive&#x2F;warehouse&#x2F;db_hive.db\natguiguUSER\n\n—————————————————————————————————————4. 2 .3 切换当前数据库hive (default)&gt; use db_hive;\n4 .3 修改数据库用户可以使用ALTER DATABASE命令为某个数据库的DBPROPERTIES设置键-值对属性值，来描述这个数据库的属性信息。hive (default)&gt; alter database db_hive\nset dbproperties(&#39;createtime&#39;&#x3D;&#39;20170830&#39;);\n在hive中查看修改结果hive&gt; desc database extended db_hive;\ndb_name comment location owner_name owner_type parameters\ndb_hive hdfs:&#x2F;&#x2F;hadoop102: 9820 &#x2F;user&#x2F;hive&#x2F;warehouse&#x2F;db_hive.db\natguigu USER &#123;createtime&#x3D;20170830&#125;\n4 .4 删除数据库1 ）删除空数据库hive&gt;drop database db_hive2;\n2 ）如果删除的数据库不存在，最好采用 if exists 判断数据库是否存在hive&gt; drop database db_hive;\nFAILED: SemanticException [Error 10072]: Database does not exist: db_hive\nhive&gt; drop database if exists db_hive2;\n3 ）如果数据库不为空，可以采用 cascade 命令，强制删除hive&gt; drop database db_hive;\nFAILED: Execution Error, return code 1 from\norg.apache.hadoop.hive.ql.exec.DDLTask.\nInvalidOperationException(message:Database db_hive is not empty. One or\nmore tables exist.)\nhive&gt; drop database db_hive cascade;\n4 .5 创建表1 ）建表语法CREATE [EXTERNAL] TABLE [IF NOT EXISTS] table_name\n[(col_name data_type [COMMENT col_comment], ...)]\n[COMMENT table_comment]\n[PARTITIONED BY (col_name data_type [COMMENT col_comment], ...)]\n[CLUSTERED BY (col_name, col_name, ...)\n[SORTED BY (col_name [ASC|DESC], ...)] INTO num_buckets BUCKETS]\n[ROW FORMAT row_format]\n[STORED AS file_format]\n[LOCATION hdfs_path]\n[TBLPROPERTIES (property_name&#x3D;property_value, ...)]\n[AS select_statement]\n2 ）字段解释说明（ 1 ）CREATE TABLE 创建一个指定名字的表。如果相同名字的表已经存在，则抛出异常；用户可以用 IF NOT EXISTS 选项来忽略这个异常。—————————————————————————————————————（ 2 ）EXTERNAL关键字可以让用户创建一个外部表，在建表的同时可以指定一个指向实际数据的路径（LOCATION），在删除表的时候，内部表的元数据和数据会被一起删除，而外部表只删除元数据，不删除数据。（ 3 ）COMMENT：为表和列添加注释。（ 4 ）PARTITIONED BY创建分区表（ 5 ）CLUSTERED BY创建分桶表（ 6 ）SORTED BY不常用，对桶中的一个或多个列另外排序（ 7 ）ROW FORMATDELIMITED [FIELDS TERMINATED BY char] [COLLECTION ITEMS TERMINATED BY char][MAP KEYS TERMINATED BY char] [LINES TERMINATED BY char]| SERDE serde_name [WITH SERDEPROPERTIES (property_name&#x3D;property_value,property_name&#x3D;property_value, …)]用户在建表的时候可以自定义 SerDe 或者使用自带的 SerDe。如果没有指定 ROWFORMAT 或者ROW FORMAT DELIMITED，将会使用自带的SerDe。在建表的时候，用户还需要为表指定列，用户在指定表的列的同时也会指定自定义的SerDe，Hive通过SerDe确定表的具体的列的数据。SerDe是Serialize&#x2F;Deserilize的简称， hive使用Serde进行行对象的序列与反序列化。（ 8 ）STORED AS指定存储文件类型常用的存储文件类型：SEQUENCEFILE（二进制序列文件）、TEXTFILE（文本）、RCFILE（列式存储格式文件）如果文件数据是纯文本，可以使用STORED AS TEXTFILE。如果数据需要压缩，使用 STOREDAS SEQUENCEFILE。（ 9 ）LOCATION ：指定表在HDFS上的存储位置。（ 10 ）AS：后跟查询语句，根据查询结果创建表。（ 11 ）LIKE允许用户复制现有的表结构，但是不复制数据。4 .5.1 管理表1 ）理论默认创建的表都是所谓的管理表，有时也被称为内部表。因为这种表，Hive会（或多或少地）控制着数据的生命周期。Hive 默认情况下会将这些表的数据存储在由配置项—————————————————————————————————————hive.metastore.warehouse.dir(例如，&#x2F;user&#x2F;hive&#x2F;warehouse)所定义的目录的子目录下。当我们删除一个管理表时，Hive也会删除这个表中数据。管理表不适合和其他工具共享数据。2 ）案例实操（ 0 ）原始数据1001 ss\n1002 ss\n1003 ss\n1004 ss\n1005 ss\n1006 ss\n1007 ss\n1008 ss\n1009 ss\n1010 ss\n1011 ss\n1012 ss\n1013 ss\n1014 ss\n1015 ss\n1016 ss\n（ 1 ）普通创建表create table if not exists student(\nid int, name string\n)\nrow format delimited fields terminated by &#39;\\t&#39;\nstored as textfile\nlocation &#39;&#x2F;user&#x2F;hive&#x2F;warehouse&#x2F;student&#39;;\n（ 2 ）根据查询结果创建表（查询的结果会添加到新创建的表中）create table if not exists student 2 as select id, name from student;\n（ 3 ）根据已经存在的表结构创建表create table if not exists student 3 like student;\n（ 4 ）查询表的类型hive (default)&gt; desc formatted student2;\nTable Type: MANAGED_TABLE\n4 .5.2 外部表1 ）理论因为表是外部表，所以Hive并非认为其完全拥有这份数据。删除该表并不会删除掉这份数据，不过描述表的元数据信息会被删除掉。2 ）管理表和外部表的使用场景每天将收集到的网站日志定期流入HDFS文本文件。在外部表（原始日志表）的基础上做大量的统计分析，用到的中间表、结果表使用内部表存储，数据通过SELECT+INSERT进入—————————————————————————————————————内部表。3 ）案例实操分别创建部门和员工外部表，并向表中导入数据。（ 0 ）原始数据dept:10 ACCOUNTING 1700\n20 RESEARCH 1800\n30 SALES 1900\n40 OPERATIONS 1700\nemp：7369 SMITH CLERK 7902 1980 - 12 - 17 800.00 20\n7499 ALLEN SALESMAN 7698 1981 - 2 - 20 1600.00 300.00 30\n7521 WARD SALESMAN 7698 1981 - 2 - 22 1250.00 500.00 30\n7566 JONES MANAGER 7839 1981 - 4 - 2 2975.00 20\n7654 MARTIN SALESMAN 7698 1981 - 9 - 28 1250.00 1400.00 30\n7698 BLAKE MANAGER 7839 1981 - 5 - 1 2850.00 30\n7782 CLARK MANAGER 7839 1981 - 6 - 9 2450.00 10\n7788 SCOTT ANALYST 7566 1987 - 4 - 19 3000.00 20\n7839 KING PRESIDENT 1981 - 11 - 17 5000.00 10\n7844 TURNER SALESMAN 7698 1981 - 9 - 8 1500.00 0.00 30\n7876 ADAMS CLERK 7788 1987 - 5 - 23 1100.00 20\n7900 JAMES CLERK 7698 1981 - 12 - 3 950.00 30\n7902 FORD ANALYST 7566 1981 - 12 - 3 3000.00 20\n7934 MILLER CLERK 7782 1982 - 1 - 23 1300.00 10\n（ 1 ）上传数据到HDFShive (default)&gt; dfs -mkdir &#x2F;student;\nhive (default)&gt; dfs -put &#x2F;opt&#x2F;module&#x2F;datas&#x2F;student.txt &#x2F;student;\n（ 2 ）建表语句，创建外部表创建部门表create external table if not exists dept(\ndeptno int,\ndname string,\nloc int\n)\nrow format delimited fields terminated by &#39;\\t&#39;;\n创建员工表create external table if not exists emp(\nempno int,\nename string,\njob string,\nmgr int,\nhiredate string,\nsal double,\ncomm double,\ndeptno int)\nrow format delimited fields terminated by &#39;\\t&#39;;\n（ 3 ）查看创建的表hive (default)&gt;show tables;\n\n—————————————————————————————————————（ 4 ）查看表格式化数据hive (default)&gt; desc formatted dept;\nTable Type: EXTERNAL_TABLE\n（ 5 ）删除外部表hive (default)&gt; drop table dept;\n外部表删除后，hdfs中的数据还在，但是metadata中dept的元数据已被删除4 .5.3 管理表与外部表的互相转换（ 1 ）查询表的类型hive (default)&gt; desc formatted student2;\nTable Type: MANAGED_TABLE\n（ 2 ）修改内部表student 2 为外部表alter table student2 set tblproperties(&#39;EXTERNAL&#39;&#x3D;&#39;TRUE&#39;);\n（ 3 ）查询表的类型hive (default)&gt; desc formatted student2;\nTable Type: EXTERNAL_TABLE\n（ 4 ）修改外部表student 2 为内部表alter table student2 set tblproperties(&#39;EXTERNAL&#39;&#x3D;&#39;FALSE&#39;);\n（ 5 ）查询表的类型hive (default)&gt; desc formatted student2;\nTable Type: MANAGED_TABLE\n注意：(‘EXTERNAL’&#x3D;’TRUE’)和(‘EXTERNAL’&#x3D;’FALSE’)为固定写法，区分大小写！4 .6 修改表4 .6.1 重命名表1 ）语法ALTER TABLE table_name RENAME TO new_table_name\n2 ）实操案例hive (default)&gt; alter table dept_partition2 rename to dept_partition3;\n4 .6.2 增加、修改和删除表分区详见7.1章分区表基本操作。4 .6.3 增加 &#x2F; 修改 &#x2F; 替换列信息1 ）语法（ 1 ）更新列ALTER TABLE table_name CHANGE [COLUMN] col_old_name col_new_name\ncolumn_type [COMMENT col_comment] [FIRST|AFTER column_name]\n（ 2 ）增加和替换列—————————————————————————————————————ALTER TABLE table_name ADD|REPLACE COLUMNS (col_name data_type [COMMENT\ncol_comment], ...)\n注：ADD是代表新增一字段，字段位置在所有列后面(partition列前)，REPLACE则是表示替换表中所有字段。2 ）实操案例（ 1 ）查询表结构hive&gt; desc dept;\n（ 2 ）添加列hive (default)&gt; alter table dept add columns(deptdesc string);\n（ 3 ）查询表结构hive&gt; desc dept;\n（ 4 ）更新列hive (default)&gt; alter table dept change column deptdesc desc string;\n（ 5 ）查询表结构hive&gt; desc dept;\n（ 6 ）替换列hive (default)&gt; alter table dept replace columns(deptno string, dname\nstring, loc string);\n（ 7 ）查询表结构hive&gt; desc dept;\n4 .7 删除表hive (default)&gt; drop table dept;\n第 5 章 DML数据操作5 .1 数据导入5 .1.1 向表中装载数据（ Load ）1 ）语法hive&gt; load data [local] inpath &#39;数据的path&#39; [overwrite] into table\nstudent [partition (partcol1&#x3D;val1,...)];\n（ 1 ）load data:表示加载数据（ 2 ）local:表示从本地加载数据到hive表；否则从HDFS加载数据到hive表（ 3 ）inpath:表示加载数据的路径（ 4 ）overwrite:表示覆盖表中已有数据，否则表示追加（ 5 ）into table:表示加载到哪张表（ 6 ）student:表示具体的表—————————————————————————————————————（ 7 ）partition:表示上传到指定分区2 ）实操案例（ 0 ）创建一张表hive (default)&gt; create table student(id string, name string) row format\ndelimited fields terminated by &#39;\\t&#39;;\n（ 1 ）加载本地文件到hivehive (default)&gt; load data local inpath\n&#39;&#x2F;opt&#x2F;module&#x2F;hive&#x2F;datas&#x2F;student.txt&#39; into table default.student;\n（ 2 ）加载HDFS文件到hive中上传文件到HDFShive (default)&gt; dfs -put &#x2F;opt&#x2F;module&#x2F;hive&#x2F;data&#x2F;student.txt\n&#x2F;user&#x2F;atguigu&#x2F;hive;\n加载HDFS上数据hive (default)&gt; load data inpath &#39;&#x2F;user&#x2F;atguigu&#x2F;hive&#x2F;student.txt&#39; into\ntable default.student;\n（ 3 ）加载数据覆盖表中已有的数据上传文件到HDFShive (default)&gt; dfs -put &#x2F;opt&#x2F;module&#x2F;data&#x2F;student.txt &#x2F;user&#x2F;atguigu&#x2F;hive;\n加载数据覆盖表中已有的数据hive (default)&gt; load data inpath &#39;&#x2F;user&#x2F;atguigu&#x2F;hive&#x2F;student.txt&#39;\noverwrite into table default.student;\n5 .1.2 通过查询语句向表中插入数据（ Insert ）1 ）创建一张表hive (default)&gt; create table student_par(id int, name string) row format\ndelimited fields terminated by &#39;\\t&#39;;\n2 ）基本插入数据hive (default)&gt; insert into table student_par\nvalues(1,&#39;wangwu&#39;),(2,&#39;zhaoliu&#39;);\n3 ）基本模式插入（根据单张表查询结果）hive (default)&gt; insert overwrite table student_par\nselect id, name from student where month&#x3D;&#39;201709&#39;;\ninsert into：以追加数据的方式插入到表或分区，原有数据不会删除insert overwrite：会覆盖表中已存在的数据注意：insert不支持插入部分字段4 ）多表（多分区）插入模式（根据多张表查询结果）hive (default)&gt; from student\ninsert overwrite table student partition(month&#x3D;&#39;201707&#39;)\nselect id, name where month&#x3D;&#39;201709&#39;\ninsert overwrite table student partition(month&#x3D;&#39;201706&#39;)\nselect id, name where month&#x3D;&#39;201709&#39;;\n\n—————————————————————————————————————5 .1.3 查询语句中创建表并加载数据（ As Select ）详见4.5.1章创建表。根据查询结果创建表（查询的结果会添加到新创建的表中）create table if not exists student3\nas select id, name from student;\n5 .1.4 创建表时通过 Location 指定加载数据路径1 ）上传数据到 hdfs 上hive (default)&gt; dfs -mkdir &#x2F;student;\nhive (default)&gt; dfs -put &#x2F;opt&#x2F;module&#x2F;datas&#x2F;student.txt &#x2F;student;\n2 ）创建表，并指定在 hdfs 上的位置hive (default)&gt; create external table if not exists student5(\nid int, name string\n)\nrow format delimited fields terminated by &#39;\\t&#39;\nlocation &#39;&#x2F;student;\n3 ）查询数据hive (default)&gt; select * from student5;\n5.1.5 Import 数据到指定 Hive 表中注意：先用export导出后，再将数据导入。hive (default)&gt; import table student 2\nfrom &#39;&#x2F;user&#x2F;hive&#x2F;warehouse&#x2F;export&#x2F;student&#39;;\n5 .2 数据导出5.2.1 Insert 导出1 ）将查询的结果导出到本地hive (default)&gt; insert overwrite local directory\n&#39;&#x2F;opt&#x2F;module&#x2F;hive&#x2F;data&#x2F;export&#x2F;student&#39;\nselect * from student;\n2 ）将查询的结果格式化导出到本地hive(default)&gt;insert overwrite local directory\n&#39;&#x2F;opt&#x2F;module&#x2F;hive&#x2F;data&#x2F;export&#x2F;student1&#39;\nROW FORMAT DELIMITED FIELDS TERMINATED BY &#39;\\t&#39;\nselect * from student;\n3 ）将查询的结果导出到 HDFS 上 ( 没有 local)hive (default)&gt; insert overwrite directory &#39;&#x2F;user&#x2F;atguigu&#x2F;student2&#39;\nROW FORMAT DELIMITED FIELDS TERMINATED BY &#39;\\t&#39;\nselect * from student;\n5.2.2 Hadoop 命令导出到本地hive (default)&gt; dfs -get &#x2F;user&#x2F;hive&#x2F;warehouse&#x2F;student&#x2F;student.txt\n&#x2F;opt&#x2F;module&#x2F;data&#x2F;export&#x2F;student3.txt;\n\n—————————————————————————————————————5.2.3 Hive Shell 命令导出基本语法：（hive -f&#x2F;-e 执行语句或者脚本 &gt; file）[atguigu@hadoop102 hive]$ bin&#x2F;hive -e &#39;select * from default.student;&#39; &gt;\n&#x2F;opt&#x2F;module&#x2F;hive&#x2F;data&#x2F;export&#x2F;student4.txt;\n5.2.4 Export 导出到 HDFS 上(defahiveult)&gt; export table default.student\nto &#39;&#x2F;user&#x2F;hive&#x2F;warehouse&#x2F;export&#x2F;student&#39;;\nexport和import主要用于两个Hadoop平台集群之间Hive表迁移。5.2.5 Sqoop 导出后续课程专门讲。5.2.6 清除表中数据（ Truncate ）注意：Truncate只能删除管理表，不能删除外部表中数据hive (default)&gt; truncate table student;\n第 6 章 查询https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Select查询语句语法：SELECT [ALL | DISTINCT] select_expr, select_expr, ...\nFROM table_reference\n[WHERE where_condition]\n[GROUP BY col_list]\n[ORDER BY col_list]\n[CLUSTER BY col_list\n| [DISTRIBUTE BY col_list] [SORT BY col_list]\n]\n[LIMIT number]\n6 .1 基本查询（ Select…From ）6 .1.1 全表和特定列查询0 ）数据准备（ 0 ）原始数据dept:10 ACCOUNTING 1700\n20 RESEARCH 1800\n30 SALES 1900\n40 OPERATIONS 1700\nemp：7369 SMITH CLERK 7902 1980 - 12 - 17 800.00 20\n\n—————————————————————————————————————7499 ALLEN SALESMAN 7698 1981 - 2 - 20 1600.00 300.00 30\n7521 WARD SALESMAN 7698 1981 - 2 - 22 1250.00 500.00 30\n7566 JONES MANAGER 7839 1981 - 4 - 2 2975.00 20\n7654 MARTIN SALESMAN 7698 1981 - 9 - 28 1250.00 1400.00 30\n7698 BLAKE MANAGER 7839 1981 - 5 - 1 2850.00 30\n7782 CLARK MANAGER 7839 1981 - 6 - 9 2450.00 10\n7788 SCOTT ANALYST 7566 1987 - 4 - 19 3000.00 20\n7839 KING PRESIDENT 1981 - 11 - 17 5000.00 10\n7844 TURNER SALESMAN 7698 1981 - 9 - 8 1500.00 0.00 30\n7876 ADAMS CLERK 7788 1987 - 5 - 23 1100.00 20\n7900 JAMES CLERK 7698 1981 - 12 - 3 950.00 30\n7902 FORD ANALYST 7566 1981 - 12 - 3 3000.00 20\n7934 MILLER CLERK 7782 1982 - 1 - 23 1300.00 10\n（ 1 ）创建部门表create table if not exists dept(\ndeptno int,\ndname string,\nloc int\n)\nrow format delimited fields terminated by &#39;\\t&#39;;\n（ 2 ）创建员工表create table if not exists emp(\nempno int,\nename string,\njob string,\nmgr int,\nhiredate string,\nsal double,\ncomm double,\ndeptno int)\nrow format delimited fields terminated by &#39;\\t&#39;;\n（ 3 ）导入数据load data local inpath &#39;&#x2F;opt&#x2F;module&#x2F;datas&#x2F;dept.txt&#39; into table\ndept;\nload data local inpath &#39;&#x2F;opt&#x2F;module&#x2F;datas&#x2F;emp.txt&#39; into table emp;\n1 ）全表查询hive (default)&gt; select * from emp;\nhive (default)&gt; select empno,ename,job,mgr,hiredate,sal,comm,deptno from\nemp ;\n2 ）选择特定列查询hive (default)&gt; select empno, ename from emp;\n注意：（ 1 ）SQL 语言大小写不敏感。（ 2 ）SQL 可以写在一行或者多行（ 3 ）关键字不能被缩写也不能分行（ 4 ）各子句一般要分行写。—————————————————————————————————————（ 5 ）使用缩进提高语句的可读性。6 .1.2 列别名1 ）重命名一个列2 ）便于计算3 ）紧跟列名，也可以在列名和别名之间加入关键字‘ AS ’4 ）案例实操查询名称和部门hive (default)&gt; select ename AS name, deptno dn from emp;\n6 .1.3 算术运算符运算符 描述\nA+B A和B 相加\nA-B A减去B\nA*B A和B 相乘\nA&#x2F;B A除以B\nA%B A对B取余\nA&amp;B A和B按位取与\nA|B A和B按位取或\nA^B A和B按位取异或\n~A A按位取反\n案例实操：查询出所有员工的薪水后加 1 显示。hive (default)&gt; select sal +1 from emp;\n6 .1.4 常用函数1 ）求总行数（ count ）hive (default)&gt; select count(*) cnt from emp;\n2 ）求工资的最大值（ max ）hive (default)&gt; select max(sal) max_sal from emp;\n3 ）求工资的最小值（ min ）hive (default)&gt; select min(sal) min_sal from emp;\n4 ）求工资的总和（ sum ）hive (default)&gt; select sum(sal) sum_sal from emp;\n5 ）求工资的平均值（ avg ）hive (default)&gt; select avg(sal) avg_sal from emp;\n6.1.5 Limit 语句典型的查询会返回多行数据。LIMIT子句用于限制返回的行数。hive (default)&gt; select * from emp limit 5;\n\n—————————————————————————————————————hive (default)&gt; select * from emp limit 2 ;\n6.1.6 Where 语句1 ）使用 WHERE 子句，将不满足条件的行过滤掉2 ） WHERE 子句紧随 FROM 子句3 ）案例实操查询出薪水大于 1000 的所有员工hive (default)&gt; select * from emp where sal &gt;1000;\n注意：where子句中不能使用字段别名。6 .1.7 比较运算符（ Between&#x2F;In&#x2F; Is Null ）1 ）下面表中描述了谓词操作符，这些操作符同样可以用于 JOIN…ON 和 HAVING 语句中。操作符 支持的数据类型 描述\nA&#x3D;B 基本数据类型 如果A等于B则返回TRUE，反之返回FALSE\nA&lt;&#x3D;&gt;B 基本数据类型 如果A和B都为NULL，则返回TRUE，如果一边为NULL，\n返回False\nA&lt;&gt;B, A!&#x3D;B 基本数据类型 A或者B为NULL则返回NULL；如果A不等于B，则返回\nTRUE，反之返回FALSE\nA&lt;B (^) 基本数据类型 A或者B为NULL，则返回NULL；如果A小于B，则返回TRUE，反之返回FALSEA&lt;&#x3D;B 基本数据类型 A或者B为NULL，则返回NULL；如果A小于等于B，则返回TRUE，反之返回FALSEA&gt;B 基本数据类型 A或者B为NULL，则返回NULL；如果A大于B，则返回TRUE，反之返回FALSEA&gt;&#x3D;B (^) 基本数据类型 A或者B为NULL，则返回NULL；如果A大于等于B，则返回TRUE，反之返回FALSEA [NOT] BETWEEN BAND C基本数据类型 如果A，B或者C任一为NULL，则结果为NULL。如果A的值大于等于B而且小于或等于C，则结果为TRUE，反之为FALSE。如果使用NOT关键字则可达到相反的效果。A IS NULL (^) 所有数据类型 如果A等于NULL，则返回TRUE，反之返回FALSEA IS NOT NULL 所有数据类型 如果A不等于NULL，则返回TRUE，反之返回FALSEIN(数值1, 数值2) 所有数据类型 使用 IN运算显示列表中的值A [NOT] LIKE B STRING 类型 B是一个SQL下的简单正则表达式，也叫通配符模式，如果A与其匹配的话，则返回TRUE；反之返回FALSE。B的表达式说明如下：‘x%’表示A必须以字母‘x’开头，‘%x’表示A必须以字母’x’结尾，而‘%x%’表示A包含有字母’x’,可以位于开头，结尾或者字符串中间。如果使用NOT关键字则可达到相反的效果。A RLIKE B, A REGEXP B STRING 类型 B是基于java的正则表达式，如果A与其匹配，则返回TRUE；反之返回FALSE。匹配使用的是JDK中的正则表达式接口\n—————————————————————————————————————实现的，因为正则也依据其中的规则。例如，正则表达式必须和\n整个字符串A相匹配，而不是只需与其字符串匹配。\n2 ）案例实操（ 1 ）查询出薪水等于 5000 的所有员工hive (default)&gt; select * from emp where sal &#x3D;5000;\n（ 2 ）查询工资在 500 到 1000 的员工信息hive (default)&gt; select * from emp where sal between 500 and 1000;\n（ 3 ）查询comm为空的所有员工信息hive (default)&gt; select * from emp where comm is null;\n（ 4 ）查询工资是 1500 或 5000 的员工信息hive (default)&gt; select * from emp where sal IN (1500, 5000);\n6.1.8 Like 和 RLike1 ）使用 LIKE 运算选择类似的值2 ）选择条件可以包含字符或数字 :% 代表零个或多个字符(任意个字符)。_ 代表一个字符。3 ） RLIKE 子句RLIKE子句是Hive中这个功能的一个扩展，其可以通过Java的正则表达式这个更强大的语言来指定匹配条件。4 ）案例实操（ 1 ）查找名字以A开头的员工信息hive (default)&gt; select * from emp where ename LIKE &#39;A%&#39;;\n（ 2 ）查找名字中第二个字母为A的员工信息hive (default)&gt; select * from emp where ename LIKE &#39;_A%&#39;;\n（ 3 ）查找名字中带有A的员工信息hive (default)&gt; select * from emp where ename RLIKE &#39;[A]&#39;;\n6 .1.9 逻辑运算符（ And&#x2F;Or&#x2F;Not ）操作符 含义\nAND 逻辑并\nOR 逻辑或\nNOT 逻辑否\n1 ）案例实操（ 1 ）查询薪水大于 1000 ，部门是 30hive (default)&gt; select * from emp where sal&gt;1000 and deptno&#x3D;30;\n\n—————————————————————————————————————（ 2 ）查询薪水大于 1000 ，或者部门是 30hive (default)&gt; select * from emp where sal&gt;1000 or deptno&#x3D;30;\n（ 3 ）查询除了 20 部门和 30 部门以外的员工信息hive (default)&gt; select * from emp where deptno not IN(30, 20);\n6 .2 分组6.2.1 Group By 语句GROUP BY语句通常会和聚合函数一起使用，按照一个或者多个列队结果进行分组，然后对每个组执行聚合操作。1 ）案例实操：（ 1 ）计算emp表每个部门的平均工资hive (default)&gt; select t.deptno, avg(t.sal) avg_sal from emp t group by\nt.deptno;\n（ 2 ）计算emp每个部门中每个岗位的最高薪水hive (default)&gt; select t.deptno, t.job, max(t.sal) max_sal from emp t\ngroup by\nt.deptno, t.job;\n6.2.2 Having 语句1 ） having 与 where 不同点（ 1 ）where后面不能写分组函数，而having后面可以使用分组函数。（ 2 ）having只用于group by分组统计语句。2 ）案例实操（ 1 ）求每个部门的平均薪水大于 2000 的部门求每个部门的平均工资hive (default)&gt; select deptno, avg(sal) from emp group by deptno;\n求每个部门的平均薪水大于 2000 的部门hive (default)&gt; select deptno, avg(sal) avg_sal from emp group by deptno\nhaving avg_sal &gt; 2000;\n6.3 Join 语句6.3.1 等值 JoinHive支持通常的SQL JOIN语句。1 ）案例实操（ 1 ）根据员工表和部门表中的部门编号相等，查询员工编号、员工名称和部门名称；hive (default)&gt; select e.empno, e.ename, d.deptno, d.dname from emp e\n\n—————————————————————————————————————join dept d on e.deptno &#x3D; d.deptno;\n6 .3.2 表的别名1 ）好处（ 1 ）使用别名可以简化查询。（ 2 ）使用表名前缀可以提高执行效率。2 ）案例实操合并员工表和部门表hive (default)&gt; select e.empno, e.ename, d.deptno from emp e join dept d\non e.deptno &#x3D; d.deptno;\n6 .3.3 内连接内连接：只有进行连接的两个表中都存在与连接条件相匹配的数据才会被保留下来。hive (default)&gt; select e.empno, e.ename, d.deptno from emp e join dept d\non e.deptno &#x3D; d.deptno;\n6 .3.4 左外连接左外连接：JOIN操作符左边表中符合WHERE子句的所有记录将会被返回。hive (default)&gt; select e.empno, e.ename, d.deptno from emp e left join\ndept d on e.deptno &#x3D; d.deptno;\n6 .3.5 右外连接右外连接：JOIN操作符右边表中符合WHERE子句的所有记录将会被返回。hive (default)&gt; select e.empno, e.ename, d.deptno from emp e right join\ndept d on e.deptno &#x3D; d.deptno;\n6 .3.6 满外连接满外连接：将会返回所有表中符合WHERE语句条件的所有记录。如果任一表的指定字段没有符合条件的值的话，那么就使用NULL值替代。hive (default)&gt; select e.empno, e.ename, d.deptno from emp e full join\ndept d on e.deptno &#x3D; d.deptno;\n6 .3.7 多表连接注意：连接 n个表，至少需要n- 1 个连接条件。例如：连接三个表，至少需要两个连接条件。数据准备1700 Beijing\n1800 London\n1900 Tokyo\n1 ）创建位置表—————————————————————————————————————create table if not exists location(\nloc int,\nloc_name string\n)\nrow format delimited fields terminated by &#39;\\t&#39;;\n2 ）导入数据hive (default)&gt; load data local inpath &#39;&#x2F;opt&#x2F;module&#x2F;datas&#x2F;location.txt&#39;\ninto table location;\n3 ）多表连接查询hive (default)&gt;SELECT e.ename, d.dname, l.loc_name\nFROM emp e\nJOIN dept d\nON d.deptno &#x3D; e.deptno\nJOIN location l\nON d.loc &#x3D; l.loc;\n大多数情况下，Hive会对每对JOIN连接对象启动一个MapReduce任务。本例中会首先启动一个MapReduce job对表e和表d进行连接操作，然后会再启动一个MapReduce job将第一个MapReduce job的输出和表l;进行连接操作。注意：为什么不是表d和表l先进行连接操作呢？这是因为Hive总是按照从左到右的顺序执行的。优化：当对 3 个或者更多表进行join连接时，如果每个on子句都使用相同的连接键的话，那么只会产生一个MapReduce job。6 .3.8 笛卡尔积1 ）笛卡尔集会在下面条件下产生（ 1 ）省略连接条件（ 2 ）连接条件无效（ 3 ）所有表中的所有行互相连接2 ）案例实操hive (default)&gt; select empno, dname from emp, dept;\n6 .4 排序6.4.1 全局排序（ Order By ）Order By：全局排序，只有一个Reducer1 ）使用 ORDER BY 子句排序ASC（ascend）: 升序（默认）DESC（descend）: 降序—————————————————————————————————————2 ） ORDER BY 子句在 SELECT 语句的结尾3 ）案例实操（ 1 ）查询员工信息按工资升序排列hive (default)&gt; select * from emp order by sal;\n（ 2 ）查询员工信息按工资降序排列hive (default)&gt; select * from emp order by sal desc;\n6 .4.2 按照别名排序按照员工薪水的 2 倍排序hive (default)&gt; select ename, sal*2 twosal from emp order by twosal;\n6 .4.3 多个列排序按照部门和工资升序排序hive (default)&gt; select ename, deptno, sal from emp order by deptno, sal;\n6 .4.4 每个 Reduce 内部排序（ Sort By ）Sort By：对于大规模的数据集order by的效率非常低。在很多情况下，并不需要全局排序，此时可以使用 sort by 。Sort by为每个reducer产生一个排序文件。每个Reducer内部进行排序，对全局结果集来说不是排序。1 ）设置 reduce 个数hive (default)&gt; set mapreduce.job.reduces&#x3D;3;\n2 ）查看设置 reduce 个数hive (default)&gt; set mapreduce.job.reduces;\n3 ）根据部门编号降序查看员工信息hive (default)&gt; select * from emp sort by deptno desc;\n4 ）将查询结果导入到文件中（按照部门编号降序排序）hive (default)&gt; insert overwrite local directory\n&#39;&#x2F;opt&#x2F;module&#x2F;data&#x2F;sortby-result&#39;\nselect * from emp sort by deptno desc;\n6 .4.5 分区（ Distribute By ）Distribute By： 在有些情况下，我们需要控制某个特定行应该到哪个reducer，通常是为了进行后续的聚集操作。 distribute by 子句可以做这件事。 distribute by 类似MR中partition（自定义分区），进行分区，结合sort by使用。对于distribute by进行测试，一定要分配多reduce进行处理，否则无法看到distributeby的效果。—————————————————————————————————————1 ）案例实操：（ 1 ）先按照部门编号分区，再按照员工编号降序排序。hive (default)&gt; set mapreduce.job.reduces&#x3D;3;\nhive (default)&gt; insert overwrite local directory\n&#39;&#x2F;opt&#x2F;module&#x2F;data&#x2F;distribute-result&#39; select * from emp distribute by\ndeptno sort by empno desc;\n注意：➢ distribute by的分区规则是根据分区字段的hash码与reduce的个数进行模除后，余数相同的分到一个区。➢ Hive要求DISTRIBUTE BY语句要写在SORT BY语句之前。6.4.6 Cluster By当distribute by和sorts by字段相同时，可以使用cluster by方式。cluster by除了具有distribute by的功能外还兼具sort by的功能。但是排序只能是升序排序，不能指定排序规则为ASC或者DESC。（ 1 ）以下两种写法等价hive (default)&gt; select * from emp cluster by deptno;\nhive (default)&gt; select * from emp distribute by deptno sort by deptno;\n注意：按照部门编号分区，不一定就是固定死的数值，可以是 20 号和 30 号部门分到一个分区里面去。第 7 章 分区表和分桶表7.1 分区表分区表实际上就是对应一个 HDFS 文件系统上的独立的文件夹，该文件夹下是该分区所有的数据文件。\nHive 中的分区 &#x3D; 分目录，把一个大的数据集根据业务需要分割成小的数据集。\n在查询时通过 WHERE 子句中的表达式选择查询所需要的指定的分区，这样的查询效率会提高很多。\nwhy? 数据量太大于是分门别类\nHow?\n1、创建\npartitioned by (day string)\ncreate table dept_partition(\n    deptno int,\n    dname string,\n    loc string\n)\npartitioned by (day string)\nrow format delimited fields terminated by &#39;\\t&#39;;\n\n\n\n2、加载数据\n3、查询\n分区名 = 值\nselect * from dept_partition where day='20200401';\n\n\n\n3、增加分区\n删除分区\n查看分区表有多少分区\n查看分区表结构\n二级分区 \nwhat：做了一级拆分后数据量还是太大，因此很容易想到，将一级分区的数据再分，比如说一级分区按天，二级分区按小时\nhow\n创建\n加载数据\n查询\n增加分区\n删除分区\n查看分区表有多少分区\n查看分区表结构\n7.2 分桶表第 8 章 函数8 .1 系统内置函数1 ）查看系统自带的函数hive&gt; show functions;\n2 ）显示自带的函数的用法hive&gt; desc function upper;\n3 ）详细显示自带的函数的用法hive&gt; desc function extended upper;\n8 .2 常用内置函数8 .2.1 空字段赋值1 ）函数说明NVL：给值为NULL的数据赋值，它的格式是NVL( value，default_value)。它的功能是如果value为NULL，则NVL函数返回default_value的值，否则返回value的值，如果两个参数都为NULL ，则返回NULL。2 ）数据准备：采用员工表3 ）查询：如果员工的 comm 为 NULL ，则用 - 1 代替hive (default)&gt; select comm,nvl(comm, -1) from emp;\nOK\ncomm _c1\n\n—————————————————————————————————————NULL -1.0\n300.0 300.0\n500.0 500.0\nNULL -1.0\n140 0.0 1400.0\nNULL -1.0\nNULL -1.0\nNULL -1.0\nNULL -1.0\n0.0 0.0\nNULL -1.0\nNULL -1.0\nNULL -1.0\nNULL -1.0\n4 ）查询：如果员工的 comm 为 NULL ，则用领导 id 代替hive (default)&gt; select comm, nvl(comm,mgr) from emp;\nOK\ncomm _c1\nNULL 7902.0\n300.0 300.0\n500.0 500.0\nNULL 7839.0\n1400.0 1400.0\nNULL 7839.0\nNULL 7839.0\nNULL 7566.0\nNULL NULL\n0.0 0.0\nNULL 7788.0\nNULL 7698.0\nNULL 7566.0\nNULL 7782.0\n8 .2.2 CASE WHEN THEN ELSE END1 ）数据准备name dept_id sex悟空 A^ 男大海 A^ 男宋宋 B^ 男凤姐 A 女婷姐 B 女婷婷 B 女2 ）需求求出不同部门男女各多少人。结果如下：dept_Id 男 女\nA 2 1\nB 1 2\n3 ）创建本地 emp_sex.txt ，导入数据—————————————————————————————————————[atguigu@hadoop102 datas]$ vi emp_sex.txt\n悟空 A 男\n大海 A 男\n宋宋 B 男\n凤姐 A 女\n婷姐 B 女\n婷婷 B 女\n4 ）创建 hive 表并导入数据create table emp_sex(\nname string,\ndept_id string,\nsex string)\nrow format delimited fields terminated by &quot;\\t&quot;;\nload data local inpath &#39;&#x2F;opt&#x2F;module&#x2F;hive&#x2F;data&#x2F;emp_sex.txt&#39; into table\nemp_sex;\n5 ）按需求查询数据select\ndept_id,\nsum(case sex when &#39;男&#39; then 1 else 0 end) male_count,\nsum(case sex when &#39;女&#39; then 1 else 0 end) female_count\nfrom emp_sex\ngroup by dept_id;\n8 .2.3 行转列1 ）相关函数说明CONCAT(string A&#x2F;col, string B&#x2F;col…)：返回输入字符串连接后的结果，支持任意个输入字符串;CONCAT_WS(separator, str1, str2,…)：它是一个特殊形式的 CONCAT()。第一个参数剩余参数间的分隔符。分隔符可以是与剩余参数一样的字符串。如果分隔符是 NULL，返回值也将为 NULL。这个函数会跳过分隔符参数后的任何 NULL 和空字符串。分隔符将被加到被连接的字符串之间;注意: CONCAT_WS must be “string or arrayCOLLECT_SET(col)：函数只接受基本数据类型，它的主要作用是将某字段的值进行去重汇总，产生Array类型字段。2 ）数据准备name constellation blood_type孙悟空 白羊座 A^大海 射手座 A^宋宋 白羊座 B^—————————————————————————————————————猪八戒 白羊座 A^凤姐 射手座 A苍老师 白羊座 B3 ）需求把星座和血型一样的人归类到一起。结果如下：射手座,A 大海|凤姐\n白羊座,A 孙悟空|猪八戒\n白羊座,B 宋宋|苍老师\n4 ）创建本地 constellation.txt ，导入数据[atguigu@hadoop102 datas]$ vim person_info.txt\n孙悟空 白羊座 A\n大海 射手座 A\n宋宋 白羊座 B\n猪八戒 白羊座 A\n凤姐 射手座 A\n苍老师 白羊座 B\n5 ）创建 hive 表并导入数据create table person_info(\nname string,\nconstellation string,\nblood_type string)\nrow format delimited fields terminated by &quot;\\t&quot;;\nload data local inpath &quot;&#x2F;opt&#x2F;module&#x2F;hive&#x2F;data&#x2F;person_info.txt&quot; into table\nperson_info;\n6 ）按需求查询数据SELECT\nt1.c_b,\nCONCAT_WS(&quot;|&quot;,collect_set(t1.name))\nFROM (\nSELECT\nNAME,\nCONCAT_WS(&#39;,&#39;,constellation,blood_type) c_b\nFROM person_info\n)t1\nGROUP BY t1.c_b\n8 .2.4 列转行1 ）函数说明EXPLODE(col)：将hive一列中复杂的Array或者Map结构拆分成多行。LATERAL VIEW用法：LATERAL VIEW udtf(expression) tableAlias AS columnAlias解释：用于和split, explode等UDTF一起使用，它能够将一列数据拆成多行数据，在此基础上可以对拆分后的数据进行聚合。—————————————————————————————————————2 ）数据准备表 6 - 7 数据准备movie category《疑犯追踪》 悬疑,动作,科幻,剧情《Lie to me》 悬疑,警匪,动作,心理,剧情《战狼 2 》 战争,动作,灾难3 ）需求将电影分类中的数组数据展开。结果如下：《疑犯追踪》 悬疑\n《疑犯追踪》 动作\n《疑犯追踪》 科幻\n《疑犯追踪》 剧情\n《Lie to me》 悬疑\n《Lie to me》 警匪\n《Lie to me》 动作\n《Lie to me》 心理\n《Lie to me》 剧情\n《战狼 2 》 战争\n《战狼 2 》 动作\n《战狼 2 》 灾难\n4 ）创建本地 movie.txt ，导入数据[atguigu@hadoop102 datas]$ vi movie_info.txt\n《疑犯追踪》 悬疑,动作,科幻,剧情\n《Lie to me》 悬疑,警匪,动作,心理,剧情\n《战狼 2 》 战争,动作,灾难\n5 ）创建 hive 表并导入数据create table movie_info(\nmovie string,\ncategory string)\nrow format delimited fields terminated by &quot;\\t&quot;;\nload data local inpath &quot;&#x2F;opt&#x2F;module&#x2F;data&#x2F;movie.txt&quot; into table\nmovie_info;\n6 ）按需求查询数据SELECT\nmovie,\ncategory_name\nFROM\nmovie_info\nlateral VIEW\nexplode(split(category,&quot;,&quot;)) movie_info_tmp AS category_name;\n8 .2.5 窗口函数（开窗函数）1 ）相关函数说明OVER()：指定分析函数工作的数据窗口大小，这个数据窗口大小可能会随着行的变而变—————————————————————————————————————化。CURRENT ROW：当前行n PRECEDING：往前n行数据n FOLLOWING：往后n行数据UNBOUNDED：起点，UNBOUNDED PRECEDING 表示从前面的起点，UNBOUNDED FOLLOWING表示到后面的终点LAG(col,n,default_val)：往前第n行数据LEAD(col,n, default_val)：往后第n行数据NTILE(n)：把有序窗口的行分发到指定数据的组中，各个组有编号，编号从 1 开始，对于每一行，NTILE返回此行所属的组的编号。注意：n必须为int类型。2 ）数据准备： name ， orderdate ， costjack,2017- 01 - 01,10\ntony,2017- 01 - 02,15\njack,2017- 02 - 03,23\ntony,2017- 01 - 04,29\njack,2017- 01 - 05,46\njack,2017- 04 - 06,42\ntony,2017- 01 - 07,50\njack,2017- 01 - 08,55\nmart,2017- 04 - 08,62\nmart,2017- 04 - 09,68\nneil,2017- 05 - 10,12\nmart,2017- 04 - 11,75\nneil,2017- 06 - 12,80\nmart,2017- 04 - 13,94\n3 ）需求（ 1 ）查询在 2017 年 4 月份购买过的顾客及总人数（ 2 ）查询顾客的购买明细及月购买总额（ 3 ）上述的场景, 将每个顾客的cost按照日期进行累加（ 4 ）查询每个顾客上次的购买时间（ 5 ）查询前20%时间的订单信息4 ）创建本地 business.txt ，导入数据[atguigu@hadoop102 datas]$ vi business.txt\n5 ）创建 hive 表并导入数据create table business(\nname string,\norderdate string,\n\n—————————————————————————————————————cost int\n) ROW FORMAT DELIMITED FIELDS TERMINATED BY &#39;,&#39;;\nload data local inpath &quot;&#x2F;opt&#x2F;module&#x2F;data&#x2F;business.txt&quot; into table\nbusiness;\n6 ）按需求查询数据（ 1 ） 查询在 2017 年 4 月份购买过的顾客及总人数select name,count(*) over ()\nfrom business\nwhere substring(orderdate,1,7) &#x3D; &#39;2017-04&#39;\ngroup by name;\n（ 2 ） 查询顾客的购买明细及月购买总额select name,orderdate,cost,sum(cost) over(partition by month(orderdate))\nfrom business;\n（ 3 ） 将每个顾客的cost按照日期进行累加select name,orderdate,cost,\nsum(cost) over() as sample1,--所有行相加\nsum(cost) over(partition by name) as sample2,--按name分组，组内数据相加\nsum(cost) over(partition by name order by orderdate) as sample3,--按name\n分组，组内数据累加\nsum(cost) over(partition by name order by orderdate rows between\nUNBOUNDED PRECEDING and current row ) as sample4 ,--和sample3一样,由起点到\n当前行的聚合\nsum(cost) over(partition by name order by orderdate rows between 1\nPRECEDING and current row) as sample5, --当前行和前面一行做聚合\nsum(cost) over(partition by name order by orderdate rows between 1\nPRECEDING AND 1 FOLLOWING ) as sample6,--当前行和前边一行及后面一行\nsum(cost) over(partition by name order by orderdate rows between current\nrow and UNBOUNDED FOLLOWING ) as sample7 --当前行及后面所有行\nfrom business;\nrows必须跟在order by 子句之后，对排序的结果进行限制，使用固定的行数来限制分区中的数据行数量（ 4 ） 查看顾客上次的购买时间select name,orderdate,cost,\nlag(orderdate,1,&#39;1900- 01 - 01&#39;) over(partition by name order by orderdate )\nas time1, lag(orderdate,2) over (partition by name order by orderdate) as\ntime2\nfrom business;\n（ 5 ） 查询前20%时间的订单信息select * from (\nselect name,orderdate,cost, ntile(5) over(order by orderdate) sorted\nfrom business\n) t\nwhere sorted &#x3D; 1;\n8 .2.6 Rank1 ）函数说明RANK() 排序相同时会重复，总数不会变—————————————————————————————————————DENSE_RANK() 排序相同时会重复，总数会减少ROW_NUMBER() 会根据顺序计算2 ）数据准备name subject score孙悟空 语文 87孙悟空 数学 95孙悟空 英语 68大海 语文 94大海 数学 56大海 英语 84宋宋 语文 64宋宋 数学 86宋宋 英语 84婷婷 语文 65婷婷 数学 85婷婷 英语 783 ）需求计算每门学科成绩排名。4 ）创建本地 score.txt ，导入数据[atguigu@hadoop102 datas]$ vi score.txt\n5 ）创建 hive 表并导入数据create table score(\nname string,\nsubject string,\nscore int)\nrow format delimited fields terminated by &quot;\\t&quot;;\nload data local inpath &#39;&#x2F;opt&#x2F;module&#x2F;data&#x2F;score.txt&#39; into table score;\n6 ）按需求查询数据select name,\nsubject,\nscore,\nrank() over(partition by subject order by score desc) rp,\ndense_rank() over(partition by subject order by score desc) drp,\nrow_number() over(partition by subject order by score desc) rmp\nfrom score;\nname subject score rp drp rmp\n孙悟空 数学 95 1 1 1\n宋宋 数学 86 2 2 2\n婷婷 数学 85 3 3 3\n大海 数学 56 4 4 4\n\n—————————————————————————————————————宋宋 英语 84 1 1 1\n大海 英语 84 1 1 2\n婷婷 英语 78 3 2 3\n孙悟空 英语 68 4 3 4\n大海 语文 94 1 1 1\n孙悟空 语文 87 2 2 2\n婷婷 语文 65 3 3 3\n宋宋 语文 64 4 4 4\n扩展：求出每门学科前三名的学生？8 .2.7 其他常用函数..\\2.资料\\05_常用函数\\常用函数.txt8 .3 自定义函数1 ）Hive 自带了一些函数，比如：max&#x2F;min等，但是数量有限，自己可以通过自定义UDF来方便的扩展。2 ）当Hive提供的内置函数无法满足你的业务处理需要时，此时就可以考虑使用用户自定义函数（UDF：user-defined function）。3 ）根据用户自定义函数类别分为以下三种：（ 1 ）UDF（User-Defined-Function）一进一出（ 2 ）UDAF（User-Defined Aggregation Function）聚集函数，多进一出类似于：count&#x2F;max&#x2F;min（ 3 ）UDTF（User-Defined Table-Generating Functions）一进多出如lateral view explode()4 ）官方文档地址https://cwiki.apache.org/confluence/display/Hive/HivePlugins5 ）编程步骤：（ 1 ）继承Hive提供的类org.apache.hadoop.hive.ql.udf.generic.GenericUDForg.apache.hadoop.hive.ql.udf.generic.GenericUDTF;（ 2 ）实现类中的抽象方法—————————————————————————————————————（ 3 ）在hive的命令行窗口创建函数添加jaradd jar linux_jar_path\n创建functioncreate [temporary] function [dbname.]function_name AS class_name;\n（ 4 ）在hive的命令行窗口删除函数drop [temporary] function [if exists] [dbname.]function_name;\n8 .4 自定义 UDF 函数0 ）需求 :自定义一个UDF实现计算给定字符串的长度，例如：hive(default)&gt; select my_len(&quot;abcd&quot;);\n4\n1 ）创建一个 Maven 工程 Hive2 ）导入依赖&lt;dependencies&gt;\n&lt;dependency&gt;\n&lt;groupId&gt;org.apache.hive&lt;&#x2F;groupId&gt;\n&lt;artifactId&gt;hive-exec&lt;&#x2F;artifactId&gt;\n&lt;version&gt;3.1.2&lt;&#x2F;version&gt;\n&lt;&#x2F;dependency&gt;\n&lt;&#x2F;dependencies&gt;\n3 ）创建一个类package com.atguigu.hive;\nimport org.apache.hadoop.hive.ql.exec.UDFArgumentException;\nimport org.apache.hadoop.hive.ql.exec.UDFArgumentLengthException;\nimport org.apache.hadoop.hive.ql.exec.UDFArgumentTypeException;\nimport org.apache.hadoop.hive.ql.metadata.HiveException;\nimport org.apache.hadoop.hive.ql.udf.generic.GenericUDF;\nimport org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;\nimport\norg.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectIn\nspectorFactory;\n&#x2F;**\n* 自定义UDF函数，需要继承GenericUDF类\n* 需求: 计算指定字符串的长度\n*&#x2F;\npublic class MyStringLength extends GenericUDF &#123;\n&#x2F;**\n*\n* @param arguments 输入参数类型的鉴别器对象\n* @return 返回值类型的鉴别器对象\n* @throws UDFArgumentException\n*&#x2F;\n@Override\npublic ObjectInspector initialize(ObjectInspector[] arguments) throws\nUDFArgumentException &#123;\n\n—————————————————————————————————————&#x2F;&#x2F; 判断输入参数的个数\nif(arguments.length !&#x3D;1)&#123;\nthrow new UDFArgumentLengthException(&quot;Input Args Length\nError!!!&quot;);\n&#125;\n&#x2F;&#x2F; 判断输入参数的类型\nif(!arguments[0].getCategory().equals(ObjectInspector.Category.PRIMITIVE)\n)&#123;\nthrow new UDFArgumentTypeException(0,&quot;Input Args Type\nError!!!&quot;);\n&#125;\n&#x2F;&#x2F;函数本身返回值为int，需要返回int类型的鉴别器对象\nreturn PrimitiveObjectInspectorFactory.javaIntObjectInspector;\n&#125;\n&#x2F;**\n* 函数的逻辑处理\n* @param arguments 输入的参数\n* @return 返回值\n* @throws HiveException\n*&#x2F;\n@Override\npublic Object evaluate(DeferredObject[] arguments) throws\nHiveException &#123;\nif(arguments[0].get() &#x3D;&#x3D; null)&#123;\nreturn 0;\n&#125;\nreturn arguments[0].get().toString().length();\n&#125;\n@Override\npublic String getDisplayString(String[] children) &#123;\nreturn &quot;&quot;;\n&#125;\n&#125;\n4 ）打成 jar 包上传到服务器 &#x2F;opt&#x2F;module&#x2F;data&#x2F;myudf.jar5 ）将 jar 包添加到 hive 的 classpathhive (default)&gt; add jar &#x2F;opt&#x2F;module&#x2F;data&#x2F;myudf.jar;\n6 ）创建临时函数与开发好的 java class 关联hive (default)&gt; create temporary function my_len as &quot;com.atguigu.hive.\nMyStringLength&quot;;\n7 ）即可在 hql 中使用自定义的函数hive (default)&gt; select ename,my_len(ename) ename_len from emp;\n8 .5 自定义UDTF函数0 ）需求自定义一个UDTF实现将一个任意分割符的字符串切割成独立的单词，例如：—————————————————————————————————————hive(default)&gt; select myudtf(&quot;hello,world,hadoop,hive&quot;, &quot;,&quot;);\nhello\nworld\nhadoop\nhive\n1 ）代码实现package com.atguigu.udtf;\nimport org.apache.hadoop.hive.ql.exec.UDFArgumentException;\nimport org.apache.hadoop.hive.ql.metadata.HiveException;\nimport org.apache.hadoop.hive.ql.udf.generic.GenericUDTF;\nimport org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;\nimport\norg.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;\nimport\norg.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector;\nimport\norg.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectIn\nspectorFactory;\nimport java.util.ArrayList;\nimport java.util.List;\npublic class MyUDTF extends GenericUDTF &#123;\nprivate ArrayList&lt;String&gt; outList &#x3D; new ArrayList&lt;&gt;();\n@Override\npublic StructObjectInspector initialize(StructObjectInspector argOIs)\nthrows UDFArgumentException &#123;\n&#x2F;&#x2F;1.定义输出数据的列名和类型\nList&lt;String&gt; fieldNames &#x3D; new ArrayList&lt;&gt;();\nList&lt;ObjectInspector&gt; fieldOIs &#x3D; new ArrayList&lt;&gt;();\n&#x2F;&#x2F;2.添加输出数据的列名和类型\nfieldNames.add(&quot;lineToWord&quot;);\nfieldOIs.add(PrimitiveObjectInspectorFactory.javaStringObjectInspector);\nreturn\nObjectInspectorFactory.getStandardStructObjectInspector(fieldNames,\nfieldOIs);\n&#125;\n@Override\npublic void process(Object[] args) throws HiveException &#123;\n&#x2F;&#x2F;1.获取原始数据\nString arg &#x3D; args[0].toString();\n&#x2F;&#x2F;2.获取数据传入的第二个参数，此处为分隔符\nString splitKey &#x3D; args[1].toString();\n&#x2F;&#x2F;3.将原始数据按照传入的分隔符进行切分\nString[] fields &#x3D; arg.split(splitKey);\n\n—————————————————————————————————————&#x2F;&#x2F;4.遍历切分后的结果，并写出\nfor (String field : fields) &#123;\n&#x2F;&#x2F;集合为复用的，首先清空集合\noutList.clear();\n&#x2F;&#x2F;将每一个单词添加至集合\noutList.add(field);\n&#x2F;&#x2F;将集合内容写出\nforward(outList);\n&#125;\n&#125;\n@Override\npublic void close() throws HiveException &#123;\n&#125;\n&#125;\n2 ）打成 jar 包上传到服务器 &#x2F;opt&#x2F;module&#x2F;hive&#x2F;data&#x2F;myudtf.jar3 ）将 jar 包添加到 hive 的 classpath 下hive (default)&gt; add jar &#x2F;opt&#x2F;module&#x2F;hive&#x2F;data&#x2F;myudtf.jar;\n4 ）创建临时函数与开发好的 java class 关联hive (default)&gt; create temporary function myudtf as\n&quot;com.atguigu.hive.MyUDTF&quot;;\n5 ）使用自定义的函数hive (default)&gt; select myudtf(&quot;hello,world,hadoop,hive&quot;,&quot;,&quot;);\n第 9 章 压缩和存储9 .1 Hadoop 压缩配置9 .1.1 MR 支持的压缩编码压缩格式 算法 文件扩展名 是否可切分\nDEFLATE DEFLATE .deflate 否\nGzip DEFLATE .gz 否\nbzip2 bzip2 .bz2 是\nLZO LZO .lzo 是\nSnappy Snappy .snappy 否\n为了支持多种压缩&#x2F;解压缩算法，Hadoop引入了编码&#x2F;解码器，如下表所示：压缩格式 对应的编码&#x2F;解码器\nDEFLATE org.apache.hadoop.io.compress.DefaultCodec\ngzip org.apache.hadoop.io.compress.GzipCodec\nbzip2 org.apache.hadoop.io.compress.BZip2Codec\nLZO com.hadoop.compression.lzo.LzopCodec\n\n—————————————————————————————————————Snappy org.apache.hadoop.io.compress.SnappyCodec\n压缩性能的比较：压缩算法 (^) 原始文件大小 压缩文件大小 压缩速度 解压速度gzip 8.3GB 1.8GB 17.5MB&#x2F;s 58MB&#x2F;sbzip2 8.3GB 1.1GB 2.4MB&#x2F;s 9.5MB&#x2F;sLZO 8.3GB 2.9GB 49.3MB&#x2F;s 74.6MB&#x2F;s\nhttp://google.github.io/snappy/On a single core of a Core i7 processor in 64-bit mode, Snappy compresses at about 250MB&#x2F;sec or more and decompresses at about 500 MB&#x2F;sec or more.9 .1.2 压缩参数配置要在Hadoop中启用压缩，可以配置如下参数（mapred-site.xml文件中）：参数 默认值 阶段 建议\nio.compression.codecs\n（在core-site.xml中配置）\norg.apache.hadoop.io.compress.DefaultCodec,\norg.apache.hadoop.io.compress.GzipCodec,\norg.apache.hadoop.io.compress.BZip2Codec,\norg.apache.hadoop.io.compress.Lz4Codec\n输入压缩 Hadoop使用文件扩展\n名判断是否支持某种\n编解码器\nmapreduce.map.output.com\npress\nfalse (^) mapper输出 这个参数设为true启用压缩mapreduce.map.output.compress.codecorg.apache.hadoop.io.compress.DefaultCodec mapper输出 使用LZO、LZ4或snappy编解码器在此阶段压缩数据mapreduce.output.fileoutputformat.compressfalse (^) reducer输出 这个参数设为true启用压缩mapreduce.output.fileoutputformat.compress.codecorg.apache.hadoop.io.compress. DefaultCodec reducer输出 使用标准工具或者编解码器，如gzip和bzip2mapreduce.output.fileoutputformat.compress.typeRECORD reducer输出 SequenceFile输出使用的压缩类型：NONE和BLOCK\n9 .2 开启 Map 输出阶段压缩（ MR 引擎）开启map输出阶段压缩可以减少job中map和Reduce task间数据传输量。具体配置如下：1 ）案例实操：（ 1 ）开启hive中间传输数据压缩功能hive (default)&gt;set hive.exec.compress.intermediate&#x3D;true;\n（ 2 ）开启mapreduce中map输出压缩功能—————————————————————————————————————hive (default)&gt;set mapreduce.map.output.compress&#x3D;true;\n（ 3 ）设置mapreduce中map输出数据的压缩方式hive (default)&gt;set mapreduce.map.output.compress.codec&#x3D;\norg.apache.hadoop.io.compress.SnappyCodec;\n（ 4 ）执行查询语句hive (default)&gt; select count(ename) name from emp;\n9 .3 开启 Reduce 输出阶段压缩当 Hive 将 输 出 写 入 到 表 中 时，输出内容同样可以进行压缩。属性hive.exec.compress.output控制着这个功能。用户可能需要保持默认设置文件中的默认值false，这样默认的输出就是非压缩的纯文本文件了。用户可以通过在查询语句或执行脚本中设置这个值为true，来开启输出结果压缩功能。1 ）案例实操：（ 1 ）开启hive最终输出数据压缩功能hive (default)&gt;set hive.exec.compress.output&#x3D;true;\n（ 2 ）开启mapreduce最终输出数据压缩hive (default)&gt;set mapreduce.output.fileoutputformat.compress&#x3D;true;\n（ 3 ）设置mapreduce最终数据输出压缩方式hive (default)&gt; set mapreduce.output.fileoutputformat.compress.codec &#x3D;\norg.apache.hadoop.io.compress.SnappyCodec;\n（ 4 ）设置mapreduce最终数据输出压缩为块压缩hive (default)&gt; set\nmapreduce.output.fileoutputformat.compress.type&#x3D;BLOCK;\n（ 5 ）测试一下输出结果是否是压缩文件hive (default)&gt; insert overwrite local directory\n&#39;&#x2F;opt&#x2F;module&#x2F;data&#x2F;distribute-result&#39; select * from emp distribute by\ndeptno sort by empno desc;\n9 .4 文件存储格式Hive支持的存储数据的格式主要有：TEXTFILE 、SEQUENCEFILE、ORC、PARQUET。—————————————————————————————————————9 .4.1 列式存储和行式存储如图所示左边为逻辑表，右边第一个为行式存储，第二个为列式存储。1 ）行存储的特点查询满足条件的一整行数据的时候，列存储则需要去每个聚集的字段找到对应的每个列的值，行存储只需要找到其中一个值，其余的值都在相邻地方，所以此时行存储查询的速度更快。2 ）列存储的特点因为每个字段的数据聚集存储，在查询只需要少数几个字段的时候，能大大减少读取的数据量；每个字段的数据类型一定是相同的，列式存储可以针对性的设计更好的设计压缩算法。TEXTFILE和SEQUENCEFILE的存储格式都是基于行存储的；ORC和PARQUET是基于列式存储的。9 .4.2 TextFile 格式默认格式，数据不做压缩，磁盘开销大，数据解析开销大。可结合Gzip、Bzip2使用，但使用Gzip这种方式，hive不会对数据进行切分，从而无法对数据进行并行操作。9 .4.3 Orc 格式Orc (Optimized Row Columnar)是Hive 0.11版里引入的新的存储格式。如下图所示可以看到每个Orc文件由 1 个或多个stripe组成，每个stripe一般为HDFS的块大小，每一个stripe包含多条记录，这些记录按照列进行独立存储，对应到Parquet中的row group的概念。每个Stripe里有三部分组成，分别是Index Data，Row Data，StripeFooter：—————————————————————————————————————1 ）Index Data：一个轻量级的index，默认是每隔1W行做一个索引。这里做的索引应该只是记录某行的各字段在Row Data中的offset。2 ）Row Data：存的是具体的数据，先取部分行，然后对这些行按列进行存储。对每个列进行了编码，分成多个Stream来存储。3 ）Stripe Footer：存的是各个Stream的类型，长度等信息。每个文件有一个File Footer，这里面存的是每个Stripe的行数，每个Column的数据类型信息等；每个文件的尾部是一个PostScript，这里面记录了整个文件的压缩类型以及FileFooter的长度信息等。在读取文件时，会seek到文件尾部读PostScript，从里面解析到File Footer长度，再读FileFooter，从里面解析到各个Stripe信息，再读各个Stripe，即从后往前读。—————————————————————————————————————9 .4.4 Parquet 格式Parquet文件是以二进制方式存储的，所以是不可以直接读取的，文件中包括该文件的数据和元数据，因此Parquet格式文件是自解析的。（ 1 ）行组(Row Group)：每一个行组包含一定的行数，在一个HDFS文件中至少存储一个行组，类似于orc的stripe的概念。（ 2 ）列块(Column Chunk)：在一个行组中每一列保存在一个列块中，行组中的所有列连续的存储在这个行组文件中。一个列块中的值都是相同类型的，不同的列块可能使用不同的算法进行压缩。（ 3 ）页(Page)：每一个列块划分为多个页，一个页是最小的编码的单位，在同一个列块的不同页可能使用不同的编码方式。通常情况下，在存储Parquet数据的时候会按照Block大小设置行组的大小，由于一般情况下每一个Mapper任务处理数据的最小单位是一个Block，这样可以把每一个行组由一个Mapper任务处理，增大任务执行并行度。Parquet文件的格式。2上图展示了一个Parquet文件的内容，一个文件中可以存储多个行组，文件的首位都是该文件的Magic Code，用于校验它是否是一个Parquet文件，Footer length记录了文件元数—————————————————————————————————————据的大小，通过该值和文件长度可以计算出元数据的偏移量，文件的元数据中包括每一个行组的元数据信息和该文件存储数据的Schema信息。除了文件中每一个行组的元数据，每一页的开始都会存储该页的元数据，在Parquet中，有三种类型的页：数据页、字典页和索引页。数据页用于存储当前行组中该列的值，字典页存储该列值的编码字典，每一个列块中最多包含一个字典页，索引页用来存储当前行组下该列的索引，目前Parquet中还不支持索引页。9 .4.5 主流文件存储格式对比实验从存储文件的压缩比和查询速度两个角度对比。存储文件的压缩比测试：1 ）测试数据log.data\n2 ） TextFile（ 1 ）创建表，存储数据格式为TEXTFILEcreate table log_text (\ntrack_time string,\nurl string,\nsession_id string,\nreferer string,\nip string,\nend_user_id string,\ncity_id string\n)\nrow format delimited fields terminated by &#39;\\t&#39;\nstored as textfile;\n（ 2 ）向表中加载数据hive (default)&gt; load data local inpath &#39;&#x2F;opt&#x2F;module&#x2F;hive&#x2F;datas&#x2F;log.data&#39;\ninto table log_text ;\n（ 3 ）查看表中数据大小hive (default)&gt; dfs -du -h &#x2F;user&#x2F;hive&#x2F;warehouse&#x2F;log_text;\n18.1 3 M &#x2F;user&#x2F;hive&#x2F;warehouse&#x2F;log_text&#x2F;log.data3 ） ORC（ 1 ）创建表，存储数据格式为ORCcreate table log_orc(\ntrack_time string,\nurl string,\nsession_id string,\nreferer string,\nip string,\n\n—————————————————————————————————————end_user_id string,\ncity_id string\n)\nrow format delimited fields terminated by &#39;\\t&#39;\nstored as orc\ntblproperties(&quot;orc.compress&quot;&#x3D;&quot;NONE&quot;); -- 设置orc存储不使用压缩\n（ 2 ）向表中加载数据hive (default)&gt; insert into table log_orc select * from log_text;\n（ 3 ）查看表中数据大小hive (default)&gt; dfs -du -h &#x2F;user&#x2F;hive&#x2F;warehouse&#x2F;log_orc&#x2F; ;\n7. 7 M &#x2F;user&#x2F;hive&#x2F;warehouse&#x2F;log_orc&#x2F;000000_04 ） Parquet（ 1 ）创建表，存储数据格式为parquetcreate table log_parquet(\ntrack_time string,\nurl string,\nsession_id string,\nreferer string,\nip string,\nend_user_id string,\ncity_id string\n)\nrow format delimited fields terminated by &#39;\\t&#39;\nstored as parquet;\n（ 2 ）向表中加载数据hive (default)&gt; insert into table log_parquet select * from log_text;\n（ 3 ）查看表中数据大小hive (default)&gt; dfs -du -h &#x2F;user&#x2F;hive&#x2F;warehouse&#x2F;log_parquet&#x2F;;\n13.1 M &#x2F;user&#x2F;hive&#x2F;warehouse&#x2F;log_parquet&#x2F;000000_0存储文件的对比总结：ORC &gt; Parquet &gt; textFile存储文件的查询速度测试：（ 1 ）TextFilehive (default)&gt; insert overwrite local directory\n&#39;&#x2F;opt&#x2F;module&#x2F;data&#x2F;log_text&#39; select substring(url,1,4) from log_text;\n（ 2 ）ORChive (default)&gt; insert overwrite local directory\n&#39;&#x2F;opt&#x2F;module&#x2F;data&#x2F;log_orc&#39; select substring(url,1,4) from log_orc;\n（ 3 ）Parquethive (default)&gt; insert overwrite local directory\n&#39;&#x2F;opt&#x2F;module&#x2F;data&#x2F;log_parquet&#39; select substring(url,1,4) from\nlog_parquet;\n存储文件的查询速度总结：查询速度相近。—————————————————————————————————————9 .5 存储和压缩结合9 .5.1 测试存储和压缩官网：https://cwiki.apache.org/confluence/display/Hive/LanguageManual+ORCORC存储方式的压缩：Key Default Notes\norc.compress ZLIB high level compression (one of NONE, ZLIB, SNAPPY)\norc.compress.size 262,144 number of bytes in each compression chunk\norc.stripe.size 268 , 435 , 456 number of bytes in each stripe\norc.row.index.stride 10,000 number of rows between index entries (must be &gt;&#x3D;\n1000)\norc.create.index true whether to create row indexes\norc.bloom.filter.columns &quot;&quot; comma separated list of column names for which\nbloom filter should be created\norc.bloom.filter.fpp 0.05 false positive probability for bloom filter (must &gt;0.0\nand &lt;1.0)\n注意：所有关于ORCFile的参数都是在HQL语句的TBLPROPERTIES字段里面出现1 ）创建一个 ZLIB 压缩的 ORC 存储方式（ 1 ）建表语句create table log_orc_zlib(\ntrack_time string,\nurl string,\nsession_id string,\nreferer string,\nip string,\nend_user_id string,\ncity_id string\n)\nrow format delimited fields terminated by &#39;\\t&#39;\nstored as orc\ntblproperties(&quot;orc.compress&quot;&#x3D;&quot;ZLIB&quot;);\n（ 2 ）插入数据insert into log_orc_zlib select * from log_text;\n（ 3 ）查看插入后数据hive (default)&gt; dfs -du -h &#x2F;user&#x2F;hive&#x2F;warehouse&#x2F;log_orc_zlib&#x2F; ;\n2.78 M &#x2F;user&#x2F;hive&#x2F;warehouse&#x2F;log_orc_none&#x2F;000000_02 ）创建一个 SNAPPY 压缩的 ORC 存储方式（ 1 ）建表语句create table log_orc_snappy(\ntrack_time string,\nurl string,\nsession_id string,\n\n—————————————————————————————————————referer string,\nip string,\nend_user_id string,\ncity_id string\n)\nrow format delimited fields terminated by &#39;\\t&#39;\nstored as orc\ntblproperties(&quot;orc.compress&quot;&#x3D;&quot;SNAPPY&quot;);\n（ 2 ）插入数据insert into log_orc_snappy select * from log_text;\n（ 3 ）查看插入后数据hive (default)&gt; dfs -du -h &#x2F;user&#x2F;hive&#x2F;warehouse&#x2F;log_orc_snappy&#x2F;;\n3. 75 M &#x2F;user&#x2F;hive&#x2F;warehouse&#x2F;log_orc_snappy&#x2F;000000_0ZLIB比Snappy压缩的还小。原因是ZLIB采用的是deflate压缩算法。比snappy压缩的压缩率高。3 ）创建一个 SNAPPY 压缩的 parquet 存储方式（ 1 ）建表语句create table log_parquet_snappy(\ntrack_time string,\nurl string,\nsession_id string,\nreferer string,\nip string,\nend_user_id string,\ncity_id string\n)\nrow format delimited fields terminated by &#39;\\t&#39;\nstored as parquet\ntblproperties(&quot;parquet.compression&quot;&#x3D;&quot;SNAPPY&quot;);\n（ 2 ）插入数据insert into log_parquet_snappy select * from log_text;\n（ 3 ）查看插入后数据hive (default)&gt; dfs -du -h &#x2F;user&#x2F;hive&#x2F;warehouse&#x2F;log_parquet_snappy&#x2F;;\n6.39 MB &#x2F;user&#x2F;hive&#x2F;warehouse&#x2F; log_parquet_snappy &#x2F;000000_04 ）存储方式和压缩总结在实际的项目开发当中，hive表的数据存储格式一般选择：orc或parquet。压缩方式一般选择snappy，lzo。第 10 章 企业级调优1 0. 1 执行计划（ Explain ）1 ）基本语法—————————————————————————————————————EXPLAIN [EXTENDED | DEPENDENCY | AUTHORIZATION] query2 ）案例实操（ 1 ）查看下面这条语句的执行计划没有生成MR任务的hive (default)&gt; explain select * from emp;\nExplain\nSTAGE DEPENDENCIES:\nStage-0 is a root stage\nSTAGE PLANS:\nStage: Stage- 0\nFetch Operator\nlimit: - 1\nProcessor Tree:\nTableScan\nalias: emp\nStatistics: Num rows: 1 Data size: 7020 Basic stats: COMPLETE\nColumn stats: NONE\nSelect Operator\nexpressions: empno (type: int), ename (type: string), job\n(type: string), mgr (type: int), hiredate (type: string), sal (type:\ndouble), comm (type: double), deptno (type: int)\noutputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5,\n_col6, _col7\nStatistics: Num rows: 1 Data size: 7020 Basic stats: COMPLETE\nColumn stats: NONE\nListSink\n有生成MR任务的hive (default)&gt; explain select deptno, avg(sal) avg_sal from emp group by\ndeptno;\nExplain\nSTAGE DEPENDENCIES:\nStage-1 is a root stage\nStage-0 depends on stages: Stage- 1\nSTAGE PLANS:\nStage: Stage- 1\nMap Reduce\nMap Operator Tree:\nTableScan\nalias: emp\nStatistics: Num rows: 1 Data size: 7020 Basic stats: COMPLETE\nColumn stats: NONE\nSelect Operator\nexpressions: sal (type: double), deptno (type: int)\noutputColumnNames: sal, deptno\nStatistics: Num rows: 1 Data size: 7020 Basic stats: COMPLETE\nColumn stats: NONE\nGroup By Operator\naggregations: sum(sal), count(sal)\nkeys: deptno (type: int)\nmode: hash\noutputColumnNames: _col0, _col1, _col2\nStatistics: Num rows: 1 Data size: 7020 Basic stats:\n\n—————————————————————————————————————COMPLETE Column stats: NONE\nReduce Output Operator\nkey expressions: _col0 (type: int)\nsort order: +\nMap-reduce partition columns: _col0 (type: int)\nStatistics: Num rows: 1 Data size: 7020 Basic stats:\nCOMPLETE Column stats: NONE\nvalue expressions: _col1 (type: double), _col2 (type:\nbigint)\nExecution mode: vectorized\nReduce Operator Tree:\nGroup By Operator\naggregations: sum(VALUE._col0), count(VALUE._col1)\nkeys: KEY._col0 (type: int)\nmode: mergepartial\noutputColumnNames: _col0, _col1, _col2\nStatistics: Num rows: 1 Data size: 7020 Basic stats: COMPLETE\nColumn stats: NONE\nSelect Operator\nexpressions: _col0 (type: int), (_col1 &#x2F; _col2) (type: double)\noutputColumnNames: _col0, _col1\nStatistics: Num rows: 1 Data size: 7020 Basic stats: COMPLETE\nColumn stats: NONE\nFile Output Operator\ncompressed: false\nStatistics: Num rows: 1 Data size: 7020 Basic stats: COMPLETE\nColumn stats: NONE\ntable:\ninput format:\norg.apache.hadoop.mapred.SequenceFileInputFormat\noutput format:\norg.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat\nserde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\nStage: Stage- 0\nFetch Operator\nlimit: - 1\nProcessor Tree:\nListSink\n（ 2 ）查看详细执行计划hive (default)&gt; explain extended select * from emp;\nhive (default)&gt; explain extended select deptno, avg(sal) avg_sal from emp\ngroup by deptno;\n10. 2 Fetch 抓取Fetch抓取是指，Hive中对某些情况的查询可以不必使用MapReduce计算。例如：SELECT* FROM employees;在这种情况下，Hive可以简单地读取employee对应的存储目录下的文件，然后输出查询结果到控制台。在hive-default.xml.template文件中hive.fetch.task.conversion默认是more，老版本hive默认是minimal，该属性修改为 more以后，在全局查找、字段查找、limit查找等都不走mapreduce。—————————————————————————————————————&lt;property&gt;\n&lt;name&gt;hive.fetch.task.conversion&lt;&#x2F;name&gt;\n&lt;value&gt;more&lt;&#x2F;value&gt;\n&lt;description&gt;\nExpects one of [none, minimal, more].\nSome select queries can be converted to single FETCH task minimizing\nlatency.\nCurrently the query should be single sourced not having any subquery\nand should not have any aggregations or distincts (which incurs RS),\nlateral views and joins.\n\nnone : disable hive.fetch.task.conversion\nminimal : SELECT STAR, FILTER on partition columns, LIMIT only\nmore : SELECT, FILTER, LIMIT only (support TABLESAMPLE andvirtual columns)\n\n\n1 ）案例实操：（ 1 ）把 hive.fetch.task.conversion设置成none，然后执行查询语句，都会执行mapreduce程序。hive (default)&gt; set hive.fetch.task.conversion&#x3D;none;\nhive (default)&gt; select * from emp;\nhive (default)&gt; select ename from emp;\nhive (default)&gt; select ename from emp limit 3;\n（ 2 ）把hive.fetch.task.conversion设置成more，然后执行查询语句，如下查询方式都不会执行mapreduce程序。hive (default)&gt; set hive.fetch.task.conversion&#x3D;more;\nhive (default)&gt; select * from emp;\nhive (default)&gt; select ename from emp;\nhive (default)&gt; select ename from emp limit 3;\n10.3 本地模式大多数的Hadoop Job是需要Hadoop提供的完整的可扩展性来处理大数据集的。不过，有时Hive的输入数据量是非常小的。在这种情况下，为查询触发执行任务消耗的时间可能会比实际job的执行时间要多的多。对于大多数这种情况，Hive可以通过本地模式在单台机器上处理所有的任务。对于小数据集，执行时间可以明显被缩短。用户可以通过设置hive.exec.mode.local.auto的值为true，来让Hive在适当的时候自动启动这个优化。set hive.exec.mode.local.auto&#x3D;true; &#x2F;&#x2F;开启本地mr\n&#x2F;&#x2F;设置local mr的最大输入数据量，当输入数据量小于这个值时采用local mr的方式，默认\n为 134217728 ，即128M\nset hive.exec.mode.local.auto.inputbytes.max&#x3D;50000000;\n&#x2F;&#x2F;设置local mr的最大输入文件个数，当输入文件个数小于这个值时采用local mr的方式，默\n认为 4\nset hive.exec.mode.local.auto.input.files.max&#x3D;10;\n1 ） 案例实操：（ 2 ）关闭本地模式（默认是关闭的），并执行查询语句—————————————————————————————————————hive (default)&gt; select count(*) from emp group by deptno;\n（ 1 ）开启本地模式，并执行查询语句hive (default)&gt; set hive.exec.mode.local.auto&#x3D;true;\nhive (default)&gt; select count(*) from emp group by deptno;\n10. 4 表的优化10. 4 .1 小表大表 Join （ MapJOIN ）将key相对分散，并且数据量小的表放在join的左边，可以使用map join让小的维度表先进内存。在map端完成join。实际测试发现：新版的hive已经对小表JOIN大表和大表JOIN小表进行了优化。小表放在左边和右边已经没有区别。案例实操1 ）需求介绍测试大表JOIN小表和小表JOIN大表的效率2 ）开启 MapJoin 参数设置（ 1 ）设置自动选择Mapjoinset hive.auto.convert.join &#x3D; true; 默认为true\n（ 2 ）大表小表的阈值设置（默认25M以下认为是小表）：set hive.mapjoin.smalltable.filesize &#x3D; 25000000;\n3 ） MapJoin 工作机制MapJoin\nTask A\nMR Local\nTask\nSmall Table b\nHashTable\nFiles\nDistribute\nTask B Cache\nMapJoinTask\nmapper\nmapper\nmapper\nOutput file1\nOutput file2\nOutput file3\n1 ）TaskA，它是一个LocalTask（在客户端\n本地执行的Task），负责扫描小表b的数据，将\n其转换成一个HashTable的数据结构，并写入本\n地 的 文 件 中，之 后 将 该 文 件 加 载 到\nDistributeCache中。\n2 ）TaskB，该任务是一个没有Reduce的MR，\n启动MapTasks扫描大表a，在Map阶段，根据a的\n每一条记录去和DistributeCache中b表对应的\nHashTable关联，并直接输出结果。\n3 ）由于MapJoin没有Reduce，所以由Map直\n接输出结果文件，有多少个MapTask，就有多\n少个结果文件。\nload\nTable a\n4 ）建大表、小表和 JOIN 后表的语句&#x2F;&#x2F; 创建大表\n\n—————————————————————————————————————create table bigtable(id bigint, t bigint, uid string, keyword string,\nurl_rank int, click_num int, click_url string) row format delimited\nfields terminated by &#39;\\t&#39;;\n&#x2F;&#x2F; 创建小表\ncreate table smalltable(id bigint, t bigint, uid string, keyword string,\nurl_rank int, click_num int, click_url string) row format delimited\nfields terminated by &#39;\\t&#39;;\n&#x2F;&#x2F; 创建join后表的语句\ncreate table jointable(id bigint, t bigint, uid string, keyword string,\nurl_rank int, click_num int, click_url string) row format delimited\nfields terminated by &#39;\\t&#39;;\n5 ）分别向大表和小表中导入数据hive (default)&gt; load data local inpath &#39;&#x2F;opt&#x2F;module&#x2F;data&#x2F;bigtable&#39; into\ntable bigtable;\nhive (default)&gt;load data local inpath &#39;&#x2F;opt&#x2F;module&#x2F;data&#x2F;smalltable&#39; into\ntable smalltable;\n6 ）小表 JOIN 大表语句insert overwrite table jointable\nselect b.id, b.t, b.uid, b.keyword, b.url_rank, b.click_num, b.click_url\nfrom smalltable s\njoin bigtable b\non b.id &#x3D; s.id;\n7 ）大表 JOIN 小表语句insert overwrite table jointable\nselect b.id, b.t, b.uid, b.keyword, b.url_rank, b.click_num, b.click_url\nfrom bigtable b\njoin smalltable s\non s.id &#x3D; b.id;\n10.4.2 大表 Join 大表1 ）空 KEY 过滤有时join超时是因为某些key对应的数据太多，而相同key对应的数据都会发送到相同的reducer上，从而导致内存不够。此时我们应该仔细分析这些异常的key，很多情况下，这些key对应的数据是异常数据，我们需要在SQL语句中进行过滤。例如key对应的字段为空，操作如下：案例实操（ 1 ）配置历史服务器配置mapred-site.xml&lt;property&gt;\n&lt;name&gt;mapreduce.jobhistory.address&lt;&#x2F;name&gt;\n&lt;value&gt;hadoop102:10020&lt;&#x2F;value&gt;\n&lt;&#x2F;property&gt;\n&lt;property&gt;\n&lt;name&gt;mapreduce.jobhistory.webapp.address&lt;&#x2F;name&gt;\n\n—————————————————————————————————————&lt;value&gt;hadoop102:19888&lt;&#x2F;value&gt;\n&lt;&#x2F;property&gt;\n启动历史服务器sbin&#x2F;mr-jobhistory-daemon.sh start historyserver\n查看jobhistoryhttp://hadoop102:19888/jobhistory（ 2 ）创建原始数据空id表&#x2F;&#x2F; 创建空id表\ncreate table nullidtable(id bigint, t bigint, uid string, keyword string,\nurl_rank int, click_num int, click_url string) row format delimited\nfields terminated by &#39;\\t&#39;;\n（ 3 ）分别加载原始数据和空id数据到对应表中hive (default)&gt; load data local inpath &#39;&#x2F;opt&#x2F;module&#x2F;data&#x2F;nullid&#39; into\ntable nullidtable;\n（ 4 ）测试不过滤空idhive (default)&gt; insert overwrite table jointable select n.* from\nnullidtable n left join bigtable o on n.id &#x3D; o.id;\n（ 5 ）测试过滤空idhive (default)&gt; insert overwrite table jointable select n.* from (select\n* from nullidtable where id is not null) n left join bigtable o on n.id &#x3D;\no.id;\n2 ）空 key 转换有时虽然某个key为空对应的数据很多，但是相应的数据不是异常数据，必须要包含在join的结果中，此时我们可以表a中key为空的字段赋一个随机的值，使得数据随机均匀地分不到不同的reducer上。例如：案例实操：不随机分布空null值：（ 1 ）设置 5 个reduce个数set mapreduce.job.reduces &#x3D; 5;（ 2 ）JOIN两张表insert overwrite table jointable\nselect n.* from nullidtable n left join bigtable b on n.id &#x3D; b.id;\n结果：如下图所示，可以看出来，出现了数据倾斜，某些reducer的资源消耗远大于其他reducer。—————————————————————————————————————随机分布空null值（ 1 ）设置 5 个reduce个数set mapreduce.job.reduces &#x3D; 5;\n（ 2 ）JOIN两张表insert overwrite table jointable\nselect n.* from nullidtable n full join bigtable o on\nnvl(n.id,rand()) &#x3D; o.id;\n结果：如下图所示，可以看出来，消除了数据倾斜，负载均衡reducer的资源消耗3 ） SMB(Sort Merge Bucket join)（ 1 ）创建第二张大表create table bigtable 2 (\nid bigint,\nt bigint,\nuid string,\nkeyword string,\nurl_rank int,\nclick_num int,\nclick_url string)\nrow format delimited fields terminated by &#39;\\t&#39;;\nload data local inpath &#39;&#x2F;opt&#x2F;module&#x2F;data&#x2F;bigtable&#39; into table bigtable 2 ;\n\n—————————————————————————————————————测试大表直接JOINinsert overwrite table jointable\nselect b.id, b.t, b.uid, b.keyword, b.url_rank, b.click_num, b.click_url\nfrom bigtable s\njoin bigtable 2 b\non b.id &#x3D; s.id;\n（ 2 ）创建分通表 1 ,桶的个数不要超过可用CPU的核数create table bigtable_buck1(\nid bigint,\nt bigint,\nuid string,\nkeyword string,\nurl_rank int,\nclick_num int,\nclick_url string)\nclustered by(id)\nsorted by(id)\ninto 6 buckets\nrow format delimited fields terminated by &#39;\\t&#39;;\nload data local inpath &#39;&#x2F;opt&#x2F;module&#x2F;data&#x2F;bigtable&#39; into table\nbigtable_buck1;\n（ 3 ）创建分通表 2 ,桶的个数不要超过可用CPU的核数create table bigtable_buck2(\nid bigint,\nt bigint,\nuid string,\nkeyword string,\nurl_rank int,\nclick_num int,\nclick_url string)\nclustered by(id)\nsorted by(id)\ninto 6 buckets\nrow format delimited fields terminated by &#39;\\t&#39;;\nload data local inpath &#39;&#x2F;opt&#x2F;module&#x2F;data&#x2F;bigtable&#39; into table\nbigtable_buck2;\n（ 4 ）设置参数set hive.optimize.bucketmapjoin &#x3D; true;\nset hive.optimize.bucketmapjoin.sortedmerge &#x3D; true;\nset\nhive.input.format&#x3D;org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat;\n（ 5 ）测试insert overwrite table jointable\nselect b.id, b.t, b.uid, b.keyword, b.url_rank, b.click_num, b.click_url\nfrom bigtable_buck1 s\njoin bigtable_buck2 b\non b.id &#x3D; s.id;\n10.4.3 Group By默认情况下，Map阶段同一Key数据分发给一个reduce，当一个key数据过大时就倾斜了。—————————————————————————————————————并不是所有的聚合操作都需要在Reduce端完成，很多聚合操作都可以先在Map端进行部分聚合，最后在Reduce端得出最终结果。1 ）开启 Map 端聚合参数设置（ 1 ）是否在Map端进行聚合，默认为Trueset hive.map.aggr &#x3D; true\n（ 2 ）在Map端进行聚合操作的条目数目set hive.groupby.mapaggr.checkinterval &#x3D; 100000\n（ 3 ）有数据倾斜的时候进行负载均衡（默认是false）set hive.groupby.skewindata &#x3D; true\n当选项设定为 true，生成的查询计划会有两个MR Job。第一个MR Job中，Map的输出结果会随机分布到Reduce中，每个Reduce做部分聚合操作，并输出结果，这样处理的结果是相同的Group By Key有可能被分发到不同的Reduce中，从而达到负载均衡的目的；第二个MR Job再根据预处理的数据结果按照Group By Key分布到Reduce中（这个过程可以保证相同的Group By Key被分布到同一个Reduce中），最后完成最终的聚合操作。hive (default)&gt; select deptno from emp group by deptno;\nStage-Stage-1: Map: 1 Reduce: 5 Cumulative CPU: 23.68 sec HDFS Read:\n19987 HDFS Write: 9 SUCCESS\nTotal MapReduce CPU Time Spent: 23 seconds 680 msec\nOK\ndeptno\n10\n20\n30\n优化以后hive (default)&gt; set hive.groupby.skewindata &#x3D; true;\nhive (default)&gt; select deptno from emp group by deptno;\nStage-Stage-1: Map: 1 Reduce: 5 Cumulative CPU: 28.53 sec HDFS Read:\n18209 HDFS Write: 534 SUCCESS\nStage-Stage-2: Map: 1 Reduce: 5 Cumulative CPU: 38.32 sec HDFS Read:\n15014 HDFS Write: 9 SUCCESS\nTotal MapReduce CPU Time Spent: 1 minutes 6 seconds 850 msec\nOK\ndeptno\n10\n20\n30\n\n—————————————————————————————————————10.4. 4 Count(Distinct) 去重统计数据量小的时候无所谓，数据量大的情况下，由于 COUNT DISTINCT操作需要用一个Reduce Task来完成，这一个Reduce需要处理的数据量太大，就会导致整个Job很难完成，一般COUNT DISTINCT使用先GROUP BY再COUNT的方式替换,但是需要注意group by造成的数据倾斜问题.1 ） 案例实操（ 1 ）创建一张大表hive (default)&gt; create table bigtable(id bigint, time bigint, uid string,\nkeyword\nstring, url_rank int, click_num int, click_url string) row format\ndelimited\nfields terminated by &#39;\\t&#39;;\n（ 2 ）加载数据hive (default)&gt; load data local inpath &#39;&#x2F;opt&#x2F;module&#x2F;data&#x2F;bigtable&#39; into\ntable bigtable;\n（ 3 ）设置 5 个reduce个数set mapreduce.job.reduces &#x3D; 5;\n（ 4 ）执行去重id查询hive (default)&gt; select count(distinct id) from bigtable;\nStage-Stage-1: Map: 1 Reduce: 1 Cumulative CPU: 7.12 sec HDFS Read:\n120741990 HDFS Write: 7 SUCCESS\nTotal MapReduce CPU Time Spent: 7 seconds 120 msec\nOK\nc0\n100001\nTime taken: 23.607 seconds, Fetched: 1 row(s)\n（ 5 ）采用GROUP by去重idhive (default)&gt; select count(id) from (select id from bigtable group by\nid) a;\nStage-Stage-1: Map: 1 Reduce: 5 Cumulative CPU: 17.53 sec HDFS Read:\n120752703 HDFS Write: 580 SUCCESS\nStage-Stage-2: Map: 1 Reduce: 1 Cumulative CPU: 4.29 sec2 HDFS Read:\n9409 HDFS Write: 7 SUCCESS\nTotal MapReduce CPU Time Spent: 21 seconds 820 msec\nOK\n_c0\n100001\nTime taken: 50.795 seconds, Fetched: 1 row(s)\n虽然会多用一个Job来完成，但在数据量大的情况下，这个绝对是值得的。10.4. 5 笛卡尔积尽量避免笛卡尔积，join的时候不加on条件，或者无效的on条件，Hive只能使用 1 个reducer来完成笛卡尔积。—————————————————————————————————————10.4. 6 行列过滤列处理：在SELECT中，只拿需要的列，如果有分区，尽量使用分区过滤，少用SELECT*。行处理：在分区剪裁中，当使用外关联时，如果将副表的过滤条件写在Where后面，那么就会先全表关联，之后再过滤，比如：案例实操：1 ）测试先关联两张表，再用 where 条件过滤hive (default)&gt; select o.id from bigtable b\njoin bigtable o on o.id &#x3D; b.id\nwhere o.id &lt;&#x3D; 10;\nTime taken: 34.406 seconds, Fetched: 100 row(s)2 ）通过子查询后，再关联表hive (default)&gt; select b.id from bigtable b\njoin (select id from bigtable where id &lt;&#x3D; 10) o on b.id &#x3D; o.id;\nTime taken: 30.058 seconds, Fetched: 100 row(s)10.4. 7 分区详见7.1章。10.4. 8 分桶详见 7 .2章。10. 5 合理设置 Map 及 Reduce 数1 ）通常情况下，作业会通过input的目录产生一个或者多个map任务。主要的决定因素有：input的文件总个数，input的文件大小，集群设置的文件块大小。2 ）是不是map数越多越好？答案是否定的。如果一个任务有很多小文件（远远小于块大小128m），则每个小文件也会被当做一个块，用一个map任务来完成，而一个map任务启动和初始化的时间远远大于逻辑处理的时间，就会造成很大的资源浪费。而且，同时可执行的map数是受限的。3 ）是不是保证每个map处理接近128m的文件块，就高枕无忧了？答案也是不一定。比如有一个127m的文件，正常会用一个map去完成，但这个文件只有一个或者两个小字段，却有几千万的记录，如果map处理的逻辑比较复杂，用一个map任务去做，肯定也比较耗时。—————————————————————————————————————针对上面的问题 2 和 3 ，我们需要采取两种方式来解决：即减少map数和增加map数；10. 5 .1 复杂文件增加 Map 数当input的文件都很大，任务逻辑复杂，map执行非常慢的时候，可以考虑增加Map数，来使得每个map处理的数据量减少，从而提高任务的执行效率。增加map的方法为：根据computeSliteSize(Math.max(minSize,Math.min(maxSize,blocksize)))&#x3D;blocksize&#x3D;128M 公式，调整maxSize最大值。让maxSize最大值低于blocksize就可以增加map的个数。案例实操：1 ）执行查询hive (default)&gt; select count(*) from emp;\nHadoop job information for Stage-1: number of mappers: 1; number of\nreducers: 1\n2 ）设置最大切片值为 100 个字节hive (default)&gt; set mapreduce.input.fileinputformat.split.maxsize&#x3D;100;\nhive (default)&gt; select count(*) from emp;\nHadoop job information for Stage-1: number of mappers: 6; number of\nreducers: 1\n10. 5 .2 小文件进行合并1 ）在map执行前合并小文件，减少map数：CombineHiveInputFormat具有对小文件进行合并的功能（系统默认的格式）。HiveInputFormat没有对小文件合并功能。set hive.input.format&#x3D;\norg.apache.hadoop.hive.ql.io.CombineHiveInputFormat;\n2 ）在Map-Reduce的任务结束时合并小文件的设置：在map-only任务结束时合并小文件，默认trueSET hive.merge.mapfiles &#x3D; true;\n在map-reduce任务结束时合并小文件，默认falseSET hive.merge.mapredfiles &#x3D; true;\n合并文件的大小，默认256MSET hive.merge.size.per.task &#x3D; 268435456;\n当输出文件的平均大小小于该值时，启动一个独立的map-reduce任务进行文件mergeSET hive.merge.smallfiles.avgsize &#x3D; 16777216;\n10. 5 .3 合理设置 Reduce 数1 ）调整 reduce 个数方法一（ 1 ）每个Reduce处理的数据量默认是256MBhive.exec.reducers.bytes.per.reducer&#x3D;256000000\n\n—————————————————————————————————————（ 2 ）每个任务最大的reduce数，默认为 1009hive.exec.reducers.max&#x3D;1009\n（ 3 ）计算reducer数的公式N&#x3D;min(参数 2 ，总输入数据量&#x2F;参数1)\n2 ）调整 reduce 个数方法二在hadoop的mapred-default.xml文件中修改设置每个job的Reduce个数set mapreduce.job.reduces &#x3D; 15;\n3 ） reduce 个数并不是越多越好（ 1 ）过多的启动和初始化reduce也会消耗时间和资源；（ 2 ）另外，有多少个reduce，就会有多少个输出文件，如果生成了很多个小文件，那么如果这些小文件作为下一个任务的输入，则也会出现小文件过多的问题；在设置reduce个数的时候也需要考虑这两个原则：处理大数据量利用合适的reduce数；使单个reduce任务处理数据量大小要合适；10. 6 并行执行Hive会将一个查询转化成一个或者多个阶段。这样的阶段可以是MapReduce阶段、抽样阶段、合并阶段、limit阶段。或者Hive执行过程中可能需要的其他阶段。默认情况下，Hive一次只会执行一个阶段。不过，某个特定的job可能包含众多的阶段，而这些阶段可能并非完全互相依赖的，也就是说有些阶段是可以并行执行的，这样可能使得整个job的执行时间缩短。不过，如果有更多的阶段可以并行执行，那么job可能就越快完成。通过设置参数hive.exec.parallel值为true，就可以开启并发执行。不过，在共享集群中，需要注意下，如果job中并行阶段增多，那么集群利用率就会增加。set hive.exec.parallel&#x3D;true; &#x2F;&#x2F;打开任务并行执行\nset hive.exec.parallel.thread.number&#x3D;16; &#x2F;&#x2F;同一个sql允许最大并行度，默认为\n8 。\n当然，得是在系统资源比较空闲的时候才有优势，否则，没资源，并行也起不来。10. 7 严格模式Hive可以通过设置防止一些危险操作：1 ）分区表不使用分区过滤将hive.strict.checks.no.partition.filter设置为true时，对于分区表，除非where语句中含有分区字段过滤条件来限制范围，否则不允许执行。换句话说，就是用户不允许扫描所有分—————————————————————————————————————区。进行这个限制的原因是，通常分区表都拥有非常大的数据集，而且数据增加迅速。没有进行分区限制的查询可能会消耗令人不可接受的巨大资源来处理这个表。2 ）使用 order by 没有 limit 过滤将hive.strict.checks.orderby.no.limit设置为true时，对于使用了order by语句的查询，要求必须使用limit语句。因为order by为了执行排序过程会将所有的结果数据分发到同一个Reducer中进行处理，强制要求用户增加这个LIMIT语句可以防止Reducer额外执行很长一段时间。3 ）笛卡尔积将hive.strict.checks.cartesian.product设置为true时，会限制笛卡尔积的查询。对关系型数据库非常了解的用户可能期望在 执行JOIN查询的时候不使用ON语句而是使用where语句，这样关系数据库的执行优化器就可以高效地将WHERE语句转化成那个ON语句。不幸的是，Hive并不会执行这种优化，因此，如果表足够大，那么这个查询就会出现不可控的情况。10. 8 JVM 重用详见hadoop优化文档中jvm重用10. 9 压缩详见第 9 章。第 11 章 Hive实战11 .1 需求描述统计硅谷影音视频网站的常规指标，各种TopN指标：– 统计视频观看数Top 10– 统计视频类别热度Top 10– 统计出视频观看数最高的 20 个视频的所属类别以及类别包含Top20视频的个数– 统计视频观看数Top 50 所关联视频的所属类别排序– 统计每个类别中的视频热度Top10,以Music为例– 统计每个类别视频观看数Top 10\n\n统计上传视频最多的用户Top 10 以及他们上传的视频观看次数在前 20 的视频\n\n\n\n—————————————————————————————————————11 .2 数据结构1 ）视频表视频表字段 备注 详细描述\nvideoId 视频唯一id（String） 11 位字符串\nuploader 视频上传者（String） 上传视频的用户名String\nage 视频年龄（int） 视频在平台上的整数天\ncategory 视频类别（Array&lt;String&gt;） 上传视频指定的视频分类\nlength (^) 视频长度（Int） 整形数字标识的视频长度views 观看次数（Int） 视频被浏览的次数rate 视频评分（Double） 满分 5 分Ratings 流量（Int） 视频的流量，整型数字conments 评论数（Int） 一个视频的整数评论数relatedId 相关视频id（Array） 相关视频的id，最多 20 个\n2 ）用户表用户表字段 备注 字段类型\nuploader 上传者用户名 string\nvideos (^) 上传视频数 intfriends (^) 朋友数量 int\n11 .3 准备工作11 .3. 1 准备表1 ）需要准备的表创建原始数据表：gulivideo_ori，gulivideo_user_ori，创建最终表：gulivideo_orc，gulivideo_user_orc2 ）创建原始数据表：（ 1 ）gulivideo_oricreate table gulivideo_ori(\n\n—————————————————————————————————————videoId string,\nuploader string,\nage int,\ncategory array&lt;string&gt;,\nlength int,\nviews int,\nrate float,\nratings int,\ncomments int,\nrelatedId array&lt;string&gt;)\nrow format delimited fields terminated by &quot;\\t&quot;\ncollection items terminated by &quot;&amp;&quot;\nstored as textfile;\n（ 2 ）创建原始数据表: gulivideo_user_oricreate table gulivideo_user_ori(\nuploader string,\nvideos int,\nfriends int)\nrow format delimited\nfields terminated by &quot;\\t&quot;\nstored as textfile;\n2 ） 创建 orc 存储格式带 snappy 压缩的表：（ 1 ）gulivideo_orccreate table gulivideo_orc(\nvideoId string,\nuploader string,\nage int,\ncategory array&lt;string&gt;,\nlength int,\nviews int,\nrate float,\nratings int,\ncomments int,\nrelatedId array&lt;string&gt;)\nstored as orc\ntblproperties(&quot;orc.compress&quot;&#x3D;&quot;SNAPPY&quot;);\n（ 2 ）gulivideo_user_orccreate table gulivideo_user_orc(\nuploader string,\nvideos int,\nfriends int)\nrow format delimited\nfields terminated by &quot;\\t&quot;\nstored as orc\ntblproperties(&quot;orc.compress&quot;&#x3D;&quot;SNAPPY&quot;);\n（ 3 ）向ori表插入数据load data local inpath &quot;&#x2F;opt&#x2F;module&#x2F;data&#x2F;video&quot; into table gulivideo_ori;\nload data local inpath &quot;&#x2F;opt&#x2F;module&#x2F;user&quot; into table gulivideo_user_ori;\n（ 4 ）向orc表插入数据insert into table gulivideo_orc select * from gulivideo_ori;\ninsert into table gulivideo_user_orc select * from gulivideo_user_ori;\n\n—————————————————————————————————————11.3. 2 安装 Tez 引擎（了解）Tez是一个Hive的运行引擎，性能优于MR。为什么优于MR呢？看下。用Hive直接编写MR程序，假设有四个有依赖关系的MR作业，上图中，绿色是ReduceTask，云状表示写屏蔽，需要将中间结果持久化写到HDFS。Tez可以将多个有依赖的作业转换为一个作业，这样只需写一次HDFS，且中间节点较少，从而大大提升作业的计算性能。1 ）将 tez 安装包拷贝到集群，并解压 tar 包[atguigu@hadoop102 software]$ mkdir &#x2F;opt&#x2F;module&#x2F;tez\n[atguigu@hadoop102 software]$ tar -zxvf &#x2F;opt&#x2F;software&#x2F;tez-0.10. 1 -\nSNAPSHOT-minimal.tar.gz - C &#x2F;opt&#x2F;module&#x2F;tez\n2 ）上传 tez 依赖到 HDFS[atguigu@hadoop102 software]$ hadoop fs -mkdir &#x2F;tez\n[atguigu@hadoop102 software]$ hadoop fs -put &#x2F;opt&#x2F;software&#x2F;tez-0.10.1-\nSNAPSHOT.tar.gz &#x2F;tez\n3 ）新建 tez-site.xml[atguigu@hadoop102 software]$ vim $HADOOP_HOME&#x2F;etc&#x2F;hadoop&#x2F;tez-site.xml\n添加如下内容：&lt;?xml version&#x3D;&quot;1.0&quot; encoding&#x3D;&quot;UTF-8&quot;?&gt;\n&lt;?xml-stylesheet type&#x3D;&quot;text&#x2F;xsl&quot; href&#x3D;&quot;configuration.xsl&quot;?&gt;\n&lt;configuration&gt;\n&lt;property&gt;\n&lt;name&gt;tez.lib.uris&lt;&#x2F;name&gt;\n&lt;value&gt;$&#123;fs.defaultFS&#125;&#x2F;tez&#x2F;tez-0.10.1-SNAPSHOT.tar.gz&lt;&#x2F;value&gt;\n&lt;&#x2F;property&gt;\n&lt;property&gt;\n&lt;name&gt;tez.use.cluster.hadoop-libs&lt;&#x2F;name&gt;\n&lt;value&gt;true&lt;&#x2F;value&gt;\n&lt;&#x2F;property&gt;\n&lt;property&gt;\n&lt;name&gt;tez.am.resource.memory.mb&lt;&#x2F;name&gt;\n&lt;value&gt;1024&lt;&#x2F;value&gt;\n\n—————————————————————————————————————&lt;&#x2F;property&gt;\n&lt;property&gt;\n&lt;name&gt;tez.am.resource.cpu.vcores&lt;&#x2F;name&gt;\n&lt;value&gt;1&lt;&#x2F;value&gt;\n&lt;&#x2F;property&gt;\n&lt;property&gt;\n&lt;name&gt;tez.container.max.java.heap.fraction&lt;&#x2F;name&gt;\n&lt;value&gt;0.4&lt;&#x2F;value&gt;\n&lt;&#x2F;property&gt;\n&lt;property&gt;\n&lt;name&gt;tez.task.resource.memory.mb&lt;&#x2F;name&gt;\n&lt;value&gt;1024&lt;&#x2F;value&gt;\n&lt;&#x2F;property&gt;\n&lt;property&gt;\n&lt;name&gt;tez.task.resource.cpu.vcores&lt;&#x2F;name&gt;\n&lt;value&gt;1&lt;&#x2F;value&gt;\n&lt;&#x2F;property&gt;\n&lt;&#x2F;configuration&gt;\n4 ）修改 Hadoop 环境变量[atguigu@hadoop102 software]$ vim\n$HADOOP_HOME&#x2F;etc&#x2F;hadoop&#x2F;shellprofile.d&#x2F;tez.sh\n添加Tez的Jar包相关信息hadoop_add_profile tez\nfunction _tez_hadoop_classpath\n&#123;\nhadoop_add_classpath &quot;$HADOOP_HOME&#x2F;etc&#x2F;hadoop&quot; after\nhadoop_add_classpath &quot;&#x2F;opt&#x2F;module&#x2F;tez&#x2F;*&quot; after\nhadoop_add_classpath &quot;&#x2F;opt&#x2F;module&#x2F;tez&#x2F;lib&#x2F;*&quot; after\n&#125;\n5 ）修改 Hive 的计算引擎[atguigu@hadoop102 software]$ vim $HIVE_HOME&#x2F;conf&#x2F;hive-site.xml\n添加&lt;property&gt;\n&lt;name&gt;hive.execution.engine&lt;&#x2F;name&gt;\n&lt;value&gt;tez&lt;&#x2F;value&gt;\n&lt;&#x2F;property&gt;\n&lt;property&gt;\n&lt;name&gt;hive.tez.container.size&lt;&#x2F;name&gt;\n&lt;value&gt;1024&lt;&#x2F;value&gt;\n&lt;&#x2F;property&gt;\n6 ）解决日志 Jar 包冲突[atguigu@hadoop102 software]$ rm &#x2F;opt&#x2F;module&#x2F;tez&#x2F;lib&#x2F;slf4j-log4j12-\n1.7.10.jar\n11 .4 业务分析11 .4.1 统计视频观看数 Top10思路：使用order by按照views字段做一个全局排序即可，同时我们设置只显示前 10条。最终代码：—————————————————————————————————————SELECT\nvideoId,\nviews\nFROM\ngulivideo_orc\nORDER BY\nviews DESC\nLIMIT 10;\n11 .4.2 统计视频类别热度 Top10思路：（ 1 ）即统计每个类别有多少个视频，显示出包含视频最多的前 10 个类别。（ 2 ）我们需要按照类别group by聚合，然后count组内的videoId个数即可。（ 3 ）因为当前表结构为：一个视频对应一个或多个类别。所以如果要group by类别，需要先将类别进行列转行(展开)，然后再进行count即可。（ 4 ）最后按照热度排序，显示前 10 条。最终代码：SELECT\nt1.category_name ,\nCOUNT(t1.videoId) hot\nFROM\n(\nSELECT\nvideoId,\ncategory_name\nFROM\ngulivideo_orc\nlateral VIEW explode(category) gulivideo_orc_tmp AS category_name\n) t1\nGROUP BY\nt1.category_name\nORDER BY\nhot\nDESC\nLIMIT 10\n11 .4.3 统计出视频观看数最高的 20 个视频的所属类别以及类别包含Top20 视频的个数思路：（ 1 ）先找到观看数最高的 20 个视频所属条目的所有信息，降序排列（ 2 ）把这 20 条信息中的category分裂出来(列转行)（ 3 ）最后查询视频分类名称和该分类下有多少个Top20的视频最终代码：SELECT\n\n—————————————————————————————————————t2.category_name,\nCOUNT(t2.videoId) video_sum\nFROM\n(\nSELECT\nt1.videoId,\ncategory_name\nFROM\n(\nSELECT\nvideoId,\nviews ,\ncategory\nFROM\ngulivideo_orc\nORDER BY\nviews\nDESC\nLIMIT 20\n) t1\nlateral VIEW explode(t1.category) t1_tmp AS category_name\n) t2\nGROUP BY t2.category_name\n11 .4.4 统计视频观看数 Top50 所关联视频的所属类别排序代码：SELECT\nt6.category_name,\nt6.video_sum,\nrank() over(ORDER BY t6.video_sum DESC ) rk\nFROM\n(\nSELECT\nt5.category_name,\nCOUNT(t5.relatedid_id) video_sum\nFROM\n(\nSELECT\nt4.relatedid_id,\ncategory_name\nFROM\n(\nSELECT\nt2.relatedid_id ,\nt3.category\nFROM\n(\nSELECT\nrelatedid_id\nFROM\n(\nSELECT\nvideoId,\nviews,\nrelatedid\nFROM\ngulivideo_orc\n\n—————————————————————————————————————ORDER BY\nviews\nDESC\nLIMIT 50\n)t1\nlateral VIEW explode(t1.relatedid) t1_tmp AS relatedid_id\n)t2\nJOIN\ngulivideo_orc t3\nON\nt2.relatedid_id &#x3D; t3.videoId\n) t4\nlateral VIEW explode(t4.category) t4_tmp AS category_name\n) t5\nGROUP BY\nt5.category_name\nORDER BY\nvideo_sum\nDESC\n) t6\n11 .4.5 统计每个类别中的视频热度 Top10 ，以 Music 为例思路：（ 1 ）要想统计Music类别中的视频热度Top 10 ，需要先找到Music类别，那么就需要将category展开，所以可以创建一张表用于存放categoryId展开的数据。（ 2 ）向category展开的表中插入数据。（ 3 ）统计对应类别（Music）中的视频热度。统计Music类别的Top 10 （也可以统计其他）SELECT\nt1.videoId,\nt1.views,\nt1.category_name\nFROM\n(\nSELECT\nvideoId,\nviews,\ncategory_name\nFROM gulivideo_orc\nlateral VIEW explode(category) gulivideo_orc_tmp AS category_name\n)t1\nWHERE\nt1.category_name &#x3D; &quot;Music&quot;\nORDER BY\nt1.views\nDESC\nLIMIT 10\n11 .4.6 统计每个类别视频观看数 Top10最终代码：—————————————————————————————————————SELECT\nt2.videoId,\nt2.views,\nt2.category_name,\nt2.rk\nFROM\n(\nSELECT\nt1.videoId,\nt1.views,\nt1.category_name,\nrank() over(PARTITION BY t1.category_name ORDER BY t1.views DESC ) rk\nFROM\n(\nSELECT\nvideoId,\nviews,\ncategory_name\nFROM gulivideo_orc\nlateral VIEW explode(category) gulivideo_orc_tmp AS category_name\n)t1\n)t2\nWHERE t2.rk &lt;&#x3D;10\n11 .4.7 统计上传视频最多的用户 Top 10 以及他们上传的视频观看次数在前 20 的视频思路：（ 1 ）求出上传视频最多的 10 个用户（ 2 ）关联gulivideo_orc表，求出这 10 个用户上传的所有的视频，按照观看数取前 20最终代码:SELECT\nt2.videoId,\nt2.views,\nt2.uploader\nFROM\n(\nSELECT\nuploader,\nvideos\nFROM gulivideo_user_orc\nORDER BY\nvideos\nDESC\nLIMIT 10\n) t1\nJOIN gulivideo_orc t2\nON t1.uploader &#x3D; t2.uploader\nORDER BY\nt2.views\nDESC\n\n—————————————————————————————————————LIMIT 20\n\n\n\n\n\n\n附录：常见错误及解决方案0 ） 如果更换 Tez 引擎后，执行任务卡住，可以尝试调节容量调度器的资源调度策略将$HADOOP_HOME&#x2F;etc&#x2F;hadoop&#x2F;capacity-scheduler.xml文件中的&lt;property&gt;\n&lt;name&gt;yarn.scheduler.capacity.maximum-am-resource-percent&lt;&#x2F;name&gt;\n&lt;value&gt;0. 1 &lt;&#x2F;value&gt;\n&lt;description&gt;\nMaximum percent of resources in the cluster which can be used to run\napplication masters i.e. controls number of concurrent running\napplications.\n&lt;&#x2F;description&gt;\n&lt;&#x2F;property&gt;\n改成&lt;property&gt;\n&lt;name&gt;yarn.scheduler.capacity.maximum-am-resource-percent&lt;&#x2F;name&gt;\n&lt;value&gt; 1 &lt;&#x2F;value&gt;\n&lt;description&gt;\nMaximum percent of resources in the cluster which can be used to run\napplication masters i.e. controls number of concurrent running\napplications.\n&lt;&#x2F;description&gt;\n&lt;&#x2F;property&gt;\n1 ）连接不上 mysql 数据库（ 1 ）导错驱动包，应该把mysql-connector-java-5.1.27-bin.jar导入&#x2F;opt&#x2F;module&#x2F;hive&#x2F;lib的不是这个包。错把mysql-connector-java-5.1.27.tar.gz导入hive&#x2F;lib包下。（ 2 ）修改user表中的主机名称没有都修改为%，而是修改为localhost2 ） hive 默认的输入格式处理是 CombineHiveInputFormat ，会对小文件进行合并。hive (default)&gt; set hive.input.format;\nhive.input.format&#x3D;org.apache.hadoop.hive.ql.io.CombineHiveInputFormat\n可以采用HiveInputFormat就会根据分区数输出相应的文件。hive (default)&gt; set\nhive.input.format&#x3D;org.apache.hadoop.hive.ql.io.HiveInputFormat;\n3 ）不能执行 mapreduce 程序可能是hadoop的yarn没开启。4 ）启动 mysql 服务时，报 MySQL server PID file could not be found! 异常。在&#x2F;var&#x2F;lock&#x2F;subsys&#x2F;mysql路径下创建hadoop102.pid，并在文件中添加内容： 43965 ）报 service mysql status MySQL is not running, but lock file (&#x2F;var&#x2F;lock&#x2F;subsys&#x2F;mysql[ 失败 ]) 异常。—————————————————————————————————————解决方案：在&#x2F;var&#x2F;lib&#x2F;mysql 目录下创建： - rw-rw—-. 1 mysql mysql 5 12月 2216:41 hadoop102.pid 文件，并修改权限为 777 。6 ） JVM 堆内存溢出描述：java.lang.OutOfMemoryError: Java heap space解决：在yarn-site.xml中加入如下代码&lt;property&gt;\n&lt;name&gt;yarn.scheduler.maximum-allocation-mb&lt;&#x2F;name&gt;\n&lt;value&gt;2048&lt;&#x2F;value&gt;\n&lt;&#x2F;property&gt;\n&lt;property&gt;\n&lt;name&gt;yarn.scheduler.minimum-allocation-mb&lt;&#x2F;name&gt;\n&lt;value&gt;2048&lt;&#x2F;value&gt;\n&lt;&#x2F;property&gt;\n&lt;property&gt;\n&lt;name&gt;yarn.nodemanager.vmem-pmem-ratio&lt;&#x2F;name&gt;\n&lt;value&gt;2.1&lt;&#x2F;value&gt;\n&lt;&#x2F;property&gt;\n&lt;property&gt;\n&lt;name&gt;mapred.child.java.opts&lt;&#x2F;name&gt;\n&lt;value&gt;-Xmx1024m&lt;&#x2F;value&gt;\n&lt;&#x2F;property&gt;\n7 ）虚拟内存限制在yarn-site.xml中添加如下配置:&lt;property&gt;\n&lt;name&gt;yarn.nodemanager.vmem-check-enabled&lt;&#x2F;name&gt;\n&lt;value&gt;false&lt;&#x2F;value&gt;\n&lt;&#x2F;property&gt;\n\nselect\n\nfrom\n\n\norder by 字段1 [asc|desc],[字段2];--同样的字段1再按字段2排序\n","slug":"Hive","date":"2022-03-31T13:03:51.088Z","categories_index":"","tags_index":"大数据","author_index":"Cencus"},{"id":"668b17fd878754e2652b25e78c8ab4f7","title":"flume","content":"我们不生产数据我们只是数据的搬运工\n\n\nyum的配置三种source\nexec：不支持断点续传\nspoolingdirector：不支持动态更新\ntaildirsource：解决上述两点，存在的问题重复上传&#x3D;&gt;不能更名，如果非要更名，只有改源码\n\n改源码\n\ntailfile里的\nupdate()&#123;\n\n&#125;\n\nreliabletaildireventreader里的 \nif(tf==null || !tf.getPath.equals(f.getAbsolutePath()))\n下源码下载src就可以\n\n\nflume agent架构\n\n\n\n\n\n\n\n\nsource-&gt;channel-&gt;sink的细节版\n第三章 flueme进阶source类型\n\nexec\nnetcat\ntaildir\nAvro\n\nchannel策略：Replicating（复制）和 Multiplexing（多路复用）。\nsink\n\navro\nhdfs\nFilerow\nlogger\n\n3.4 flume企业开发案例3.4.1 复制和多路复用需求：\n\n\n\n\n\n\n\n\n\n端口号有web端口和rpc端口等等\n重要的只有对各个对象的配置\n3.4.2 负载均衡和故障转移有感觉到如果连续发就是同一个来接受，这样的话资源开销感觉应该小一些，测试得到确实如此，如果是连续发送（间隔小于一个较小的时间），就会进入同一个服务端，如果发送的间隔较长（5s左右），然后就会轮流来。\n3.4.1 聚合Multiplexing Channel Selector\n同名上传并不会覆盖，而是会在源文件后面加一个后缀.n\n自定义拦截器（加工器）需求：\npackage com.atguigu.interceptor;\n\nimport org.apache.flume.Context;\nimport org.apache.flume.Event;\nimport org.apache.flume.interceptor.Interceptor;\n\nimport java.util.ArrayList;\nimport java.util.List;\nimport java.util.Map;\n\npublic class TypeInterceptor implements Interceptor &#123;\n\n    //存放批处理后的事件数组，声明\n    private List&lt;Event> addHeaderEvents;\n\n    //开启\n    @Override\n    public void initialize() &#123;\n        //初始化\n        addHeaderEvents = new ArrayList&lt;>();\n    &#125;\n\n    //处理单个事件\n    @Override\n    public Event intercept(Event event) &#123;\n        /**\n         * 根据消息中是否包含atguigu，获取header和body，如果body里有atguigu\n         */\n        Map&lt;String, String> headers = event.getHeaders();\n        String body = new String(event.getBody());\n\n        //看body中是否含有atguigu\n        if (body.contains(\"atguigu\")) &#123;\n            headers.put(\"type\", \"atguigu\");\n        &#125; else &#123;\n            headers.put(\"type\", \"default\");\n        &#125;\n\n        //3. 返回数据\n        return event;\n    &#125;\n\n    //批处理\n    @Override\n    public List&lt;Event> intercept(List&lt;Event> list) &#123;\n        //1.清空全局集合\n        addHeaderEvents.clear();\n\n        //2.遍历list做处理放入集合\n        for (Event event : list) &#123;\n            addHeaderEvents.add(intercept(event));\n        &#125;\n\n        //3. 返回数据\n        return addHeaderEvents;\n    &#125;\n\n    //关闭\n    @Override\n    public void close() &#123;\n\n    &#125;\n\n    //建立bulider\n    public static class Builder implements Interceptor.Builder&#123;\n\n        //创建拦截器对象\n        @Override\n        public Interceptor build() &#123;\n            return new TypeInterceptor();\n        &#125;\n\n        //获取配置信息\n        @Override\n        public void configure(Context context) &#123;\n\n        &#125;\n    &#125;\n&#125;\n\n\n\n自定义source需求：\npackage com.atguigu.source;\n\nimport org.apache.flume.Context;\nimport org.apache.flume.Event;\nimport org.apache.flume.EventDeliveryException;\nimport org.apache.flume.PollableSource;\nimport org.apache.flume.conf.Configurable;\nimport org.apache.flume.event.SimpleEvent;\nimport org.apache.flume.source.AbstractSource;\n\nimport javax.xml.stream.events.StartDocument;\nimport java.nio.charset.StandardCharsets;\nimport java.util.HashMap;\n\npublic class MySource extends AbstractSource implements Configurable, PollableSource &#123;\n\n    //声明数据的前后缀，在configure中做初始化\n    private String prefix;//前缀\n    private String subfix;//后缀\n    private long delay;\n\n    @Override\n    public Status process() throws EventDeliveryException &#123;\n\n\n        //2. 循环创建\n        try &#123;\n            for (int i = 0; i &lt; 5; i++) &#123;\n\n                //1. 声明事件\n                Event event = new SimpleEvent();\n                HashMap&lt;String, String> header = new HashMap&lt;>();\n                event.setHeaders(header);\n\n                if (subfix != null) &#123;\n                    event.setBody((prefix + \"atguigu_\" + i + subfix).getBytes());\n                &#125; else &#123;\n                    event.setBody((prefix + \"atguigu_\" + i).getBytes());\n                &#125;\n\n\n                getChannelProcessor().processEvent(event);//处理事件\n\n                Thread.sleep(delay);\n\n                return Status.READY;\n\n            &#125;\n        &#125; catch (Exception e) &#123;\n            e.printStackTrace();\n            return Status.BACKOFF;\n        &#125;\n\n        return null;\n\n    &#125;\n\n    @Override\n    public long getBackOffSleepIncrement() &#123;\n        return 0;\n    &#125;\n\n    @Override\n    public long getMaxBackOffSleepInterval() &#123;\n        return 0;\n    &#125;\n\n    @Override\n    public void configure(Context context) &#123;\n        prefix = context.getString(\"pre\", \"pre-\");//给了默认值，key是a1.sources.r1.pre里的pre\n\n        subfix = context.getString(\"sub\");//不给默认值\n\n        delay = context.getLong(\"delay\", 2000L);\n\n    &#125;\n&#125;\n\n\n自定义sink需求：\npackage com.atguigu.sink;\n\nimport org.apache.flume.*;\nimport org.apache.flume.conf.Configurable;\nimport org.apache.flume.sink.AbstractSink;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n\npublic class MySink extends AbstractSink implements Configurable &#123;\n    private String prefix;\n    private String subfix;\n\n    //创建logger对象\n    private Logger logger = LoggerFactory.getLogger(MySink.class);\n\n    @Override\n    public Status process() throws EventDeliveryException &#123;\n        //1. 获取channel开启事物\n        Channel channel = getChannel();\n        Transaction transaction = channel.getTransaction();\n        transaction.begin();\n\n        //2. 从channel中抓取数据打印到控制台\n        try &#123;\n            // 2.1 抓取数据\n            Event event = channel.take();\n\n            //2.2 处理数据用logger对象打印到console\n            logger.info(prefix + new String(event.getBody()) + subfix);\n\n            //2.3 提交\n            transaction.commit();\n            return Status.READY;\n\n        &#125;catch (Exception e)&#123;\n            //抓到异常，先回滚\n            transaction.rollback();\n            return Status.BACKOFF;\n\n        &#125;finally &#123;\n            transaction.close();\n        &#125;\n    &#125;\n\n    //对配置文件读取赋值操作\n    @Override\n    public void configure(Context context) &#123;\n        prefix = context.getString(\"pre\", \"pre-\");\n        subfix = context.getString(\"sub\");\n\n    &#125;\n&#125;\n\n\n3.8 flume流量监控3.8.1 Ganglia的安装与配置读取数据然后展示到前端网页，并且有持久化到磁盘\n3.8.2 Ganglia启动3.8.3 Ganglia使用4. 企业面试题4.1 你是如何实现 Flume 数据传输的监控的4.2 Flume 的 Source，Sink，Channel 的作用？你们 Source 是什么类型？4.3 Flume 的 Channel Selectors4.4 Flume 参数调优4.5 Flume 的事务机制4.6 Flume 采集数据会丢失吗?hadoop的常用端口\n\n\n断点续传文件\n\n\n\n\n\n\n\n\n\n昨天的问题是flume单机测试端口监听可以，一上hdfs就失败卡住，我还以为是那个什么slf的问题，昨天没有去查看日志，然后就删了配配了删也就有点无头苍蝇的感觉，今天早上想起来去tail -f 监控日志，发现是拒绝连接，然后开始hdfs没启动，后面启动了还是报错，大概定位到job的配置文件的问题，但是具体还是不清楚，然后后面看到了端口号是9000我觉得有问题，然后就改了9870，发现9870不能用，要用8020，是属于什么rpc的端口，目前还不知道rpc是什么意思，准备晚上可以查一下\n\n\n\n\n\n\n\n\n\n​ RPC是指远程过程调用，也就是说两台服务器A、B，一个应用部署在A服务器上，想要调用B服务器上应用提供的函数&#x2F;方法，由于不在一个内存空间，不能直接调用，需要通过网络去发起一次调用请求获取结果。\n感觉就是我想让你做件事\n","slug":"flume","date":"2022-03-31T13:03:51.085Z","categories_index":"","tags_index":"","author_index":"Cencus"},{"id":"c42ea271ec848445301016f3b979e666","title":"linux学习笔记","content":"\n\n基础指令指令格式指令体 参数 位置 \n1、ls——列出路径：\n\n\n\n\n\n\n\n\n\n相对路径：有一个参照物——当前工作目录，**..&#x2F;代表返回上级目录，.&#x2F;代表当前目录绝对路径：以&#x2F;**开头的写法\n参数\n\na：all列出所有，包括隐藏文件（以**.**开头的文件）各列属性解析\n\nl：list\n\nh：以可读性较高的格式（主要是占用大小来展示）问题：为什么很多文件都是4k？A：文件夹本身占用4k大小，而不代表文件夹内容占用4K大小。（更加本质的原因是什么？我才和指针有关）\n\n\n\n2、pwd——打印当前目录3、cd——切换工作目录\n4、mkdir——创建目录mkdir 路径\n所有权限：可读可写可执行\nmkdir -p 路径-p：创建多级目录\n&#96;mkdir -p &#x2F;root&#x2F;a&#x2F;b&#x2F;c&#x2F;d’abcd开始都不存在，如果不加-p不得行\nmkdir 路径1 路径2 路径3一次性创建多个目录\n5、touch——创建文件touch 文件名.后缀在当前工作目录下创建文件\ntouch 目录touch 文件1 文件2创建多个文件\n6、cp——复制指令copy\ncp 被复制的文档路径&#x2F;文件名 文档复制到的路径&#x2F;[文件名]注两个文件名可以不一样，相当于复制完了再执行重命名操作，但是一般不这么做第二个的文件名可选\ncp -r 被复制的文件夹路径 复制到的文件夹路径-r表示递归，不只要拿文件夹还要文件夹下的文件和文件夹。\n\n7、mv——移动文件到新的位置move 移动\nmv 需要移动的文档路径&#x2F;文件名 移动到的文件路径&#x2F;[文件名]\nmv 需要移动的文件夹 需要移动到的地方mv &#x2F;linuxcopy&#x2F;py后面的&#x2F;可写可不写\n\n重命名\n8、rm——删除指令removerm -选项 文件选项：\n\n-f\n-r\n\n双击tab可以预览有哪些符合条件的，再继续写\nrm -f 文件\nrm -rf 文件夹\nrm -rf 文件 文件夹\n删除所有以linux开头的文件rm -rf linux**不管是什么，前面一样就行。通配符\n9、vimvim是一款文本编辑器\nvim 文件的路径打开一个文件（可以不存在，可以存在）\n10、输出重定向比如说ls有输出，一般都会显示到终端中，有时需要将命令结果保存到文件中进行分析统计，需要使用输出重定向\n\n&gt;：覆盖输出\n&gt;&gt;：追加输出\n\n\n\n\n\n\n\n\ncsdn中”&gt;”不能直接输入而是输入&amp;gt;，同理”&lt;”用&amp;lt;ls -al &gt; ls.txt\n\n\n创建了新文件ls.txt，如果有的\ncat作用1：cat有直接打开一个文件的功能 ，类似于预览\ncat 文件路径cat 还可以对文件进行合并cat 待合并的文件路径1 待合并的文件路径2 &gt; 合并之后的文件路径\n进阶指令1、df指令——查看磁盘空间作用：查看磁盘空间经常用df -h\nmonted on挂载点——分配盘符\n2、free 指令——查看内存使用情况语法：free [-m]-m：Mb\n\n3、head——查看前n行查看一个文件的前n行，如果不指定n，则默认前10行也是一种预览，和cat类似但是cat会全部弄出来head -n 文件路径\n4、tail——查看后n行查看一个文件的后n行，如果不指定n，则默认后10行\ntail-n 文件路径tail -f 文件路径作用：可以通过tail来查看文件的动态变化，变化的内容不能是用户手动增加的退出按q即可\n5、less——查看文件以较少内容进行输出，按下辅助功能键查看更多语法：less 需要查看的文件路径\n空格，一页一页的翻数字+回车上下方向键\n6、wc——统计文件内容信息信息：行数，单词数，字节数语法：wc -lwc 需要统计的文件路径\n\nl：lines：行数\nw：words，单词数\nc：byte，字节数单词数：\n\n\n\n\n\n\n\n\n依靠空格来判断单词数量，中 国 （三个单词）\ndate指令（重点）——表示操作时间日期操作：读取，设置（后面shell脚本需要用）\n\n\n+号表示读取\ndate输出当前时间\n\nCST：当地时间\nUCT：\nGMT\n\ndate +%F输出年月日必须大写\n复杂写法：date &quot;+%Y-%m-%d&quot;\n大小写不一样\ndate “+%F %T”为什么要有””呢，因为要把后面的当做一个整体\n等价于：date &quot;+%Y-%m-%d  %H:%M:%S&quot;\n符号可选：+之后，-之前单位可选：day，mouth，year\n\n8、cal——操作日历cal直接输出当前月份的日历\n9、clear&#x2F;ctrl+L作用：清除终端中已经存在的命令和结果\n10、管道管道符：|不是1或者小写的l作用：\n\n☆过滤\n特殊\n扩展处理不能单独使用，必须需要配合前面所讲的一些指令一起使用，作用是为了辅助作用\n\n过滤，类似于正则需要通过管道查询出根目录下包含“b”字母的文档名称ls /|grep bLinux grep 命令用于查找文件里符合条件的字符串\n\nls &#x2F; | wc -l查看&#x2F;下有多少个文件\n高级指令hostnamehost主机name名字操作服务器的主机名（读取，设置）这个设置是一次性的，是后面基本不用的只是掌握读取就行\n为什么要操作它： \nhostname\nhostname -f输出当前主机名总得fqdn（全限定域名）\n\nidwhoami在脚本中使用告诉程序自己是谁，日志shell脚本\n☆ps -efps 查看服务器的进程信息\n\nps结果中过滤出想要查看的进程状态ps -ef | gerp 进程名称\ntop查看服务器的进程站的资源退出按q\nvi和vim编辑器6.1 vi和vim的基本介绍vi是文本编辑器vim是增强版的文本编辑器（加入了代码补充，高亮等等）\n6.2 三种模式\n正常模式用vim打开文件，就可以进入正常模式，在正常模式下可以使用快捷键\n插入模式&#x2F;编辑模式该模式下，可以输入内容按下i,I,o,O,a,A,r,R都可以进入，但有小的区别\n命令行模式可以保存退出，可以输入一些命令，可以提供相关指令，完成读取，存盘，替换，离开等等\n\n\n6.3 vi和vim的快捷键\n拷贝当前行yy，拷贝当前行向下的5行，并粘贴。p粘贴不用着急慢慢弄\n删除当前行dd,删除当前行向下的5行5dd\n在文件中查找某个单词，【命令行 &#x2F;关键词，回车查找，输入n就是查找下一个】\n设置文件的行号，取消文件的行号（命令下**:set nu和:set nonu**）\n编辑 &#x2F;etc&#x2F;profile文件，使用快捷键到文档的最末行：G（shift+g）和最首行gg(正常模式下)，注意都是在正常模式下执行的\n在一个文件中输入hello，然后有撤销这个动作，在正常模式下输入u\n编辑 &#x2F;etc&#x2F;profile文件，并将光标移动到第20行 shift+g第一步显示行号：:set nu第二步：输入20第三步：输入shift+g\n\n要么就是唯一映射，要么就是有操作栈，达成唯一条件就跳出\n","slug":"linux学习笔记","date":"2022-03-29T15:15:42.000Z","categories_index":"","tags_index":"大数据","author_index":"Cencus"},{"id":"464082c80523a34f8ff30e1785265886","title":"kafka","content":"第一章\n\n\n\n\n\n\n\n\n生产商，超市与消费者的爱恨情仇\n1. 定义分布式，发布订阅，消息队列\n2. 消息队列的应用场景\n缓存消峰\n解耦\n异步通信\n\n3.两种模式\n点对点（一个生产者 一个消费者 一个主题 会删除数据）\n发布订阅模式（多个生产者，多个消费者而且相互独立，多个主题，不会删除数据）\n\n\n\n\n点对点\n订阅\n\n\n\n不具有topic(类似于一种分类)\n√\n\n\n点对点数据传输完成数据会被销毁，\n√\n\n\n用户也无法根据自己想要的topic进行消费\n√\n\n\n在hadoop中数据副本是不存在主从关系的，但是在kafka中是有的\n所有操作均&#x3D;&#x3D;优先对leader操作，Follower只有当leader挂掉才会工作，成为新的leader&#x3D;&#x3D;kafka如何对服务器上下线进行感知以及标识leader-&gt;与zookeeper合作，通过zookeeper配合使用，但是慢慢的会丢弃掉zook在2.8版本以前必须依赖，但之后就不用了\n4. 架构一、生产者\n二、broker\n（1）broker就是服务器\n（2）topic主题\n（3）分区\n（4）可靠性 副本\n（5）生产者与消费者只对leader操作\n三、消费者\n（1）消费者之间相互独立\n（2）消费者组，比如宝妈组，一个分区只能由一个消费者消费\n四、zookeeper\n存储了\n（1）broker.ids 谁在线\n（2）每个分区的leader和isr\n第二章 kafka快速入门1. 安装配置kafka\nvim conf/server.properties\nbroker.id&#x3D;0\nlog.dirs&#x3D;&#x2F;opt&#x2F;module&#x2F;kafka&#x2F;datas\nzookeeper.connect&#x3D;hadoop102:2181,hadoop103:2181,hadoop104:2181&#x2F;kafka\n\n\n\n分发并改变broker.id为不一样的数字\n一个问题：当broker.id设置过一样的并且已经启动过了，kafka已经向zookeeper做了注册时，kafka可能会异常停止，解决方法：\n\n删除掉所有节点上的所有kafka目录\n开启zookeeper，以客户端形式删除掉整个kafka目录\n\n#开启zookeeper\n\n#登录zookeeper\nbin&#x2F;zkCli.sh -server 主机ip:2181\n\n# 删除kafka目录\ndeleteall &#x2F;kafka\n\n\n\n启动时先启动zk再启动kf\n停止时先停止kf再停止zk\n脚本\n#!&#x2F;bin&#x2F;bash\ncase $1 in\n&quot;start&quot;)\n\tfor i in hadoop102 hadoop103 hadoop104\n\tdo\n\t\tssh $i &quot;绝对路径&quot;\n\tdone\n;;\n&quot;stop&quot;)\n\n;;\nesac\n\n\n\n2.常用命令行一、主题\n（1）–bootstrap-server hadoop102:9092,hadoop103:9092\n（2）–topic first\n（3）–create\n（4）–delete\n（5）–alter\n（6）–list\n（7）–describe\n（8）–partitions\n（9）–replication-factor\n二、生产者\n（1）–bootstrap-server hadoop102:9092,hadoop103:9092\n（1）–topic first\n三、消费者\n（1）–bootstrap-server hadoop102:9092,hadoop103:9092\n（1）–topic first\n第三章、kafka生产者3.1 生产者发送消息流程3.1.1发送原理\n3.1.2 重要参数3.2 异步发送API什么叫异步发送？代码编写：\n&lt;dependencies>\n    &lt;dependency>\n    &lt;groupId>org.apache.kafka&lt;/groupId>\n    &lt;artifactId>kafka-clients&lt;/artifactId>\n    &lt;version>3.0.0&lt;/version>\n    &lt;/dependency>\n&lt;/dependencies>\n代码模板\npackage com.atguigu.kafka.producer;\n\nimport org.apache.kafka.clients.producer.KafkaProducer;\nimport org.apache.kafka.clients.producer.ProducerConfig;\nimport org.apache.kafka.clients.producer.ProducerRecord;\nimport org.apache.kafka.common.serialization.StringSerializer;\n\nimport java.util.Properties;\n\npublic class CustomProducer &#123;\n\n    public static void main(String[] args) &#123;\n        //1. 创建配置\n        Properties properties = new Properties();\n\n        //2. 对配置进行自定义\n        properties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG,\"hadoop102:9092,hadoop103:9092\");\n\n    \n\n        //3. 创建kafka生产者\n        // \"\" string\n        KafkaProducer&lt;String, String> KafkaProducer = new KafkaProducer&lt;>(properties);\n\n        //4. 让生产者发送数据\n\n        for (int i = 0; i &lt; 5; i++) &#123;\n            KafkaProducer.send(new ProducerRecord&lt;>(\"first\",\"atguigu\" + i));\n        &#125;\n\n        //5. 关闭资源\n        KafkaProducer.close();\n    &#125;\n&#125;\n\n\n3.2.2 带回调函数的异步发送回调函数会在 producer 收到 ack 时调用，为异步调用，该方法有两个参数，分别是元数据信息（RecordMetadata）和异常信息（Exception），如果 Exception 为 null，说明消息发送成功，如果 Exception 不为 null，说明消息发送失败。\npackage com.atguigu.kafka.producer;\n\nimport org.apache.kafka.clients.producer.*;\nimport org.apache.kafka.common.serialization.StringSerializer;\n\nimport java.util.Properties;\n\npublic class CustomProducerCallback &#123;\n\n    public static void main(String[] args) &#123;\n        //0. 配置\n        Properties properties = new Properties();\n\n        //连接集群BOOTSTRAP_SERVERS_CONFIG等于--bootstrap-server后面的配置\n        properties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, \"hadoop102:9092,hadoop103:9092\");\n\n        //指定key:value \"key.serializer\";\n//        properties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG,\"org.apache.kafka.common.serialization.StringSerializer\");\n        properties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());\n        properties.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());\n\n        //1. 创建kafka生产消费者\n        // \"\" string\n        KafkaProducer&lt;String, String> KafkaProducer = new KafkaProducer&lt;>(properties);\n\n        //2. 发送数据,启用回调函数\n        for (int i = 0; i &lt; 5; i++) &#123;\n            KafkaProducer.send(new ProducerRecord&lt;>(\"first\", \"atguigu\" + i), new Callback() &#123;\n                @Override\n                public void onCompletion(RecordMetadata recordMetadata, Exception e) &#123;\n                    if (e == null) &#123;\n                        System.out.println(\"主题：\" + recordMetadata.topic() + \"分区：\" + recordMetadata.partition());\n                    &#125;\n                &#125;\n            &#125;);\n        &#125;\n\n        //3. 关闭资源\n        KafkaProducer.close();\n    &#125;\n&#125;\n\n3.3 同步发送API只需要在异步发送的基础上再.get()方法\npackage com.atguigu.kafka.producer;\n\nimport org.apache.kafka.clients.producer.KafkaProducer;\nimport org.apache.kafka.clients.producer.ProducerConfig;\nimport org.apache.kafka.clients.producer.ProducerRecord;\nimport org.apache.kafka.common.serialization.StringSerializer;\n\nimport java.util.Properties;\nimport java.util.concurrent.ExecutionException;\n\npublic class CustomProducerSync &#123;\n\n    public static void main(String[] args) throws ExecutionException, InterruptedException &#123;\n        //0. 配置\n        Properties properties = new Properties();\n\n        //连接集群BOOTSTRAP_SERVERS_CONFIG等于--bootstrap-server后面的配置\n        properties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG,\"hadoop102:9092,hadoop103:9092\");\n\n        //指定key:value \"key.serializer\";\n//        properties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG,\"org.apache.kafka.common.serialization.StringSerializer\");\n        properties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());\n        properties.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());\n\n        //1. 创建kafka生产消费者\n        // \"\" string\n        KafkaProducer&lt;String, String> KafkaProducer = new KafkaProducer&lt;>(properties);\n\n        //2. 发送数据\n        for (int i = 0; i &lt; 5; i++) &#123;\n            KafkaProducer.send(new ProducerRecord&lt;>(\"first\",\"atguigu\" + i)).get();//只需要send后面.get()就变成了同步\n        &#125;\n\n        //3. 关闭资源\n        KafkaProducer.close();\n    &#125;\n&#125;\n\n3.4 生产者分区3.4.1 分区好处\n便于合理使用存储资源，每个Partition在一个Broker上存储，可以把海量的数据按照分区切割成一块一块数据存储在多台Broker上。合理控制分区的任务，可以实现负载均衡的效果。\n提高并行度，生产者可以以分区为单位发送数据；消费者可以以分区为单位进行消费数据3.4.2 生产者发送消息的分区策略默认分区器 DefaultPartitioner\n\n\n指明partition就用该partition\n\n如果没有分区策略但是有key就依据key的hash值来进行分区(将key与topic的partition数取余得到partition值)\n\n如果既没有分区策略又没有key，那么就会选择**黏性分区策略(sticky partition)**，随机选择一个分区，并尽可能一直 使用该分区，待\n\n该分区的batch已满或者已完成，Kafka再随机一个分区进行使用（和上一次的分区不同）。\n\n\n例如：第一次随机选择0号分区，等0号分区当前批次满了（默认16k）或者linger.ms设置的时间到， Kafka再随机一个分区进 行使用（如果还是0会继续随机）。\n3.4.3 自定义分区器需求：如果含有atguigu就进入0号分区，否则进入1号分区\npackage com.atguigu.kafka.producer;\n\nimport org.apache.kafka.clients.producer.Partitioner;\nimport org.apache.kafka.common.Cluster;\n\nimport java.util.Map;\n\npublic class MyPartitioner implements Partitioner &#123;\n    //最重要的方法\n    @Override\n    public int partition(String topic, Object key, byte[] keyBytes, Object value, byte[] valueBytes, Cluster cluster) &#123;\n        // 获取数据\n        // atguigu hello\n        String msg = value.toString();\n        int partition;\n\n        if (msg.contains(\"atguigu\"))&#123;\n            //发送到0号分区\n            partition = 0;\n\n        &#125;else &#123;\n            partition = 1;\n        &#125;\n        return partition;\n    &#125;\n\n    @Override\n    public void close() &#123;\n\n    &#125;\n\n    @Override\n    public void configure(Map&lt;String, ?> map) &#123;\n\n    &#125;\n&#125;\n\n\n然后需要在producer中去做配置\nproperties.put(ProducerConfig.PARTITIONER_CLASS_CONFIG,\"com.atguigu.kafka.producer.MyPartitioner\");\n\n3.5 生产经验——如何提高吞吐量主要有三个参数\n\nbatch.size：每一个发送数据块的大小，默认16k\n\nlinger.ms：发车的等待时间：默认0ms，企业中一般时间在5~100ms\n\nRecordAccumulator：缓冲区的大小，默认32M\n\n\n比喻：包裹大小，仓库大小和快递车的等待时间\n配置方法：在main中对properties进行配置\n// batch.size：批次大小，默认 16K\n properties.put(ProducerConfig.BATCH_SIZE_CONFIG, 16384);\n // linger.ms：等待时间，默认 0\n properties.put(ProducerConfig.LINGER_MS_CONFIG, 1);\n // RecordAccumulator：缓冲区大小，默认 32M：buffer.memory\n properties.put(ProducerConfig.BUFFER_MEMORY_CONFIG, 33554432);\n // compression.type：压缩，默认 none，可配置值 gzip、snappy、lz4 和 zstd\nproperties.put(ProducerConfig.COMPRESSION_TYPE_CONFIG,\"snappy\");\n\n还有压缩方式：compression.type，企业中一般使用snappy\n3.6 生产经验——数据可靠性数据完全可靠条件：ACK &#x3D; -1 + 分区副本 &gt;&#x3D; 2 + ISR里应答的最小副本 &gt;&#x3D; 2所谓的ISR就是一个还活着的人的群聊，如果一个人长时间不吭声，那就把它踢出群聊数据重复分析：producer发给了leader，然后ack此时&#x3D;-1，leader向Follower同步，然后再同步给了Follower后leader挂掉了，然后此时会重新选举一个leader，然后因为leader并没有向producer发送我已经接受完毕的消息，这个时候producer会再发一次，如果刚刚的Follower当上了leader那么数据就会发生重复\n3.7 生产经验——数据去重3.7.1 概念幂等性与事务：\n3.7.2 幂等性\n幂等性原理\n\n幂等性：保证producer发送给broker端的数据的不重复\n精确一次：幂等性 + 数据可靠条件\n重复数据判断的依据：&lt;PID, Partition, SeqNumber&gt;\nPID是kafka启动时分配一个新的，代表了一次会话，Partition是分区，代表了一个分区，SeqNumber是对数据的标号，精准定位了一个数据，如果这个向量完全一致那么就是重复数据，就会被在内存中干掉\ntips：只能判断单次会话（启停一次kafka集群）\n\n如何开启\n\n默认开启enable.idempotence = true/false\n3.7.3 生产者事务\n依然在解决重复数据，开启了幂等性只能保证单次会话，而事务就是在填补这个坑\nKafka事务的API\n//初始化事物\npublic void initTransactions();\n//开启事物\npublic void beginTransaction() throws ProducerFencedException;\n//提交事务\npublic void commitTransaction() throws ProducerFencedException;\n\n//在事务内提交已经消费的偏移量，用于消费者\npublic void sendOffsetsToTransaction();\n\n\n//放弃事务，类似回滚\npublic void abortTransaction() throws ProducerFencedException;\n\n\n\n\n\npackage com.atguigu.kafka.producer;\n\nimport org.apache.kafka.clients.producer.KafkaProducer;\nimport org.apache.kafka.clients.producer.ProducerConfig;\nimport org.apache.kafka.clients.producer.ProducerRecord;\nimport org.apache.kafka.common.serialization.StringSerializer;\n\nimport java.util.Properties;\n\npublic class CustomProducerTransaction &#123;\n\n    public static void main(String[] args) &#123;\n        //0. 配置\n        Properties properties = new Properties();\n\n        //连接集群BOOTSTRAP_SERVERS_CONFIG等于--bootstrap-server后面的配置\n        properties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG,\"hadoop102:9092,hadoop103:9092\");\n\n        //指定key:value \"key.serializer\";\n//        properties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG,\"org.apache.kafka.common.serialization.StringSerializer\");\n        properties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());\n        properties.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());\n\n        //指定事务id\n        properties.put(ProducerConfig.TRANSACTIONAL_ID_CONFIG,\"transaction01\");\n\n\n        //1. 创建kafka生产消费者\n        // \"\" string\n        KafkaProducer&lt;String, String> KafkaProducer = new KafkaProducer&lt;>(properties);\n\n        //初始化事物\n        KafkaProducer.initTransactions();\n\n        //开启事务\n        KafkaProducer.beginTransaction();\n\n        try &#123;\n            //2. 发送数据\n            for (int i = 0; i &lt; 5; i++) &#123;\n                KafkaProducer.send(new ProducerRecord&lt;>(\"first\",\"atguigu\" + i));\n            &#125;\n            //发送数据结束提交事务\n\n//            i = 1 / 0;\n\n            KafkaProducer.commitTransaction();\n\n        &#125;catch (Exception e)&#123;\n            //发送失败，终止事务，回滚事务\n            KafkaProducer.abortTransaction();\n\n        &#125;finally &#123;\n            //3. 关闭资源\n            KafkaProducer.close();\n        &#125;\n    &#125;\n&#125;\n\n\n3.8 生产经验——数据有序3.9 生产经验——数据乱序（面试开发重点）如何保证单分区有序？\n\n\n\nkafka1.x 以前\nkafka1.x 以后\n\n\n\nmax.in.flight.requests.per.connection=1不需要考虑是否开启幂等性\n未开启幂等性 max.in.flight.requests.per.connection需要设置为1开启幂等性 max.in.flight.requests.per.connection需要设置小于等于5。\n\n\n为何开启幂等性可以保证5个以内的数据有序\nkafka服务端会缓存producer发来的最近5个request的元数据（对于序号正常的数据（商品）可以直接落盘（上架），对于序号不正确的商品就等到5个来齐了，从序号不对的地方开始进行排序，完了之后再上架）\n第4章 kafka broker4.1 kafka broker的工作流程4.1.1 zookeeper存储的kafka信息\nbroker.ids\n\nleader\n\n辅助选举的control谁能抢到谁就是leader\n\n\n4.1.2 broker的工作流程\n\n集群启动后，向zk注册，讲我还活着，\n然后就开始选老大，谁先坐上zk里面controller控制者的宝座，谁就当选leader，\n选出来的controller就监控broker的变化，并且把相应的信息写入到zk，\n然后其他节点就从该节点去认识自己的兄弟们，\n假设原来的leader挂掉了，那么controller就会知道，然后又开始组织新一轮的选举，并且去更新相应的信息\n\n4.1.3 broker重要参数略\n4.2 生成经验——节点服役与退役4.2.1 服役新节点1、 准备一个新的服务器，安装有Java，kafka等等环境\n2、配置新的服务器\nvim KAFKA_HOME/conf/server.properties\n#broker.id 必须要改\nbroker.id&#x3D;105\nlog.dirs&#x3D;&#x2F;opt&#x2F;module&#x2F;kafka&#x2F;datas\nzookeeper.connect&#x3D;hadoop102:2181,hadoop103:2181,hadoop104:2181&#x2F;kafka\n\n\n\n3、执行负载均衡\n3.1、创建一个要均衡的主题vim topics-to-move.json\n&#123;\n \"topics\": [\n &#123;\"topic\": \"first\"&#125;[,&#123;\"topic\": \"second\"&#125;]\n ],\n \"version\": 1\n\n\n3.2、创建执行计划\nbin/kafka-reassign-partitions.sh -- bootstrap-server hadoop102:9092 --topics-to-move-json-file  topics-to-move.json --broker-list \"102,103,104,105\" --generate\n\n执行结束出现一下内容\nCurrent partition replica assignment\n&#123;&quot;version&quot;:1,&quot;partitions&quot;:[&#123;&quot;topic&quot;:&quot;first&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[103,102,104],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;first&quot;,&quot;partition&quot;:1,&quot;replicas&quot;:[104,103,105],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;first&quot;,&quot;partition&quot;:2,&quot;replicas&quot;:[105,104,102],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;]&#125;\n\nProposed partition reassignment configuration\n&#123;&quot;version&quot;:1,&quot;partitions&quot;:[&#123;&quot;topic&quot;:&quot;first&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[102,104,103],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;first&quot;,&quot;partition&quot;:1,&quot;replicas&quot;:[103,102,104],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;first&quot;,&quot;partition&quot;:2,&quot;replicas&quot;:[104,103,102],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;]&#125;\n\n\n\n复制Proposed partition reassignment configuration以下的内容\n&#123;&quot;version&quot;:1,&quot;partitions&quot;:[&#123;&quot;topic&quot;:&quot;first&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[102,104,103],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;first&quot;,&quot;partition&quot;:1,&quot;replicas&quot;:[103,102,104],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;first&quot;,&quot;partition&quot;:2,&quot;replicas&quot;:[104,103,102],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;]&#125;\n\n创建创建副本存储计划：vim increase-replication-factor.json\n#按i插入刚刚复制的内容\n&#123;&quot;version&quot;:1,&quot;partitions&quot;:[&#123;&quot;topic&quot;:&quot;first&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[102,104,103],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;first&quot;,&quot;partition&quot;:1,&quot;replicas&quot;:[103,102,104],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;first&quot;,&quot;partition&quot;:2,&quot;replicas&quot;:[104,103,102],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;]&#125;\n\n执行副本存储计划\nbin/kafka-reassign-partitions.sh -- bootstrap-server hadoop102:9092 --reassignment-json-file increase-replication-factor.json  --execute\n\n验证副本存储计划\nbin/kafka-reassign-partitions.sh -- bootstrap-server hadoop102:9092 --reassignment-json-file increase-replication-factor.json  --verify\n\n\n\n4.2.2 退役旧节点a、创建一个均衡主题\n[atguigu@hadoop102 kafka]$ vim topics-to-move.json\n&#123;\n &quot;topics&quot;: [\n &#123;&quot;topic&quot;: &quot;first&quot;&#125;\n ],\n &quot;version&quot;: 1\n&#125;\n\n\nb、创建执行计划\nbin&#x2F;kafka-reassign-partitions.sh --bootstrap-server hadoop102:9092 --topics-to-move-json-file topics-to-move.json --broker-list &quot;0,1,2&quot; --generate\n\nCurrent partition replica assignment\n&#123;&quot;version&quot;:1,&quot;partitions&quot;:[&#123;&quot;topic&quot;:&quot;first&quot;,&quot;partition&quot;:0,&quot;replic\nas&quot;:[2,0,1],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;first&quot;,&quot;par\ntition&quot;:1,&quot;replicas&quot;:[3,1,2],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;to\npic&quot;:&quot;first&quot;,&quot;partition&quot;:2,&quot;replicas&quot;:[0,2,3],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;\nany&quot;,&quot;any&quot;]&#125;]&#125;\nProposed partition reassignment configuration\n&#123;&quot;version&quot;:1,&quot;partitions&quot;:[&#123;&quot;topic&quot;:&quot;first&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[2,0,1],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;first&quot;,&quot;par\ntition&quot;:1,&quot;replicas&quot;:[0,1,2],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;to\npic&quot;:&quot;first&quot;,&quot;partition&quot;:2,&quot;replicas&quot;:[1,2,0],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;\nany&quot;,&quot;any&quot;]&#125;]&#125;\n\n\nc、创建副本存储计划\n[atguigu@hadoop102 kafka]$ vim increase-replication-factor.json\n\n\n&#123;&quot;version&quot;:1,&quot;partitions&quot;:[&#123;&quot;topic&quot;:&quot;first&quot;,&quot;partition&quot;:0,&quot;replic\nas&quot;:[2,0,1],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;first&quot;,&quot;par\ntition&quot;:1,&quot;replicas&quot;:[0,1,2],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;to\npic&quot;:&quot;first&quot;,&quot;partition&quot;:2,&quot;replicas&quot;:[1,2,0],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;\nany&quot;,&quot;any&quot;]&#125;]&#125;\n\nd、执行副本存储计划\nbin&#x2F;kafka-reassign-partitions.sh --bootstrap-server hadoop102:9092 --reassignment-json-file increase-replication-factor.json --execute\n\ne、验证副本存储计划\n[atguigu@hadoop102 kafka]$ bin&#x2F;kafka-reassign-partitions.sh --bootstrap-server hadoop102:9092 --reassignment-json-fileincrease-replication-factor.json --verify\n\n\nStatus of partition reassignment:\nReassignment of partition first-0 is complete.\nReassignment of partition first-1 is complete.\nReassignment of partition first-2 is complete.\nClearing broker-level throttles on brokers 0,1,2,3\nClearing topic-level throttles on topic first\n\n\n\n4.3 Kafka副本4.3.1 副本基本信息\nAR：所有副本统称\n\nISR：还喘气的\n\nOSR：over了的喘不了气了\n\n\n4.3.2 leader选举流程// ar=[1,0,2];\n// isr =[0,1,2]\n\nwhile(ar!=null &amp;&amp; ar.first() not in isr)&#123;\n    ar.pop()\n&#125;\n\nar排在前的活着的成为leader\n4.3.3 leader和Follower故障处理细节（超级系）LEO(log end offset)：每个副本的最后一个offset，leo &#x3D; offset + 1\nHW（high watermark）：副本中最小的leo（也就是最小的offset）（也就是木桶最短的那一个）\n一、Follower故障\n\n提出群聊isr\n其他人正常工作\nFollower恢复后，会读取上次的HW，并将log高于HW的位置截掉然后从HW开始向leader同步\n等Follower的leo大于等于该partition的leader的HW时，就是追上来了，就可以重新假如ISR\n\n一句话：HW就是抄的最慢的人抄到哪里了，某个broker的LEO就是就是这个人写到哪里了，故障了恢复了就从抄的最慢的人那里开始抄到leader到的那里\n二、leader故障\n\n重选leader\n然后让所有Follower向我看齐\n\n一句话：谁当皇帝，谁敢比我抄的多卷子给他撕了。\n4.3.4 分区副本分配（副本在broker中怎么存储）思考：负载均衡 + 可靠性\n好你已经可以当架构师了\n就是这样，保证前面两条\n4.3.5 生产经验——手动调整分区副本存储人话：就是我想把副本存在哪儿怎么存我就存在哪儿怎么存\n步骤：\n1、创建副本存储计划\n[atguigu@hadoop102 kafka]$ vim increase-replication-factor.json\n&#123;\n&quot;version&quot;:1,\n&quot;partitions&quot;:[&#123;&quot;topic&quot;:&quot;three&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[0,1]&#125;,\n&#123;&quot;topic&quot;:&quot;three&quot;,&quot;partition&quot;:1,&quot;replicas&quot;:[0,1]&#125;,\n&#123;&quot;topic&quot;:&quot;three&quot;,&quot;partition&quot;:2,&quot;replicas&quot;:[1,0]&#125;,\n&#123;&quot;topic&quot;:&quot;three&quot;,&quot;partition&quot;:3,&quot;replicas&quot;:[1,0]&#125;]\n&#125;\n\n\n\n\n2、执行\nbin&#x2F;kafka-reassign-partitions.sh --bootstrap-server hadoop102:9092 --reassignment-json-file increase-replication-factor.json --execute\n\n\n\n3、验证\nbin&#x2F;kafka-reassign-partitions.sh --bootstrap-server hadoop102:9092 --reassignment-json-file increase-replication-factor.json --verify\n\n\n\n4、查看\nbin&#x2F;kafka-topics.sh --bootstrap-server hadoop102:9092 --describe --topic three\n\n\n\n4.3.6 Leader partition 负载平衡正常情况下，Kafka本身会自动把Leader Partition均匀分散在各个机器上，来保证每台机器的读写吞吐量都是均匀的。但是如果某些broker宕机，会导致Leader Partition过于集中在其他少部分几台broker上，这会导致少数几台broker的读写请求压力过高，其他宕机的 broker重启之后都是follower partition，读写请求很低，造成集群负载不均衡。\n\nauto.leader.rebalance.enable，默认是true。 自动Leader Partition 平衡 \nleader.imbalance.per.broker.percentage， 默认是10%。每个broker允许的不平衡 的leader的比率。如果每个broker超过 了这个值，控制器会触发leader的平衡。\nleader.imbalance.check.interval.seconds， 默认值300秒。检查leader负载是否平衡 的间隔时间。\n\n某一个节点的不平衡率：\n\n比如计算节点broker0的平衡率 ，首先要知道AR的长度为4，然后需要找到不平衡数：遍历所有分区，找到所有AR的首位是0但是leader不是0的，也就是应该当leader而没有当的，图中为1，则计算平衡率为 1&#x2F;4 &#x3D; 25% ，很明显需要触发leader partition平衡\n同理broker1计算应为 0\n但是我们能够看到图中依然是均衡的（leader均匀分布在集群），所以其实并不需要做负载均衡，同时触发再平衡还会消耗性能，一般都不会使用，就算是使用也会将leader.imbalance.per.broker.percentage调整到45%\n4.3.7 生产经验——增加副本因子需求：提高某个主题的replication数\n操作：无法通过命令行，只能通过计划\n1、手动增加副本存储\n[atguigu@hadoop102 kafka]$ vim increase-replication-factor.json\n\n2、粘贴内容\n3、执行副本存储计划\n","slug":"kafka","date":"2022-03-29T06:02:54.000Z","categories_index":"","tags_index":"大数据,kafka","author_index":"Cencus"},{"id":"f9e7089c97c02add0e0305f89cbc4548","title":"正则","content":"工具http://regex101.com页面解读\n限定符匹配对象是一个字符\n\n\n\n\n符号\n解释\n\n\n\n?\n限定符，前面的字符要出现0次或者[1]次,&#x2F;used?代表d出现0次或者1次，也就是可有可无\n\n\n\n|  前面的字符要出现0和或者多次\n\n+|匹配出现1次以上的字符{}|指定前面字符出现的次数或者范围\n\n\n\n\n\n\n\n\n\n{}匹配例子：ab{6}c：出现6次b，ab{2,6}：出现2次到6次，如果想出现两次以上：ab{2,}\n例子：\nac\nabc\nabbbbbc\nadc\naddddc\n如果想匹配多个字符的重复，比如想匹配ababababc，ab重复，可以将ab括起来，再加限定符这里可以用(ab)+\n或运算符示例：\na cat\na dog\na bird\na (cat|dog)：a后面是空格，后面括号代表要么是dog要么是cat、括号不可少，否则就变成了a cat|dog也就是要么是a cat 要么是dog。字符类：[abc]+，方括号里的内容代表内容只能取自他们\nabc \ntiger\naabbcc\ndog\n可以在[]里指定范围：[a-zA-z]代表所有英文字符再括号的前面加一个^号代表括号内之外的字符[^1-9]代表数字字符之外的字符\n元字符空格…正则表达式中的大多数元字符都是以\\开头\n\n\n\n写法\n解释\n\n\n\n\\d\n数字字符&#x3D;[0-9]\n\n\n\\D\n非数字字符&#x3D;[^0-9]\n\n\n\\w\n单词字符&#x3D;[a-zA-z0-9]\n\n\n\\W\n非单词字符&#x3D;[^a-zA-z0-9]\n\n\n\\s\n空白符，包含tab与换行符\n\n\n\\S\n非空白字符\n\n\n.\n任意字符，不包含换行符\n\n\n^\n匹配行首：^a只会匹配行首的a\n\n\n$\n匹配行位：a$只会匹配行尾的a\n\n\n贪婪与懒惰匹配*+{}默认匹配尽可能多的字符\n&lt;span>&lt;b>This is a sample text&lt;/b>&lt;/span>\n想要匹配&lt;.+&gt;：结果是全部匹配了，因为.会匹配尽可能多的字符解法：&lt;.+?&gt;：将贪婪匹配切换为懒惰匹配\n实例：16进制颜色值匹配#00\n#ffffff\n#ffaaff\n#00hh00\n#aabbc\n#000000\n#ffffffff\n可以用\\b代表字符边界，可以作为边界，你也就不要向下匹配了，就像这里的最后一个一样实例代码：#[a-fA-f0-9]&#123;6&#125;\\b\nip地址的匹配123\n255.255.255.0\n192.168.0.1\n0.0.0.0\n256.1.1.1\nThis is a string\n123.123.0\n四段数字构成，数字段之间用.隔开，注意这里.是指代所有字符，要用原本的意思进行转义\\.\n高级部分捕获、断言、平衡组，递归\n参考资料正则表达式30分钟入门教程 作者：deerchaoRegex tutorial—A quick cheatsheet by examples (英文) 作者：Jonny FoxRegular Expressions Tutorial (英文)\n","slug":"正则","date":"2022-03-28T10:12:04.000Z","categories_index":"","tags_index":"default","author_index":"Cencus"},{"id":"7f77c660cadf02b64dd36120e903f5f0","title":"Mysql","content":"MySQL数据库基本操作DDL（不涉及数据）数据库操作注意SQL不区分大小写\n\n读数据库的常用操作\n\n查看所有数据库show databases;\n创建数据库create database;\n切换（选择要操作的）数据库use mydb1;\n删除数据库drop database;\n修改数据库编码alter database mydb1 character set utf8;\n表创建创建表格式创建表是构建一张空表，指定这个表的名字，这个表有几列，每一列叫什么名字，以及每一列存储的数据类型\ncreate [if not exists] 表名(\n\t字段名1 类型[宽度] [约束条件] [comment '字段说明'],\n\t字段名2 类型[宽度] [约束条件] [comment '字段说明'],\n\t字段名3 类型[宽度] [约束条件] [comment '字段说明'],\n)[];\n\n例如\nuse mydb1;\ncreate table student(\n\tsid int,\n\tname varchar(20),\n\tage int,\n\tbirth date,\n\taddress varchar(20)\n);\n数据类型数字int\n字符串varchar(20)，如果不足20不会浪费存储空间，像变长一样\n日期\n\n\n\n\n\n\n\ndate\nYYYY.MM.DD\n\n\ntime\nH:M:S\n\n\ndatetime\ndate+time\n\n\n其他操作# 查看所有数据库所有表\nSHOW TABLES;\n\n# 查看指定表的创建语句\nSHOW CREATE TABLE student;\t\t\t\t\t\n\n# 查看表结构\nDESC student;\n\n# 删除表\nDROP TABLE student;\n\n\n修改表结构# 修改表结构 alter table 表名 列名 类型(长度) [约束];\n# 为student表增加一列\nALTER TABLE student ADD dept VARCHAR(20);\n\nDESC student;\n\n# 修改列名和类型 ALTER TABLE 表名 change 旧列名 新列名 类型(长度) [约束];\n# 修改dept列为department列，并且拓宽长度\nALTER TABLE student CHANGE dept department VARCHAR(30);\n\nDESC student;\n\n\n# 删除列 alter table 表名 drop 列名;\n# 删除学生表中department列\nALTER TABLE student DROP department;\n\nDESC student;\n\n\n# 修改表名 rename table 表名 to 新表名\n# 修改student 为stu\nRENAME TABLE student TO stu;\n\nDESC stu;\n\n\nDML基本操作：DML(Data Manipulation Language)，用于对数据进行更新（增删改），注意没有查\n\n插入insert\n# 1.\ninsert into 表(列名1，列名2,....) values(值1，值2，值3...)//向表中插入，注意列与值一一对应，类型也要对应\n# 2.\ninsert into 表 values (值1，值2，值3...);//向表中所有列给值\n\n# 3.一个insert插入多行\ninsert into 表(列名1，列名2,....) \nvalues\n(值1，值2，值3...),\n(值1，值2，值3...),\n(值1，值2，值3...);\n\n\n# 例如\nINSERT INTO student VALUES(1001,'张三',21,'2000-12-24','北京')\n#注意字符由''引处，\n\n删除delete\n#1.删除数据\ndelete from 表名 [where 条件]\n#2.清空表数据\ntruncate table 表名 或者 truncate 表名\n\n\n-- 注意delete和truncate原理不同，delete只删除内容，而truncate类似于drop table，可以理解为是将整个表删除后再创建该表\n\n\n\n\n\n更新&#x2F;修改update\n#1. 修改所有行\nupdate 表名 set 字段名=值，字段名=值...;\n\n\n# 2. 修改满足条件的行\nupdate 表名 set 字段名=值，字段名=值... where 条件;\n\n\n# 例子\n-- 将所有学生的地址修改为重庆\nupdate student set address='重庆';\n\n-- 将id为1004的学生的地址修改为北京\nupdate student set address='北京' where id=1004;\n-- 将id为1005的学生的地址修改为北京，成绩修改为100\nupdate student set address ='广州',score='100' where sid=1005;\n\n练习\nUSE mydb1;\n\nCREATE TABLE employee(\nid INT,\nNAME VARCHAR(20),\ngender VARCHAR(10),\nsalary DOUBLE\n);\n\n\nCREATE TABLE mydb1.employee(\nid INT,\nNAME VARCHAR(20),\ngender VARCHAR(10),\nsalary DOUBLE\n);\n\nINSERT INTO employee VALUES(1,'张三','男',2000),(2,'王五','男',1000);\n\nUPDATE employee SET salary=4000,gender='女' WHERE NAME='王五';\n\n-- 王五的薪水在原有的基础上增加1k\nUPDATE employee SET salary=salary+1000 WHERE NAME='王五'\n\nMySQL约束概念约束英文：constraint约束实际上就是对表中数据的&#x3D;&#x3D;限制条件&#x3D;&#x3D;作用例如手机号不能为空，身份证号不允许重复分类\n\n主键约束\n自增长约束\n非空约束\n唯一性约束\n默认约束\n零填充约束\n外键约束1、主键约束\n\n概念\n\n主键约束可以是一个列作为主键或者是多个列联合作为主键，&#x3D;&#x3D;其值能唯一标识一行数据，便于尽快找到&#x3D;&#x3D;\n相当于 &#x3D;&#x3D;唯一约束 + 非空约束&#x3D;&#x3D;，主键约束不允许重复，也不允许出现空值\n每个表最多只允许一个主键（多个列的主键是联合为了一个来唯一标识）\n关键字叫：primary key\n当创建主键约束时，系统默认会在对应的列或者是列组合上建立对应的唯一索引。\n\n操作\n\n添加单列主键\n添加多列主键\n删除主键\n\n1. 添加单列主键方式1：\ncreate table 表名(\n...\n&lt;字段名> &lt;数据类型> primary key,\n...\n);\n\n--例如：\nCREATE TABLE mydb1.emp1(\neid INT PRIMARY KEY,\nNAME VARCHAR(20),\ndetpID INT,\nsalary DOUBLE\n);\n\n方式2：\ncreate table 表名(\n...\n&lt;字段名> &lt;数据类型>,\n...,\n[constraint 主键名（取的）] primary key(id)\n)\n\n\n-- 例如\nCREATE TABLE mydb1.emp1(\neid INT ,\nNAME VARCHAR(20),\ndetpID INT,\nsalary DOUBLE,\nconstraint pk1 primary key(eid) -- constaint pk1可以省略\n);\n主键的作用\n\n不允许重复\n不允许空值INSERT INTO mydb1.`emp1` VALUES(1001,'张三',1,5000);\nINSERT INTO mydb1.`emp1` VALUES(1001,'王五',1,5000);\nINSERT INTO mydb1.`emp1` VALUES(NULL,'张三',1,5000);\n\n\n\n2. 添加多列联合主键所谓的联合主键，就是多个列组成了一个主键注意：    1. &#x3D;&#x3D;当主键是由多个字段组成时，不能直接在字段名后面声明主键。&#x3D;&#x3D;    2. &#x3D;&#x3D;一张表只能有一个主键，联合主键也是一个主键&#x3D;&#x3D;\n语法\ncreate table 表名(\n...\nprimary key (字段1，字段2，字段3)\n);\n例如\ncreate table emp3(\nname varchar(20),\ndeptID int,\nsalary double,\nprimary key(name,deptID)\n)\n作用\n\n作为主键的各列&#x3D;&#x3D;不完全相同&#x3D;&#x3D;\n不允许&#x3D;&#x3D;任意列为空值&#x3D;&#x3D;\n\n-- 只要主键列不是完全相同都是可以加进去的\nINSERT INTO mydb1.emp3 VALUES('张三',1,3000);\nINSERT INTO mydb1.emp3 VALUES('张三',2,3000);\nINSERT INTO mydb1.emp3 VALUES('李四',1,3000);\n\n\n-- 联合主键的各列都不能为空\nINSERT INTO mydb1.emp3 VALUES(NULL,1,3000);\nINSERT INTO mydb1.emp3 VALUES('张三',NULL,3000);\n\n\n\n3. 操作表结构添加主键添加单列主键：ALTER TABLE 表名 ADD PRIMARY KEY(字段名)\n\nCREATE TABLE mydb1.emp5(\neid INT,\nNAME VARCHAR(20),\ndeptId INT,\nsalary DOUBLE\n);\n\nALTER TABLE mydb1.emp5 ADD PRIMARY KEY(eid);\n\n\n添加多列主键：ALTER TABLE 表名 ADD PRIMARY KEY(字段名1，字段名2)\n\nCREATE TABLE mydb1.emp5(\neid INT,\nNAME VARCHAR(20),\ndeptId INT,\nsalary DOUBLE\n);\n\nALTER TABLE mydb1.emp5 ADD PRIMARY KEY(NAME,deptId);\n\n\n4. 删除主键语法：alter table 数据表名 drop primary key;例子\nALTER TABLE mydb1.emp5 DROP PRIMARY KEY;\n\n2、自增长约束概念在mysql中，当主键定义为自增长后，这个主键的值就不需要用户再管了，而由DBMS来自动赋值，每增长一个数据，主键会自动同步增长，通过给字段添加 &#x3D;&#x3D;auto_increment&#x3D;&#x3D; 来实现主键自动增长\n语法\n字段名 数据类型 auto_increment\n操作\ncreate table t_user1(\nid int primary key auto_increment,\nname varchar(20)\n);\n特点 指定自增长字段的初始值 方式1：创建表时指定 CREATE TABLE t_user2(\nid INT PRIMARY KEY AUTO_INCREMENT,\nNAME VARCHAR(20)\n)AUTO_INCREMENT=100; 方式2：创建表后修改表 alter table 表名 auto_increment=100;\ndelete与truncate在删除自增列后的变化\n3、非空约束概念语法方式1：创建表时字段名 数据类型 not null方式2：创建表后alter table 表名 modify 字段 类型 not null\n添加非空约束方式方式1：\ncreate table t_user6(\nid int,\nname varchar(20) not null,\naddress varchar(20) not null\n)\n方式2;\ncreate table t_user7(\nid int,\nname varchar(20),\naddress varchar(20)\n);\n\n\nalter table t_user7 modify name varchar(20) not null;\nalter table t_user7 modify address varchar(20) not null;\n\n删除非空约束alter table 表名 modify 字段名 类型;\nalter table t_user7 modify name varchar(20);\nalter table t_user7 modify address varchar(20);\n\n查看表结构desc table_name，结果如下图\n4、唯一约束概念\n语法方式1：创建表时字段名 数据类型 unique方式2：创建表后alter table 表名 add constraint 约束名 unique(列)添加唯一约束方式1：\nCREATE TABLE t_user8 ( \n id INT , \n NAME VARCHAR(20) , \n phone_number VARCHAR(20) UNIQUE -- 指定唯一约束 \n);\n在mysql中null与任何值都不重复，所以可以在phonenumber为唯一约束时，下面成立\nINSERT INTO t_user8 VALUES(1001,'张三',NULL);\nINSERT INTO t_user8 VALUES(1001,'张三',NULL);\n方式2：首先创建一个表\nCREATE TABLE t_user9 ( \n id INT , \n NAME VARCHAR(20) , \n phone_number VARCHAR(20)\n);\n添加约束\nALTER TABLE t_user9 ADD CONSTRAINT unique_pn UNIQUE(phone_number); -- 此处的约束名在删除约束时用得到\n删除约束\n-- 删除约束，如果没有指定名字，那么约束名字默认为列名\nALTER TABLE t_user9 DROP INDEX unique_pn;\n\n5、默认约束概念\n语法\n方式1：&lt;字段名> &lt;数据类型> default &lt;默认值>;\n方式2：alter table 表名 modify 列名 类型 default 默认值;\n\n方式方式1：创建时在其后加default &#39;值&#39;\nCREATE TABLE t_user10 ( \n  id INT , \n  NAME VARCHAR(20) , \n  address VARCHAR(20) DEFAULT '北京' -- 指定默认约束 \n);\n方式2：创建表后修改表结构先创建表\nCREATE TABLE t_user11 ( \n  id INT , \n  NAME VARCHAR(20) , \n  address VARCHAR(20)\n);\n再添加约束\nALTER TABLE t_user11 MODIFY address VARCHAR(20) DEFAULT '深圳';\n\n删除约束\nALTER TABLE t_user11 MODIFY address VARCHAR(20) DEFAULT NULL;\n6、零填充约束7、总结DQL运算排序查询","slug":"Mysql","date":"2022-03-28T08:20:28.000Z","categories_index":"","tags_index":"sql","author_index":"Cencus"},{"id":"fc02d5383b2538f023ce4a0201d30c9f","title":"yum的配置","content":"1. 备份之前的仓库文件mv &#x2F;etc&#x2F;yum.repos.d&#x2F;CentOS-Base.repo &#x2F;etc&#x2F;yum.repos.d&#x2F;CentOS-Base.repo.back\n\n下载wget工具sudo yum install -y wget或者点这个链接\n下载阿里云仓库文件wget -o /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyum.com/repo/Centos-7.repo\n使用-o指定文件名\n清除缓存yum clean all\nyum makecache\n弄好后报错：\n\n\n\n\n\n\n\n\n\nFile contains no section headers.\nfile: file:&#x2F;&#x2F;&#x2F;etc&#x2F;yum.repos.d&#x2F;CentOS-Base.repo, line: 1\n‘–2022-03-28 23:07:08–  http://mirrors.aliyum.com/repo/Centos-7.repo\\n&#39;\n解决办法：\nsudo rm -f /etc/yum.repos.d/*\n然后重新：\nwget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo\n接着：yum clean all\nsudo rpm -Uvh http://dl.fedoraproject.org/pub/epel/6/x86_64/epel-release-6-8.noarch.rpm\n","slug":"yum的配置","date":"2022-03-28T07:05:42.000Z","categories_index":"","tags_index":"yum,linux优化","author_index":"Cencus"}]